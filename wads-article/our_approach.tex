\subsection{Our approach}
We aim to design a model of universal hashing which guarantees the worst case running time of Find operation independently on the stored set. Moreover we want all operations to have constant running times in the expected amortised case. One reason for amortisation is the fact that we have to keep the load factor in the predefined bounds. Reasonable space consumption determines the lower bound and low expected chain lengths give the upper bound.

Our hash table preserves all properties of plain universal hashing but adds the worst case warranty for a look up. The warranty we are able to achieve strongly depends on the underlying universal system. Our first step is to point out the systems which are already known to give convenient limits on the length of the longest chain. The second step is to use these high quality universal systems with our simple model of hashing.

The obvious way how to provide the worst case warranty is to rehash the whole table when the length of the longest chain exceeds the prescribed limit. Although this is the case of our model we have a single requirement on the limit function. The probability that a random function from $H$ creates a chain exceeding the limit is less than a prescribed probability rate. When the data is unsuitable for a chosen hash function we switch to another function. Finally our requirement says that the newly chosen function matches the data with high probability. All these ideas are formalised later in Sec. \ref{section-model}.

In Sect. \ref{section-limit} we deal with various universal systems and their limit functions. In already mentioned Sect. \ref{section-model} we describe our model and connect it with the discussed universal systems. Section \ref{section-conclusion} points out further improvements and alternatives of the model. Possibilities of finding other limit functions are discussed, too. The proposed new approaches could be competitive with current plain hashing and should have better real world behaviour than chained hashing.
