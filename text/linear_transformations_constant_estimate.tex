\section{Estimating the multiplicative constant}

\subsection{Integral}
When we were proving the upper bond on the length of the longest chain in a hash table we defined the constant $I$ as:
\begin{displaymath}
I = \displaystyle\int\limits_4^\infty 2 \left(\frac{r}{\log r}\right)^{-\log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)}\textit{.}
\end{displaymath}

Now we will extend the definition of $I$ to $I_{\epsilon}$. This integral is one part of the multiplicative constant $16c_\epsilon(4 + I)$.
\begin{displaymath}
I_{\epsilon} = \frac{1}{1 - \epsilon} \displaystyle\int\limits_4^\infty \left(\frac{r}{\log r}\right)^{-\log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)}\textit{.}
\end{displaymath}
Originally we used $I_{\frac{1}{2}} = I$. The extension is motivated by the fact that we were not forced to use $\epsilon$ equal to $0.5$. If we chose other values we could obtain even smaller integral values. If we select an arbitrary $\epsilon$ the conditional probability of event $E_2 | E_1$ is constrained as $P(E_2 | E_1) \geq 1 - \epsilon$. And the bound on the probability of event $E_1$ becomes $P(E_1) \leq \frac{1}{1-\epsilon} P(E_2)$.

Evaluation of the integral $I$ may be split into two parts. We try to compare it to a function which has a convergent improper integral. The function chosen here is $r^{1.5}$. But the chosen function becomes greater after the value $r = 16$. In the interval $[4, 16]$ the integral $I$ is bounded by its upper Riemann sum.

For $r = 16$:
\begin{displaymath}
\frac{1}{16 ^ {1.5}} = \frac{1}{64}
\end{displaymath}

\begin{displaymath}
\left(\frac{16}{\log 16}\right)^{-\log \left(\frac{16}{\log 16}\right) - \log \log \left(\frac{16}{\log 16}\right)} = 4^{-2 - 1} = \frac{1}{64}
\end{displaymath}

For $r > 16$ we can use our estimates.
\begin{displaymath}
\begin{split}
I_{\epsilon} 
	& \leq \frac{1}{1 - \epsilon} \left( \displaystyle \sum_{r = 4}^{15} \left(\frac{r}{\log r}\right)^{-\log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} + \int\limits_{16}^\infty \frac{1}{r^{1.5}} dr \right) \\
	& \leq \frac{1}{1 - \epsilon} \left(4.3 + \frac{1}{2}\right) = \frac{4.8}{1-\epsilon}
\end{split}
\end{displaymath}

\subsection{Choosing $\epsilon$}
The most important step is the optimisation of the value $16 c_\epsilon (4 + I_{\epsilon})$. This is the explicit formula for the multiplicative constant. This value is less then 22 568 for $\epsilon = 0.91$, $k = 3.28$ and $l = 0.5$. These values come from a slight modification of the theorem \ref{theorem-set-onto-by-linear-transform} and we will explain them later. Though the asymptotic rate of the growth is $O(\log n \log \log n)$ with this large multiplicative constant the estimate becomes less than linear for $n$ approximately equal to one million.

The first approach is to modify the proof of the theorem \ref{theorem-set-onto-by-linear-transform} in a way that we parametrise every constant in it. Values of the parameters are than optimised to get the least multiplicative constant. The optimisation itself is not performed analytically because of its complexity that is caused by the emerged constraints. 

Instead we created a straightforward program that assigns each parameter values from a predefined interval. They are enumerated uniformly with the prescribed step. The multiplicative constant is then computed for every such assignment and the best achieved value is remembered.

We actually created two parameters $k$ and $l$. The limit of the size of the set $T_0(A)$ is changed to $\frac{|A|}{k}$, for $k > 2$. The second parameter is obtained by modifying the dimension of the factor vector space $Z_2^u$. We change the definition of $u$ to:
\begin{displaymath}
u = \left\lceil \log \left(\frac{2^l |A|}{\epsilon}\right) \right\rceil \textit{.}
\end{displaymath}

The probability of the event $T(A) \neq Z_2^t$ can be expressed by using law of total probability as:
\begin{displaymath}
\begin{split}
P(T(A) \neq Z_2^t) 
    & = P(T(A) \neq Z_2^t \wedge |T_0(A)| \leq \frac{|A|}{k}) + P(T(A) \neq Z_2^t \wedge |T_0(A)| > \frac{|A|}{k}) \\ 
    & \leq P(|T_0(A)| \leq \frac{|A|}{k}) + P(T_1(T_0(A)) \neq Z_2^t \wedge |T_0(A)| > \frac{|A|}{k}) \\
    & \leq \epsilon \\
\end{split}
\end{displaymath}
The right side, $\epsilon$, is the needed result which we must obtain by choosing the convenient $c_{\epsilon}$. Also the estimate of $c_{\epsilon}$ was modified comparing to the original proof. By putting $c_{\epsilon}$ equal to $4\left(\frac{2}{\epsilon}\right)^{\frac{8}{\epsilon}}$ we could not get a good result. So we compute the constant $c_{\epsilon}$ explicitly without any estimations from the last inequality in the proof of theorem \ref{theorem-set-onto-by-linear-transform}.

\begin{lemma}
\label{lemma-collision-count}
When the size of the set $|T_0(A)|$ is less than $\frac{|A|}{k}$ for $1 \leq k$ then there are at least $\frac{|A|(k - 1)}{2}$ collisions.
\end{lemma} 
\begin{proof}
Define the sequence $b_i$ for $i \in T_0(A)$ where $b_i = \left|A \cap T_0^{-1}(i)\right|$. Also note that $\sum_{i \in T_0(A)} b_i = |A|$.
The number of all colliding pairs can be computed as follows.
\begin{displaymath}
\frac{1}{2} \sum_{i \in T_0(A)} b_i (b_i - 1) \geq \frac{|A|}{2}\left(\frac{|A|}{|T_0(A)|} - 1\right) \geq \frac{|A|(k - 1)}{2}
\end{displaymath}
The last inequality can be obtained from Cauchy–Bunyakovsky–Schwarz inequality.
\end{proof}

\begin{displaymath}
\begin{split}
P(|T_0(A)| \leq \frac{|A|}{k}) & + P(T_1(T_0(A)) \neq Z_2^t \wedge |T_0(A)| > \frac{|A|}{k}) \\ 
& \leq \frac{|A| - 1}{(k - 1)|W|} + \left(1 - \frac{|A|}{k|W|}\right)^{u - t - \log t + \log \log \left(\frac{1}{1 - \frac{|A|}{k|W|}}\right)} \\
& < \frac{\epsilon}{(k - 1) 2 ^ l} + \left(1 - \frac{\epsilon}{2 k 2^l}\right)^{\log c_\epsilon + l - \log \epsilon + \log \log \left(\frac{1}{1 - \frac{\epsilon}{2 k 2^l}}\right)} \\
& \leq \epsilon \\
\end{split}
\end{displaymath}

For convenience we define a new variable $\alpha'$ equal to $1 - \frac{\epsilon}{2 k 2 ^l}$. From the above inequality we need to find the minimal value of $c_\epsilon$:
\begin{displaymath}
\begin{split}
\frac{\epsilon}{(k - 1) 2 ^ l} + {\alpha'}^{\log c_\epsilon + l - \log \epsilon + \log \log \left(\frac{1}{\alpha'}\right)} & \leq \epsilon \\
{\alpha'}^{\log c_\epsilon}{\alpha'}^{l - \log \epsilon + \log \log \left(\frac{1}{\alpha'}\right)} & \leq \epsilon - \frac{\epsilon}{(k - 1) 2 ^ l} \\
{\alpha'}^{\log c_\epsilon} & \leq \left(\epsilon - \frac{\epsilon}{(k - 1) 2 ^ l}\right) {\alpha'}^{\log \epsilon - l - \log \log \left(\frac{1}{\alpha'}\right)} \\
{\log c_\epsilon} & \geq \frac{\log \left( \left( \epsilon - \frac{\epsilon}{(k - 1) 2 ^ l}\right) {\alpha'}^{\log \epsilon - l - \log \log \left(\frac{1}{\alpha'}\right)}\right)}{\log \alpha'}  \\
\end{split}
\end{displaymath}


\section{Hashing a linear amount of elements}
A common use of a hash table is with load factors lower than one. We already showed if there is a super-linear amount of elements hashed we can expect a reasonable result in the worst case. However the most important is the expected size of a bucket. The upper bound of the expected size is equal to $1 + c \alpha$. So using a load factor lower than one has a significant impact on the average case. We need to examine a scheme of hashing $\alpha_f n$ elements into a hash table of size $n$ where $\alpha_f$ is the table's load factor. 

We need to change the previous claims to fit our new scheme: 
\begin{theorem}
\label{theorem-n-to-n}
When hashing $\alpha_f n$ elements into a table of size $n$ the expected length of the longest chain is bounded by $O(\alpha_f \log n \log \log n)$.
\end{theorem}
\begin{proof}
We have to modify the previous lemmas and their proofs from. Apparently we must lower the value of a chain length when we get a convenient probability estimate proportionally to $\alpha_f$. 

\begin{remark}
There is a constant $C$ so that for all $r > 4$, $S$ is the hashed set ($S \subset D$, $|S| = \alpha_f n$) and $B = Z_2^{\log n}$ is the hash table the probability of existence of a long chain is:
\begin{displaymath}
P(lpsl > r \alpha_f C \log n \log \log n) \leq 2 \left(\frac{r}{\log r}\right)^{-\log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)}\textit{.}
\end{displaymath}
\end{remark}
\begin{proof}
We show changes that need to be made. 
\begin{displaymath}
\begin{split}
l & = \left\lfloor \log n + \log \log n + \log r - \log \log r + \log \alpha_f + 1 \right\rfloor \\
t & = 4\alpha_f c_{\frac{1}{2}} \log n \log \log n \\
d & = \frac{2^l}{\alpha_f n \log n} \\
\end{split}
\end{displaymath}

The assumptions of the remark \ref{remark-e2-probability} are met. The factor $\alpha_f$ may appear in the denominator because the size of the hashed set is certainly limited by $\alpha_f n \leq \alpha_f n \log n$:
\begin{displaymath}
d = \frac{2^l}{\alpha_f n \log n} \geq \frac{\alpha_f n \log n r}{\alpha_f n \log n \log r} > 1 \textit{.}
\end{displaymath}

In the proof of the \ref{remark-e2-probability} we also used a relation among $h_1(S)$, $A = Z_2^l$, $\alpha$ and $d$. We must show that similar relations hold. This makes our reasoning valid for this scheme, too.
\begin{displaymath}
\alpha = \frac{|h_1(S)|}{|A|}\leq \frac{|S|}{|A|} = \frac{\alpha_f n}{2^l} \leq \frac{\alpha_f n \log n}{2^l} = \frac{1}{d} \textit{.}
\end{displaymath}

For using the other lemma the value of the variable $t$ has to be large enough:
\begin{displaymath}
\begin{split}
c_{\frac{1}{2}} \frac{2^l}{n} \log \left(\frac{2^l}{n}\right) 
	& < c_{\frac{1}{2}} 2 \alpha_f \log n \left( \frac{r}{\log r} \right) \left(2 \log \log n \log r \right) \\
	& \leq 4 c_{\frac{1}{2}} \alpha_f \log n \log \log n \log r \\
	& = t
\end{split}
\end{displaymath}

The conditions of the remark \ref{remark-prob-t-length-chain} are fulfilled. Its proof is still right, since we use it with no further assumptions. The same vector space $A = Z_2^l$ is constructed; we never referenced the value of $l$. The vector space $B$, the hash table, is unchanged.

We continue identically to the previous case:
\begin{displaymath}
P(E1) \leq \frac{1}{P(E2|E1)}P(E2) \leq 2 d ^ {-\log d - \log \log d}\textit{.}
\end{displaymath}
\end{proof}

In order to achieve the desired length we modify the calculation by taking $K = C \alpha_f \log n \log \log n$.
\begin{displaymath}
\begin{split}
E lpsl 
	& = \int\limits_0^\infty P(lpspl > t) dt \\
	& \leq 4K + \int\limits_{4K}^\infty P(lpspl > t) dt \\
	& = 4K + K\int\limits_4^\infty P(lpspl > tK) dt \\
	& \leq K(4 + I) = O(K) = O(\alpha_f \log n \log \log n)
\end{split}
\end{displaymath}

Expression $I$ has not been modified.
\end{proof}

\section{Obtaining an even better multiplicative constant}
For the model of hashing linear amount of elements we only used the estimates valid also in the case of hashing the super-linear count of items. Now in order to obtain a better result we have to reconsider our choices. The ideas remain the same but we will change some claims to suit our needs.

The previous and not very successful attempt gave us a constant suitable for hashing millions of elements. With a constant in the order of hundreds our estimates starts beating the linear one when hashing around 40 000 elements.

The constant $c_\epsilon$ plays the most crucial role. We try to lower its value first. Instead of making the factor space $W$ larger than the set $A$ we will bound it between $\frac{|A|}{4}$. But it still must be larger than the target space $Z_2^t$. Of course this is possible since the set $A$ contains at least $t2^t$ elements.

\begin{remark}
For every convenient $\epsilon$ there is a constant $c_\epsilon$ such that for every subset $A$, $|A| \geq c_\epsilon t 2^t$ of the source space $V$ and for an uniformly selected linear transformation $T$ to the target space $Z_2^t$ the probability of mapping $A$ to the whole target space is at least $1-\epsilon$:
\begin{displaymath}
P(T(A) = Z_2^t) \geq 1 - \epsilon \textit{.}
\end{displaymath}
\end{remark}
\begin{proof}
We present the proof with all the possible parameters that can have their value chosen to optimise the value of $c_\epsilon$. According to these parameters the interval for $\epsilon$ may be found.

For every value $\frac{|A|}{2^l}$ there is a power of two $2^u = |W|$ less or equal to $\frac{2|A|}{2^l}$. The value of $l$ will be non-negative and so $|W| \leq |A|$. As in the previous modifications we will estimate the two probabilities obtained by using the law of total probability.

According to the lemma \ref{lemma-collision-count} when the result $T_0(A)$ has less than $\frac{|W|}{k}$ elements there must be at least $\frac{|A|}{2}\left(\frac{k|A|}{|W|} - 1\right)$ collisions caused by it.

\begin{displaymath}
P(|T_0(A)| \geq \frac{|W|}{k}) \leq \frac{|A|(|A| - 1)}{2|W|\frac{|A|}{2}\left(\frac{k|A|}{|W|} - 1\right)} \leq \frac{|A|}{k|A| - |W|} \leq \frac{|A|}{k|A| - \frac{2|A|}{2^l}} \leq \frac{2^l}{2^l k - 2}
\end{displaymath}

The remaining case is when $|T_0(A)| \geq \frac {|W|}{k}$.

\begin{displaymath}
\alpha = 1 - \frac{|T_0(A)|}{|W|} \leq 1 - \frac{1}{k}
\end{displaymath}

\begin{displaymath}
P(T(A) \neq Z_2^t) \leq \left(1 - \frac{1}{k}\right)^{\log c_\epsilon + \log \log \left( \frac{1}{1 - \frac{1}{k}} \right) - l}
\end{displaymath}

The sum of both cases is greater than the whole probability of event $T(A) \neq Z_2^t$ and must be less than $\epsilon$.

\begin{displaymath}
\begin{split}
\frac{2^l}{2^l k - 2} + \left(1 - \frac{1}{k}\right)^{\log c_\epsilon + \log \log \left( \frac{1}{1 - \frac{1}{k}} \right) - l} & \leq \epsilon \\
\left(1 - \frac{1}{k}\right)^{\log c_\epsilon} & \leq \frac{\epsilon - \frac{2^l}{2^l k - 2}}{\log \log \left( \frac{1}{1 - \frac{1}{k}} \right) - l} \\
\log c_\epsilon & \geq \frac{\log \left(\frac{\epsilon - \frac{2^l}{2^l k - 2}}{\log \log \left( \frac{1}{1 - \frac{1}{k}} \right) - l}\right)}{\log \left(1 - \frac{1}{k}\right)}
\end{split}
\end{displaymath}

For some values $\epsilon$, $k$ and $l$ it may happen $\frac{2^l}{2^l k - 2} \geq \epsilon$. In this case no $c_\epsilon$ may be found using this proof. However with this computation slightly better results for valid choices are found. Compare the value 17.31 ($\epsilon = 0.8967$) to 67.77 ($\epsilon = 0.98$).
\end{proof}

Now we use this approach to prove a better bound on the expected length of the longest chain. The novel part lies in choosing a slightly different and smaller size of the factor space $A$.
\begin{theorem}
\label{elpsl-hashing-linear-amount}
When hashing a linear amount of $\alpha_f n$ elements into a table of size $n$ the expected length of the longest chain is bounded by $O(\alpha_f \log n \log \log n)$.
\end{theorem}
\begin{proof}
Our choices for $r \geq 4$:
\begin{displaymath}
\begin{split}
l & = \lfloor \log n + \log r - \log \log r + \log \alpha_f + 1\rfloor \\
d & = \frac{2^l}{\alpha_f n} \geq \frac{\alpha_f n r}{\alpha_f n \log r} = \frac{r}{\log r} \geq 2 \\
t & = 2 \alpha_f c_\epsilon r \\
\end{split}
\end{displaymath}

The choice for $d$ gives us the probability of the event $E_2$.
\begin{displaymath}
\begin{split}
\alpha & = 1 - \frac{|A - h_1(S)|}{|A|} \leq \frac{|S|}{|A|} = \frac{1}{d} \\
\log d & = l - \log \alpha_f - \log n \geq l - \log n \\
P(E_2) 
	& \leq \alpha^{l - \log n - \log \log n + \log \log \left( \frac{1}{\alpha} \right)} \\
	& \leq \left( \frac{1}{d} \right)^{\log d - \log \log n + \log \log d} \\
	& \leq \left(\frac{r}{\log r}\right)^{\log \log n - \log d - \log \log d} \\
	& \leq \left(\frac{r}{\log r}\right)^{\log \log n - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} \\
\end{split}
\end{displaymath}

The verification of validity of choice for the variable $t$ follows:
\begin{displaymath}
\begin{split}
c_\epsilon \frac{2^l}{n} \log \left( \frac{2^l}{n} \right) 
	& \leq 2 c_\epsilon \alpha_f \frac{r}{\log r} \log \left( \frac{2 \alpha_f r}{\log r} \right) \\
	& \leq 2 c_\epsilon \alpha_f \frac{r}{\log r} \log r = 2 c_\epsilon \alpha_f r = t
\end{split}
\end{displaymath}

Thus $P(lpsl \geq 2 \alpha_f c_\epsilon r) \leq \frac{1}{1 - \epsilon} \left(\frac{r}{\log r}\right)^{\log \log n - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)}$. From this fact we obtain a bound on the expected longest length.

\begin{displaymath}
\begin{split}
E lpsl 
	& = \int\limits_0^\infty P(lpsl \geq t) dt \leq 2 c_\epsilon 4 + \int\limits_{8 c_\epsilon}^\infty P(lpsl \geq t) dt \\ 
	& = 8c_\epsilon + 2 c_\epsilon \int\limits_{4}^\infty P(lpsl \geq 2c_\epsilon r) dr \\
	& = 2c_\epsilon \left(4 + \frac{1}{1 - \epsilon} \int\limits_{4}^\infty \min\left(1, \left(\frac{r}{\log r}\right)^{\log \log n - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)}\right) dr \right) \\ 
	& \leq 2c_\epsilon \left( 4 + \frac{1}{1-\epsilon}\left( 2 \log n \log \log n - 4 + I \right) \right) \\
\end{split}
\end{displaymath}

For the integral $I$ estimation for simplicity we will denote the value $d = \frac{r}{\log r}$.
\begin{displaymath}
I = \int\limits_{2 \log n \log \log n}^\infty d ^ {\log \log n - \log d - \log \log d} dr \leq 14
\end{displaymath}

The whole bound looks like:
\begin{displaymath}
E lpsl \leq \frac{4c_\epsilon}{1 - \epsilon} \log n \log \log n + 2 c_\epsilon \left( \frac{I - 4}{1 - \epsilon} + 4 \right) \textit{.}
\end{displaymath}

The estimate of integral $\int\limits_{4}^\infty \left(\frac{r}{\log r}\right)^{\log \log n - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} dr$ remains to be unveiled. First we must realise that the exponent must be negative in order to get the integrand below 1. And it is sufficient to have $\log \left( \frac{r}{\log r} \right) > \log \log n$. Our choice is to put $r > 2 \log n \log \log n$:
\begin{displaymath}
\begin{split}
d & \geq \log n \\
\frac{r}{\log r} & \geq \log n \\
\frac{r}{\log r} & \geq \frac{2 \log n \log \log n}{1 + \log \log n + \log \log \log n} \\
	& = \frac{2 \log n}{1 + \frac{1}{\log \log n} + \frac{\log \log \log n}{\log \log n}} \\
	& \geq \log n \\
\end{split}
\end{displaymath}

Which is valid for $n \geq 16$. The value of $I$ is determined in the similar fashion as in the previous case. The best estimate achieved is 538 $\log n \log \log n$ + 2~897.
\end{proof}

\section{Better hashing schemes}
The expected length of the longest chain gives us the generic hint when we should rehash the table to guarantee the worst case time for the member operation. The lower the value $E lpsl$ is the better worst case bound is achieved. The probability of event that there is chain exceeding the value $c E lpsl$ is bounded by $\frac{1}{c}$. This is a straightforward use of the Markov inequality. 

The probability bound of existence of a long chain has two important consequences:
\begin{itemize}
\item At most the fraction $\frac{1}{c}$ of all the functions cause long chains; longer than $c E lpsl$. The probability of rehashing for the uniform function's choice is then $\frac{1}{c}$.
\item When rehashing; when a long chain appeared; to guarantee the bound of $c E lpsl$ the probability of finding a suitable function is $1 - \frac{1}{c}$. Note that this holds under the assumption of the uniform choice of an universal function.
\end{itemize}

Mentioned probability bound computed from the expected value can be improved by direct use of the cumulative density function of the variable $lpsl$. We already found and consequently improved it in the previous theorems. To sum up we can say that the approach with the expected length is more general but achieves worse performance. Note that whenever we know the probability density function we are able to compute the expected value.

\section{Better bound estimate for the systems of linear functions}
\label{section-linear-systems-linear-amount-constant-estimate}
From now on we concentrate on obtaining a bound on the length of the chain if we allow the probability of rehashing $\frac{1}{c}$. The corresponding bound is computed from the density function.

\begin{displaymath}
\begin{split}
P(lpsl \geq 2\alpha_f c_\epsilon r) & \leq \frac{1}{c} \\ 
\frac{1}{1 - \epsilon}\left(\frac{r}{\log r}\right)^{\log \log n - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} & \leq \frac{1}{c}
\end{split}
\end{displaymath}
Let us remark that the probability estimate works for $r \geq 4$ only, proof of theorem \ref{elpsl-hashing-linear-amount}.

Let $d = k \log n$ and $d \leq \frac{r}{\log r}$ for a positive constant $k$. When the exponent of function $d^{\log \log n -\log d - \log \log d}$ is negative it is decreasing and the previous inequality holds if
\begin{displaymath}
d ^ {\log \log n - \log d -\log \log d} \leq \frac{1 - \epsilon}{c}\textit{.}
\end{displaymath}

As first for the exponent we have:
\begin{displaymath}
\begin{split}
\log & \log n - \log (k \log n) - \log \log (k \log n) \\
	& = \log \log n - \log k - \log\log n - \log \log (k \log n) \\
	& \leq - \log k - \log \log n \\
\end{split}
\end{displaymath}
The last inequality holds for $\log (k \log n) \geq 1$ and this is equivalent to $n \geq 2^{\frac{2}{k}}$.

We selected $d$ as a lower bound for $\frac{r}{\log r}$. Now we need to find a suitable value for $r$. Putting $r = 2 d \log d$ we need to confirm $d$ is a lower bound. For every $d$ greater or equal to $2$ the estimate works
\begin{displaymath}
\frac{r}{\log r} = \frac{2 d \log d}{1 + \log d + \log \log d} \geq d \textit{.}
\end{displaymath}
The assumption for the probability estimate used was $r \geq 4$ which gets fulfilled by choosing $d \geq 2$ as well.

And we explicitly compute the dependence of $r$ on $n$:
\begin{displaymath}
r = 2 d \log d = 2 k \log n (\log k + \log \log n) = 2 k \log n \log \log n + 2 k \log k \log n
\end{displaymath}



Since $d = k \log n$ for the probability we want:
\begin{displaymath}
\begin{split}
d ^ {\log \log n - \log d -\log \log d} & \leq \frac{1 - \epsilon}{c} \\
\left(k \log n\right)^{-\log k -\log \log n} & \leq \frac{1 - \epsilon}{c} \\
\end{split}
\end{displaymath}
This gives us the lowest $n$ when our bound is valid. 

Also note that all the assumptions for the claims used are met. The bound for the fixed probability $\frac{1}{c}$ when the table is rehashed should be optimised. This bound is obtained as:
\begin{displaymath}
2 \alpha_f c_\epsilon r = 4 c_\epsilon k \alpha_f (\log n \log \log n + \log k \log n)
\end{displaymath}
And the best estimate is for the minimum $c_\epsilon$ which from the above section is 17.31 for the $\epsilon = 0.8967$.

And for example by fixing $k = 1$ and $c = 2$ with the best $\epsilon$ one gets:
\begin{displaymath}
\begin{split}
\left(\frac{1}{\log n}\right)^{\log \log n} & \leq \frac{1 - \epsilon}{2} \\
\end{split}
\end{displaymath}

The estimate is certainly valid for $n \geq 32$. The bound's multiplicative constant without the load factor is $17.31 . 1 . 4$ which is less than 70. This bound becomes better than linear ($n$) for $n$ approximately equal to 3 000. But we were not forced to using $k = 1$, we assumed only the positiveness of $k$. So by choosing a lower value we obtain a better bound. The smaller value of $k$ postpones the validity of our estimates to greater values of $n$.

For small hash tables the best bound is achieved when the value of $n$ when our estimates are valid is the same as the value obtained by the above estimate. This value is for $k = 0.2766$, $n \geq 572$ and the multiplicative constant is $19.152$. This value has also been computed by a simple computer program.
