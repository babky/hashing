\chapter{Estimating the multiplicative constant}
Improvements of the previous results of hashing with systems of linear transformations are needed since the obtained multiplicative constants are enormous. At first we decrease value of constant $c_\epsilon$ and it lower the multiplicative constant as well. Because typical load factors are lower than one we assumed hashing only linear amounts of elements with respect to the size of the hash table. In the previous chapter we showed similar asymptotic result. It has to be improved and to gain even better results.

To complete our goal we change the previous claims to fit the scheme of hashing linear amounts of elements. As already mentioned good asymptotic behaviour is not enough since we can not neglect the inherited constant factors. In the presented proofs of corresponding theorems we try to make the estimates as tight as possible.

\section{Minimising the integral part}
\label{section-integral-estimate}
When we were performing last part of the proof of Theorem \ref{theorem-n-logn-to-n}, the upper bond on the length of the longest chain in a hash table, we defined constant $I_\epsilon$, for $0 < \epsilon < 1$, as
\[
I_{\epsilon} = \frac{1}{1 - \epsilon} \displaystyle\int\limits_4^\infty \left(\frac{r}{\log r}\right)^{-\log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} \text{.}
\]
This integral is part of the multiplicative constant $16c_\epsilon(4 + I_\epsilon)$. To lower its value we provide a way how to estimate $I_\epsilon$.

In the original proof of theorem in \cite{DBLP:journals/jacm/AlonDMPT99} only special case $I_{\frac{1}{2}}$ was considered. Our extension to $I_\epsilon$ is motivated by the fact that we are not forced to use $\epsilon$ equal to $0.5$. If we chose other values we could obtain even smaller constant values.

Evaluation of the integral $I_\epsilon$ is split into two parts. We compare integrand of $I_\epsilon$ to a function $f(r)$ which has convergent improper integral $\int_{4}^{\infty} f(r) dr$. The function chosen here is $r^{1.5}$. The chosen function becomes greater than integrand for $r \geq 16$. In the interval $[4, 16]$ he value of integral $I_\epsilon$ is bounded by its upper Riemann sum.

For $r = 16$:
\[
	\frac{1}{16 ^ {1.5}} = \frac{1}{64}
\]

\[
	\left(\frac{16}{\log 16}\right)^{-\log \left(\frac{16}{\log 16}\right) - \log \log \left(\frac{16}{\log 16}\right)} = 4^{-2 - 1} = \frac{1}{64}
\]

Combining our estimates gains the following.
\[
\begin{split}
I_{\epsilon} 
	& \leq \frac{1}{1 - \epsilon} \left( \displaystyle \sum_{r = 4}^{15} \left(\frac{r}{\log r}\right)^{-\log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} + \int\limits_{16}^\infty \frac{1}{r^{1.5}} dr \right) \\
	& \leq \frac{1}{1 - \epsilon} \left(4.3 + \frac{1}{2}\right) = \frac{4.8}{1-\epsilon}
\end{split}
\]

\section{Parametrisation of the original proofs}
Let us present a simple parametrisation of choice of $\epsilon$ to obtain minimal value of $c_\epsilon$ and multiplicative constant. This simple but powerful technique is used later with combination of newly achieved results.

The most important step is the optimisation of the value $16 c_\epsilon (4 + I_{\epsilon})$ what is the explicit formula for the multiplicative constant. The greatest part of value of this constant is taken by the factor $c_\epsilon$. To get the lowest possible a slight modification of proof of Theorem \ref{theorem-set-onto-by-linear-transform} is needed.

The first approach is to modify the proof of Theorem \ref{theorem-set-onto-by-linear-transform} in a way that we parametrise every chosen constant in it. Values of the parameters are than set in order to get the least multiplicative constant. The optimisation itself is not performed analytically because of its complexity that is caused by the constraints that emerge. 

We created a straightforward program that assigns each parameter values from a predefined interval instead. These values are enumerated uniformly with the prescribed step. The multiplicative constant is then computed for every such assignment and the best achieved value is remembered.

We parametrised the proof by two variables $k$ and $l$. The limit on the size of set $T_0(S)$ is changed to $\frac{|S|}{k}$, for $k > 2$. The second parameter is obtained by modifying the dimension of the factor vector space $\vecspace{u}$. We change the definition of $u$ to:
\[
u = \left\lceil \log \left(\frac{2^l |S|}{\epsilon}\right) \right\rceil \text{.}
\]

As seen in the original proof the probability of the event $T(S) \neq \vecspace{t}$ can be expressed by using law of total probability as:
\[
\begin{split}
& \Prob{T(S) \neq Z_2^t} \\
    & \qquad = \Prob{T(S) \neq Z_2^t \wedge |T_0(S)| \leq \frac{|S|}{k}} + \Prob{T(S) \neq Z_2^t \wedge |T_0(S)| > \frac{|S|}{k}} \\ 
    & \qquad \leq \Prob{|T_0(S)| \leq \frac{|S|}{k}} + \Prob{T_1(T_0(S)) \neq Z_2^t \wedge |T_0(S)| > \frac{|S|}{k}} \\
    & \qquad \leq \epsilon \\
\end{split}
\]

The right side, $\epsilon$, is the wanted result which must be gained by choosing the convenient $c_{\epsilon}$. Also the estimate of $c_{\epsilon}$ was modified comparing to the original proof. By putting $c_{\epsilon}$ equal to $4\left(\frac{2}{\epsilon}\right)^{\frac{8}{\epsilon}}$ we can not get a good result. Value of $c_{\epsilon}$ is computed explicitly without any inaccurate estimations.

\begin{lemma}
\label{lemma-collision-count}
Let $T_0: U \rightarrow A$ be an arbitrary function and $S \subseteq U$. If the size of the set $|T_0(S)|$ is less than $\frac{|S|}{k}$ for $1 \leq k$ then there are at least $\frac{|S|(k - 1)}{2}$ collisions. We say that two elements $x \neq y \in S$ collide if $T_0(x) = T_0(y)$.
\end{lemma} 
\begin{proof}
Define the sequence $b_i \in \mathbb{N}_0$ for $i \in T_0(S)$ where $b_i = \left|S \cap T_0^{-1}(i)\right|$. Also note that $\sum_{i \in T_0(S)} b_i = |S|$.
The number of all colliding pairs can be computed as follows.
\[
\begin{split}
|\{ \{x, y\} \setdelim x \neq y \in S, T_0(x) = T_0(y) \}| 
	& = \frac{1}{2} \sum_{i \in T_0(S)} b_i (b_i - 1) \\ 
	& \geq \frac{|S|}{2}\left(\frac{|S|}{|T_0(S)|} - 1\right) \\
	& \geq \frac{|S|(k - 1)}{2}
\end{split}
\]
The first inequality can be obtained from Cauchy–Bunyakovsky–Schwarz inequality.
\end{proof}

For the probability of the first event, $|T_0(S)| \leq \frac{|S|}{k}$, we use the Markov inequality as in the original proof. The only difference is that the number of collisions is at least $\frac{|S|(k - 1)}{2}$ by Lemma \ref{lemma-collision-count}. The expected number remains the same $\dbinom{|S|}{2}2 ^ {-u}$.
\[
\begin{split}
\Prob{|T_0(S)| \leq \frac{|S|}{k}} 
	& \leq \Prob{d_S \geq \frac{|S|(k - 1)}{2}} \\
	& \leq \frac{\dbinom{|S|}{2}2 ^ {-u}}{\frac{|S|(k - 1)}{2}} \\
	& = \frac{|S| - 1}{(k - 1) 2^u} \\
\end{split}
\]

The second event's probability estimate is based on Theorem \ref{theorem-linear-function-set-onto} as well. Just note it is used for the set $T_0(S)$ transformation $T_1$, the source and resulting vector spaces are $\vecspace{u}$ and $\vecspace{t}$ respectively. The inverse density $\mu = 1 - \frac{|T_0(S)|}{2^u}$.

The following upper bound on the $\mu$ is very useful to be shown.
\[
\begin{split}
\mu = 1 - \frac{|T_0(S)|}{2^u} 
	& \leq 1 - \frac{|S|}{k 2 ^ u} \\
	& \leq 1 - \frac{\epsilon}{2 k 2 ^ l}
\end{split}
\]

From the assumption on the set $S$ we also have that
\begin{gather*}
	|S| \geq c_\epsilon 2^t t \\
	u = \left\lceil \log \left(\frac{2^l |S|}{\epsilon}\right) \right\rceil \geq l + \log c_\epsilon + t + \log t - \log \epsilon \text{.} \\
\end{gather*}

The bounds on the probabilities are then obtained.
\[
\begin{split}
& \Prob{T_1(T_0(S)) \neq \vecspace{t} \wedge |T_0(S)| > \frac{|S|}{k}} \\
	& \qquad \leq \mu ^ {u - t - \log t + \log \log \left(\frac{1}{\mu}\right)} \\
	& \qquad \leq \left(1 - \frac{\epsilon}{2 k 2 ^ l}\right)^{l + \log c_\epsilon + t + \log t - \log \epsilon - t - \log t + \log \log \left(\frac{1}{1 - \frac{\epsilon}{2 k 2 ^ l}}\right)} \\
	& \qquad \leq \left(1 - \frac{\epsilon}{2 k 2 ^ l}\right)^{l + \log c_\epsilon - \log \epsilon + \log \log \left(\frac{1}{1 - \frac{\epsilon}{2 k 2 ^ l}}\right)} \\
\end{split}
\]

\[
\begin{split}
& \Prob{|T_0(S)| \leq \frac{|S|}{k}} + \Prob{T_1(T_0(S)) \neq Z_2^t \wedge |T_0(S)| > \frac{|S|}{k}} \\ 
	& \qquad \leq \frac{\epsilon}{(k - 1) 2 ^ l} + \left(1 - \frac{\epsilon}{2 k 2^l}\right)^{\log c_\epsilon + l - \log \epsilon + \log \log \left(\frac{1}{1 - \frac{\epsilon}{2 k 2^l}}\right)} \\
	& \qquad \leq \epsilon \\
\end{split}
\]

The last inequality is the proved bound that is achieved by the convenient but lowest possible choice of $c_\epsilon$. From the last expression the only thing we need is to have variable $c_\epsilon$ standalone so that the minimal value of $c_\epsilon$ is found. For convenience define a new variable $\mu' = 1 - \frac{\epsilon}{2 k 2 ^l}$, just notice that $\mu'$ upper bounds $\mu$.
\[
\begin{split}
\frac{\epsilon}{(k - 1) 2 ^ l} + {\mu'}^{\log c_\epsilon + l - \log \epsilon + \log \log \left(\frac{1}{\mu'}\right)} & \leq \epsilon \\
{\mu'}^{\log c_\epsilon}{\mu'}^{l - \log \epsilon + \log \log \left(\frac{1}{\mu'}\right)} & \leq \epsilon - \frac{\epsilon}{(k - 1) 2 ^ l} \\
{\mu'}^{\log c_\epsilon} & \leq \left(\epsilon - \frac{\epsilon}{(k - 1) 2 ^ l}\right) {\mu'}^{\log \epsilon - l - \log \log \left(\frac{1}{\mu'}\right)} \\
{\log c_\epsilon} & \geq \frac{\log \left( \left( \epsilon - \frac{\epsilon}{(k - 1) 2 ^ l}\right) {\mu'}^{\log \epsilon - l - \log \log \left(\frac{1}{\mu'}\right)}\right)}{\log \mu'}  \\
\end{split}
\]

So we modified the proof of the original theorem in order to obtain a better values of $c_\epsilon$. Its value is computed for every choice of parameters $k$ and $l$ directly from the above expression. The best value of multiplicative constant obtained by the presented method is something less then 22 568 for $\epsilon = 0.91$, $k = 3.28$ and $l = 0.5$. Though the asymptotic rate for $\lpsl$ is $O(\log m \log \log m)$ with this large multiplicative constant our estimate becomes less than linear, $\Expect{\lpsl} \leq n$, for the number of elements stored approximately equal to one million.

\section{Obtaining a better multiplicative constant}
For the model of hashing linear amount of elements we used the estimates that are valid in case of hashing the super-linear number items as well. In order to obtain a better result we have to reconsider our choices. 

The previous and to be honest not very successful attempt gave us a constant suitable for hashing millions of elements. With a constant in the order of hundreds our estimates starts beating the linear one when hashing approximately 40 000 elements.

The constant $c_\epsilon$ plays the most crucial role and therefore we lower its value first. Instead of making the factor space $A$ larger than stored set $S$ we will bound it between $|B|$ and $\frac{2|S|}{2 ^ l}$ where $l \geq 1$ is a parameter. This space must still be larger than the target space $\vecspace{t}$ since we have to ensure existence of surjective mapping $T_1: A \rightarrow B$. Of course this is possible since we assume that set $S$ contains at least $c_\epsilon t 2^t$ elements.

\begin{remark}
\label{remark-better-c-e}
Let $U = \vecspace{w}$ and $B = \vecspace{t}$ and $T \in LT(U, B)$ be a random uniformly chosen linear transformation. Then for every $0 < \epsilon < 1$ there is a constant $c_\epsilon$ such that for every subset $S \subset U$, $|S| \geq c_\epsilon t 2^t$ the following holds
\[
\Prob{T(A) = \vecspace{t}} \geq 1 - \epsilon \text{.}
\]
\end{remark}
\begin{proof}
The proof we present is fully parametrised so that we can choose values of its arguments to optimise the $c_\epsilon$ and final multiplicative constant.

For every value $\frac{|S|}{2^l}$, $l \in \mathbb{R}, l \geq 1$, there is an uniquely determined number $u \in \mathbb{N}$ such that $A = \vecspace{u}$ and $\frac{|S|}{2 ^ l} \leq |A| \leq \frac{2|S|}{2 ^ l}$. As in the previous proof we will estimate the two probabilities obtained by using the law of total probability.

According to Lemma \ref{lemma-collision-count} when the image $T_0(S)$ has less than $\frac{|A|}{k}$ elements there must be at least $\frac{|S|}{2}\left(\frac{k|S|}{|A|} - 1\right)$ collisions caused by $T_0$. We used the lemma for set $S$ and fraction of resulting elements $\frac{|S|}{k'} = \frac{|A|}{k}$. Since resulting number of collisions equals $\frac{|S|}{2}(k' - 1)$ and $k' = \frac{k|S|}{|A|}$ we get the wanted result.

The bound on one of probabilities given by the law of total probability is obtained by using Markov inequality again.
\[
\begin{split}
\Prob{|T_0(S)| \leq \frac{|A|}{k}} 
	& \leq \frac{|S|(|S| - 1)}{2|A|\frac{|S|}{2}\left(\frac{k|S|}{|A|} - 1\right)} \\
	& \leq \frac{|S|}{k|S| - |A|} \\
	& \leq \frac{|S|}{k|S| - \frac{2|S|}{2^l}} \\ 
	& = \frac{2^l}{2^l k - 2} \\
\end{split}
\]

The remaining case is when $|T_0(S)| > \frac {|A|}{k}$. Define $\mu$ as the inverse density of $T_0(S)$ in space $A$. Then the assumption made implies
\[
	\mu = 1 - \frac{|T_0(S)|}{|A|} < 1 - \frac{1}{k} < 1 \text{.}
\]

\[
\begin{split}
\Prob{T(S) \neq \vecspace{t} \wedge |T_0(S)| > \frac {|A|}{k}} 
	& \leq \mu ^ {u - t - \log t + \log \log \frac{1}{\mu}} \\ 
	& = \mu ^ {\log |A| - t - \log t + \log \log \frac{1}{\mu}} \\ 
	& \leq \mu ^ {\log |S| - l - t - \log t + \log \log \frac{1}{\mu}} \\ 
	& \leq \mu ^ {\log c_\epsilon + t + \log t - l - t - \log t + \log \log \frac{1}{\mu}} \\ 
	& \leq \left(1 - \frac{1}{k}\right) ^ {\log c_\epsilon - l + \log \log \left(\frac{1}{1 - \frac{1}{k}} \right)}
\end{split}
\]

Whole probability of event $T(S) \neq Z_2^t$ must be less than $\epsilon$.
\[
\begin{split}
& \Prob{T(S) \neq \vecspace{t}} \\
	& \qquad = \Prob{T(S) \neq \vecspace{t} \wedge |T_0(S)| > \frac {|A|}{k}} + \Prob{T(S) \neq \vecspace{t} \wedge |T_0(S)| \leq \frac {|A|}{k}} \\
	& \qquad \leq \Prob{T(S) \neq \vecspace{t} \wedge |T_0(S)| > \frac {|A|}{k}} + \Prob{|T_0(S)| \leq \frac {|A|}{k}} \\
	& \qquad \leq \epsilon
\end{split}
\]

Constant $c_\epsilon$ is computed directly from the above inequality.
\[
\begin{split}
\frac{2^l}{2^l k - 2} + \left(1 - \frac{1}{k}\right)^{\log c_\epsilon + \log \log \left( \frac{1}{1 - \frac{1}{k}} \right) - l} & \leq \epsilon \\
\left(1 - \frac{1}{k}\right)^{\log c_\epsilon} & \leq \frac{\epsilon - \frac{2^l}{2^l k - 2}}{\left(1 - \frac{1}{k}\right) ^ {\log \log \left(\frac{1}{1 - \frac{1}{k}}\right) - l}} \\
\log c_\epsilon & \geq \frac{\log \left(\frac{\epsilon - \frac{2^l}{2^l k - 2}}{\left(1 - \frac{1}{k}\right) ^ {\log \log \left( \frac{1}{1 - \frac{1}{k}} \right) - l}}\right)}{\log \left(1 - \frac{1}{k}\right)}
\end{split}
\]

For some values $\epsilon$, $k$ and $l$ it may happen $\frac{2^l}{2^l k - 2} \geq \epsilon$. In this case no $c_\epsilon$ may be found using this proof. For such cases we can use the previous proof and hence the remark holds. However with this computation better results for valid choices are found. Compare values of $c_\epsilon$ obtained using this method; 17.31 (for choices $\epsilon = 0.8967$, k = 2.06, l = 2) to 67.77 (for $\epsilon = 0.98$, k = 3.28, l = 0.5) using the previous one.
\end{proof}

Now the improved estimate of $c_\epsilon$ can be used to show a better bound on the expected length of the longest chain. The novel part lies in choosing a slightly different and smaller size of the factor space $A$.
\begin{theorem}
\label{theorem-hashing-linear-amount}
When hashing a linear amount of $\alpha m$ elements into a table of size $m \geq 16$ the expected length of the longest chain is bounded by $O(\alpha \log m \log \log m)$ for $\alpha \in \left[0.5, 1 \right]$.
\end{theorem}
\begin{proof}
As in the previous cases assume hashing universe $U = \vecspace{w}$ to hash table $B = \vecspace{t}$ and factor space $A = \vecspace{u}$ where dimension $u$ is specified lately. Because of Model \ref{remark-model-uniform-linear-map-selection} we can uniformly select linear mappings $T_0: U \rightarrow A$ and surjective $T_1: A \rightarrow B$.

Our choices for $r \geq 4$:
\[
\begin{split}
u & = \lfloor \log m + \log r - \log \log r + \log \alpha + 1\rfloor \\
d & = \frac{|A|}{|S|} = \frac{2 ^ u}{\alpha m} \geq \frac{\alpha m r}{\alpha m \log r} = \frac{r}{\log r} \geq 2 \\
l & = 2 \alpha c_\epsilon r \\
\end{split}
\]

We have to verify that $|A| > |B|$.
\[
2 ^ u \geq \frac{\alpha m r }{\log r} \geq m \frac{\alpha r}{\log r} \geq m = |B|
\]

The choice for $d$ gives us the probability of event $E_2$, as stated in the proof of Remark \ref{remark-e2-probability}.
\[
\begin{split}
\mu & = 1 - \frac{|A - T_0(S)|}{|A|} \leq \frac{|S|}{|A|} = \frac{1}{d} < 1 \\
\log d & = u - \log \alpha - \log m
\end{split}
\]

\[
\begin{split}
\Prob{E_2}
	& \leq \mu ^ {u - \log m - \log \log m + \log \log \left( \frac{1}{\mu} \right)} \\
	& \leq \left(\frac{1}{d}\right) ^ {\log d + \log \alpha - \log \log m + \log \log d} \\
	& \leq \left(\frac{r}{\log r}\right) ^ {\log \log m - \log \alpha - \log d - \log \log d} \\
	& \leq \left(\frac{r}{\log r}\right) ^ {\log \log m - \log \alpha - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} \\
\end{split}
\]

Verification of validity of choice for variable $l$ follows:
\[
\begin{split}
c_\epsilon \frac{2 ^ u}{m} \log \left( \frac{2 ^ u}{m} \right) 
	& \leq 2 c_\epsilon \alpha \frac{r}{\log r} \log \left( \frac{2 \alpha r}{\log r} \right) \\
	& \leq 2 c_\epsilon \alpha \frac{r}{\log r} \log r \\
	& = 2 \alpha c_\epsilon r = l
\end{split}
\]

Because assumptions of both remarks, Remark \ref{remark-prob-l-length-chain} and Remark \ref{remark-e2-probability}, are satisfied the probability of existence of a long chain is bounded as: \[ \Prob{\lpsl \geq 2 \alpha c_\epsilon r} \leq \frac{1}{1 - \epsilon} \left(\frac{r}{\log r}\right)^{\log \log m - \log \alpha - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} \text{.}\]. From this fact we obtain a bound on the expected longest length.

\[
\begin{split}
\Expect{\lpsl}
	& = \int\limits_0^\infty \Prob{\lpsl \geq r} dr \leq 8 c_\epsilon \alpha + \int\limits_{8 c_\epsilon \alpha}^\infty \Prob{\lpsl \geq r} dr \\ 
	& = 8c_\epsilon \alpha + 2 c_\epsilon \alpha \int\limits_{4}^\infty \Prob{\lpsl \geq 2c_\epsilon \alpha r} dr \\
	& = 2c_\epsilon \alpha \left(4 + \frac{1}{1 - \epsilon} \int\limits_{4}^\infty \min\left(1, \left(\frac{r}{\log r}\right)^{\log \log m - \log \alpha - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)}\right) dr \right) \\ 
	& \leq 2c_\epsilon \alpha \left( 4 + \frac{1}{1-\epsilon}\left( 2 \log m \log \log m - 4 + I'_\alpha \right) \right) \\
\end{split}
\]

Whole bound on $\Expect{\lpsl}$ looks like:
\[
\Expect{\lpsl} \leq \frac{4c_\epsilon}{1 - \epsilon} \log m \log \log m + 2 c_\epsilon \left( \frac{I'_\alpha - 4}{1 - \epsilon} + 4 \right) \text{.}
\]

The estimate of integral $\int\limits_{4}^\infty \min \left(1, \left(\frac{r}{\log r}\right)^{\log \log m - \log \alpha - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)}\right) dr$ has to be shown. First we must realise that if the exponent is non-positive the integrand is certainly less or equal to one. If $r \geq 2 \log m \log \log m$ then $\log \left( \frac{r}{\log r} \right) \geq \log \log m$ because:
\[
\begin{split}
\frac{r}{\log r} 
	& \geq \frac{2 \log m \log \log m}{1 + \log \log m + \log \log \log m} \\
	& = \frac{2 \log m}{1 + \frac{1}{\log \log m} + \frac{\log \log \log m}{\log \log m}} \\
	& \geq \log m \text{.}
\end{split}
\]

Or equivalently:
\[
	\log \left( \frac{r}{\log r} \right) \geq \log \log m \text{.}
\]

Because this bound is valid for $m \geq 16$ we have that $r \geq 2 \log m \log \log m \geq 16$. The remaining part of the exponent \[ -\log \alpha - \log \log \left( \frac{r}{\log r} \right) \leq -\log 0.5 - \log \log \frac{16}{4} = 0 \] is not positive, as well. 

Value of $I'_\alpha$ is determined in the similar fashion as value of $I_\epsilon$ in Section \ref{section-integral-estimate}. During estimation of $I'_\alpha$ put $d(r) = \frac{r}{\log r}$ for simplicity.
\[
\begin{split}
I'_\alpha 
	& = \int_{2 \log m \log \log m}^{\infty} d(r) ^ {\log \log m - \log \alpha - \log d(r) - \log \log d(r)} dr \\
	& \leq \int\limits_{16}^{\infty} d(r) ^ {1 - \log \log d(r)} dr \\
	& \leq \displaystyle\sum_{r \in \{16, 16.1, \dots, 2048 \}} \frac{d(r) ^ {1 - \log \log d(r)}}{10} + 
		\int\limits_{2048}^{\infty} r ^ {-1.3} dr \\
	& \leq 3.01 + \frac{2048 ^ {-0.3}}{0.3} \leq 3.36
\end{split}
\]

The best estimate for $\Expect{\lpsl}$ achieved by this technique is for choices $\epsilon = 0.8$, $k = 2.26$, $l = 2$ and equals $538 \alpha \log m \log \log m + 44$.
\end{proof}
