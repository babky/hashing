\chapter{Estimating the multiplicative constant}
Improvements of the previous results of hashing with systems of linear transformations are needed since the obtained multiplicative constants are enormous. At first we decrease value of constant $c_\epsilon$ and it lower the multiplicative constant as well. Because typical load factors are lower than one we assumed hashing only linear amounts of elements with respect to the size of the hash table. In the previous chapter we showed similar asymptotic result. It has to be improved and to gain even better results.

To complete our goal we change the previous claims to fit the scheme of hashing linear amounts of elements. As already mentioned good asymptotic behaviour is not enough since we can not neglect the inherited constant factors. In the presented proofs of corresponding theorems we try to make the estimates as tight as possible.

\section{Minimising the integral part}
When we were performing last part of the proof of theorem \ref{theorem-n-logn-to-n}, the upper bond on the length of the longest chain in a hash table, we defined constant $I_\epsilon$, for $0 < \epsilon < 1$, as
\[
I_{\epsilon} = \frac{1}{1 - \epsilon} \displaystyle\int\limits_4^\infty \left(\frac{r}{\log r}\right)^{-\log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} \text{.}
\]
This integral is part of the multiplicative constant $16c_\epsilon(4 + I_\epsilon)$. To lower its value we provide a way how to estimate $I_\epsilon$.

In the original proof of theorem in \cite{DBLP:journals/jacm/AlonDMPT99} only special case $I_{\frac{1}{2}}$ was considered. Our extension to $I_\epsilon$ is motivated by the fact that we are not forced to use $\epsilon$ equal to $0.5$. If we chose other values we could obtain even smaller constant values.

Evaluation of the integral $I_\epsilon$ is split into two parts. We compare integrand of $I_\epsilon$ to a function $f(r)$ which has convergent improper integral $\int_{4}^{\infty} f(r) dr$. The function chosen here is $r^{1.5}$. The chosen function becomes greater than integrand for $r \geq 16$. In the interval $[4, 16]$ he value of integral $I_\epsilon$ is bounded by its upper Riemann sum.

For $r = 16$:
\[
	\frac{1}{16 ^ {1.5}} = \frac{1}{64}
\]

\[
	\left(\frac{16}{\log 16}\right)^{-\log \left(\frac{16}{\log 16}\right) - \log \log \left(\frac{16}{\log 16}\right)} = 4^{-2 - 1} = \frac{1}{64}
\]

Combining our estimates gains the following.
\[
\begin{split}
I_{\epsilon} 
	& \leq \frac{1}{1 - \epsilon} \left( \displaystyle \sum_{r = 4}^{15} \left(\frac{r}{\log r}\right)^{-\log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} + \int\limits_{16}^\infty \frac{1}{r^{1.5}} dr \right) \\
	& \leq \frac{1}{1 - \epsilon} \left(4.3 + \frac{1}{2}\right) = \frac{4.8}{1-\epsilon}
\end{split}
\]

\section{Setting the parameters of the system}
Let us present a simple parametrisation of choice of $\epsilon$ to obtain minimal value of $c_\epsilon$ and multiplicative constant. This simple but powerful technique is used later with combination of newly achieved results.

The most important step is the optimisation of the value $16 c_\epsilon (4 + I_{\epsilon})$ what is the explicit formula for the multiplicative constant. Greatest part of value of this constant is taken by the factor $c_\epsilon$. To get the lowest possible a slight modification of proof of theorem \ref{theorem-set-onto-by-linear-transform} is needed.

The first approach is to modify the proof of the theorem \ref{theorem-set-onto-by-linear-transform} in a way that we parametrise every chosen constant in it. Values of the parameters are than set in order to get the least multiplicative constant. The optimisation itself is not performed analytically because of its complexity that is caused by the constraints that emerge. 

We created a straightforward program that assigns each parameter values from a predefined interval instead. These values are enumerated uniformly with the prescribed step. The multiplicative constant is then computed for every such assignment and the best achieved value is remembered.

We parametrised the proof by two variables $k$ and $l$. The limit on the size of set $T_0(A)$ is changed to $\frac{|A|}{k}$, for $k > 2$. The second parameter is obtained by modifying the dimension of the factor vector space $\vecspace{u}$. We change the definition of $u$ to:
\[
u = \left\lceil \log \left(\frac{2^l |A|}{\epsilon}\right) \right\rceil \text{.}
\]

As seen in the original proof the probability of the event $T(A) \neq \vecspace{t}$ can be expressed by using law of total probability as:
\[
\begin{split}
& \Prob{T(A) \neq Z_2^t} \\
    & \qquad = \Prob{T(A) \neq Z_2^t \wedge |T_0(A)| \leq \frac{|A|}{k}} + \Prob{T(A) \neq Z_2^t \wedge |T_0(A)| > \frac{|A|}{k}} \\ 
    & \qquad \leq \Prob{|T_0(A)| \leq \frac{|A|}{k}} + \Prob{T_1(T_0(A)) \neq Z_2^t \wedge |T_0(A)| > \frac{|A|}{k}} \\
    & \qquad \leq \epsilon \\
\end{split}
\]

The right side, $\epsilon$, is the wanted result which must be gained by choosing the convenient $c_{\epsilon}$. Also the estimate of $c_{\epsilon}$ was modified comparing to the original proof. By putting $c_{\epsilon}$ equal to $4\left(\frac{2}{\epsilon}\right)^{\frac{8}{\epsilon}}$ we can not get a good result. Value of $c_{\epsilon}$ is computed explicitly without any inaccurate estimations.

\begin{lemma}
\label{lemma-collision-count}
Let $T_0: U \rightarrow W$ be a function and $A \subseteq U$. If the size of the set $|T_0(A)|$ is less than $\frac{|A|}{k}$ for $1 \leq k$ then there are at least $\frac{|A|(k - 1)}{2}$ collisions. By collision of two elements $x \neq y \in A$ the unordered pair $\{ x, y \}$ such that $T_0(x) = T_0(y)$ is understood.
\end{lemma} 
\begin{proof}
Define the sequence $b_i \in \mathbb{N}_0$ for $i \in T_0(A)$ where $b_i = \left|A \cap T_0^{-1}(i)\right|$. Also note that $\sum_{i \in T_0(A)} b_i = |A|$.
The number of all colliding pairs can be computed as follows.
\[
\begin{split}
|\{ \{x, y\} \setdelim x \neq y \in A, T_0(x) = T_0(y) \}| 
	& = \frac{1}{2} \sum_{i \in T_0(A)} b_i (b_i - 1) \\ 
	& \geq \frac{|A|}{2}\left(\frac{|A|}{|T_0(A)|} - 1\right) \\
	& \geq \frac{|A|(k - 1)}{2}
\end{split}
\]
The first inequality can be obtained from Cauchy–Bunyakovsky–Schwarz inequality.
\end{proof}

For the probability of the first event, $|T_0(A)| \leq \frac{|A|}{k}$, we use the Markov inequality as in the original proof. The only difference is that the number of collisions is at least $\frac{|A|(k - 1)}{2}$ by lemma \ref{lemma-collision-count}. The expected number remains the same $\dbinom{|A|}{2}2 ^ {-u}$.
\[
\begin{split}
\Prob{|T_0(A)| \leq \frac{|A|}{k}} 
	& \leq \Prob{d_a \geq \frac{|A|(k - 1)}{2}} \\
	& \leq \frac{\dbinom{|A|}{2}2 ^ {-u}}{\frac{|A|(k - 1)}{2}} \\
	& = \frac{|A| - 1}{(k - 1) 2^u} \\
\end{split}
\]

The second event's probability estimate is based on the theorem \ref{theorem-linear-function-set-onto} as well. Just note it is used for the set $T_0(A)$ transformation $T_1$, the source and resulting vector spaces are $\vecspace{u}$ and $\vecspace{t}$ respectively. The inverse density $\mu = 1 - \frac{|T_0(A)|}{2^u}$.

The following upper bound on the $\mu$ is very useful to be shown.
\[
\begin{split}
\mu = 1 - \frac{|T_0(A)|}{2^u} 
	& \leq 1 - \frac{|A|}{k 2 ^ u} \\
	& \leq 1 - \frac{\epsilon}{2 k 2 ^ l}
\end{split}
\]

From the assumption on the set $A$ we also have that
\begin{gather*}
	|A| \geq c_\epsilon 2^t t \\
	u = \left\lceil \log \left(\frac{2^l |A|}{\epsilon}\right) \right\rceil \geq l + \log c_\epsilon + t + \log t - \log \epsilon \text{.} \\
\end{gather*}

\[
\begin{split}
& \Prob{T_1(T_0(A)) \neq \vecspace{t} \wedge |T_0(A)| > \frac{|A|}{k}} \\
	& \qquad \leq \mu ^ {u - t - \log t + \log \log \left(\frac{1}{\mu}\right)} \\
	& \qquad \leq \left(1 - \frac{\epsilon}{2 k 2 ^ l}\right)^{l + \log c_\epsilon + t + \log t - \log \epsilon - t - \log t + \log \log \left(\frac{1}{1 - \frac{\epsilon}{2 k 2 ^ l}}\right)} \\
	& \qquad \leq \left(1 - \frac{\epsilon}{2 k 2 ^ l}\right)^{l + \log c_\epsilon - \log \epsilon + \log \log \left(\frac{1}{1 - \frac{\epsilon}{2 k 2 ^ l}}\right)} \\
\end{split}
\]

\[
\begin{split}
& \Prob{|T_0(A)| \leq \frac{|A|}{k}} + \Prob{T_1(T_0(A)) \neq Z_2^t \wedge |T_0(A)| > \frac{|A|}{k}} \\ 
	& \qquad \leq \frac{\epsilon}{(k - 1) 2 ^ l} + \left(1 - \frac{\epsilon}{2 k 2^l}\right)^{\log c_\epsilon + l - \log \epsilon + \log \log \left(\frac{1}{1 - \frac{\epsilon}{2 k 2^l}}\right)} \\
	& \qquad \leq \epsilon \\
\end{split}
\]

The last inequality is the proved bound that is achieved by the convenient but lowest possible choice of $c_\epsilon$. From the last expression the only thing we need is to have variable $c_\epsilon$ standalone so that the minimal value of $c_\epsilon$ is found. For convenience define a new variable $\mu' = 1 - \frac{\epsilon}{2 k 2 ^l}$.
\[
\begin{split}
\frac{\epsilon}{(k - 1) 2 ^ l} + {\mu'}^{\log c_\epsilon + l - \log \epsilon + \log \log \left(\frac{1}{\mu'}\right)} & \leq \epsilon \\
{\mu'}^{\log c_\epsilon}{\mu'}^{l - \log \epsilon + \log \log \left(\frac{1}{\mu'}\right)} & \leq \epsilon - \frac{\epsilon}{(k - 1) 2 ^ l} \\
{\mu'}^{\log c_\epsilon} & \leq \left(\epsilon - \frac{\epsilon}{(k - 1) 2 ^ l}\right) {\mu'}^{\log \epsilon - l - \log \log \left(\frac{1}{\mu'}\right)} \\
{\log c_\epsilon} & \geq \frac{\log \left( \left( \epsilon - \frac{\epsilon}{(k - 1) 2 ^ l}\right) {\mu'}^{\log \epsilon - l - \log \log \left(\frac{1}{\mu'}\right)}\right)}{\log \mu'}  \\
\end{split}
\]

So we modified the proof of the original theorem in order to obtain a better values of $c_\epsilon$. Its value is computed for every choice of parameters $k$ and $l$ directly from the above expression. The best value obtained by presented method is less then 22 568 for $\epsilon = 0.91$, $k = 3.28$ and $l = 0.5$. Though the asymptotic rate of the growth is $O(\log n \log \log n)$ with this large multiplicative constant the estimate becomes less than linear estimate, $\Expect{\lpsl} \leq n$, for the number of elements stored approximately equal to one million.

\section{Obtaining an even better multiplicative constant}
For the model of hashing linear amount of elements we only used the estimates valid also in the case of hashing the super-linear count of items. Now in order to obtain a better result we have to reconsider our choices. The ideas remain the same but we will change some claims to suit our needs.

The previous and not very successful attempt gave us a constant suitable for hashing millions of elements. With a constant in the order of hundreds our estimates starts beating the linear one when hashing around 40 000 elements.

The constant $c_\epsilon$ plays the most crucial role. We try to lower its value first. Instead of making the factor space $W$ larger than the set $A$ we will bound it between $\frac{|A|}{4}$. But it still must be larger than the target space $Z_2^t$. Of course this is possible since the set $A$ contains at least $t2^t$ elements.

\begin{remark}
For every convenient $\epsilon$ there is a constant $c_\epsilon$ such that for every subset $A$, $|A| \geq c_\epsilon t 2^t$ of the source space $V$ and for an uniformly selected linear transformation $T$ to the target space $Z_2^t$ the probability of mapping $A$ to the whole target space is at least $1-\epsilon$:
\begin{displaymath}
P(T(A) = Z_2^t) \geq 1 - \epsilon \textit{.}
\end{displaymath}
\end{remark}
\begin{proof}
We present the proof with all the possible parameters that can have their value chosen to optimise the value of $c_\epsilon$. According to these parameters the interval for $\epsilon$ may be found.

For every value $\frac{|A|}{2^l}$ there is a power of two $2^u = |W|$ less or equal to $\frac{2|A|}{2^l}$. The value of $l$ will be non-negative and so $|W| \leq |A|$. As in the previous modifications we will estimate the two probabilities obtained by using the law of total probability.

According to the lemma \ref{lemma-collision-count} when the result $T_0(A)$ has less than $\frac{|W|}{k}$ elements there must be at least $\frac{|A|}{2}\left(\frac{k|A|}{|W|} - 1\right)$ collisions caused by it.

\begin{displaymath}
P(|T_0(A)| \geq \frac{|W|}{k}) \leq \frac{|A|(|A| - 1)}{2|W|\frac{|A|}{2}\left(\frac{k|A|}{|W|} - 1\right)} \leq \frac{|A|}{k|A| - |W|} \leq \frac{|A|}{k|A| - \frac{2|A|}{2^l}} \leq \frac{2^l}{2^l k - 2}
\end{displaymath}

The remaining case is when $|T_0(A)| \geq \frac {|W|}{k}$.

\begin{displaymath}
\alpha = 1 - \frac{|T_0(A)|}{|W|} \leq 1 - \frac{1}{k}
\end{displaymath}

\begin{displaymath}
P(T(A) \neq Z_2^t) \leq \left(1 - \frac{1}{k}\right)^{\log c_\epsilon + \log \log \left( \frac{1}{1 - \frac{1}{k}} \right) - l}
\end{displaymath}

The sum of both cases is greater than the whole probability of event $T(A) \neq Z_2^t$ and must be less than $\epsilon$.

\begin{displaymath}
\begin{split}
\frac{2^l}{2^l k - 2} + \left(1 - \frac{1}{k}\right)^{\log c_\epsilon + \log \log \left( \frac{1}{1 - \frac{1}{k}} \right) - l} & \leq \epsilon \\
\left(1 - \frac{1}{k}\right)^{\log c_\epsilon} & \leq \frac{\epsilon - \frac{2^l}{2^l k - 2}}{\log \log \left( \frac{1}{1 - \frac{1}{k}} \right) - l} \\
\log c_\epsilon & \geq \frac{\log \left(\frac{\epsilon - \frac{2^l}{2^l k - 2}}{\log \log \left( \frac{1}{1 - \frac{1}{k}} \right) - l}\right)}{\log \left(1 - \frac{1}{k}\right)}
\end{split}
\end{displaymath}

For some values $\epsilon$, $k$ and $l$ it may happen $\frac{2^l}{2^l k - 2} \geq \epsilon$. In this case no $c_\epsilon$ may be found using this proof. However with this computation slightly better results for valid choices are found. Compare the value 17.31 ($\epsilon = 0.8967$) to 67.77 ($\epsilon = 0.98$).
\end{proof}

Now we use this approach to prove a better bound on the expected length of the longest chain. The novel part lies in choosing a slightly different and smaller size of the factor space $A$.
\begin{theorem}
\label{elpsl-hashing-linear-amount}
When hashing a linear amount of $\alpha_f n$ elements into a table of size $n$ the expected length of the longest chain is bounded by $O(\alpha_f \log n \log \log n)$.
\end{theorem}
\begin{proof}
Our choices for $r \geq 4$:
\begin{displaymath}
\begin{split}
l & = \lfloor \log n + \log r - \log \log r + \log \alpha_f + 1\rfloor \\
d & = \frac{2^l}{\alpha_f n} \geq \frac{\alpha_f n r}{\alpha_f n \log r} = \frac{r}{\log r} \geq 2 \\
t & = 2 \alpha_f c_\epsilon r \\
\end{split}
\end{displaymath}

The choice for $d$ gives us the probability of the event $E_2$.
\begin{displaymath}
\begin{split}
\alpha & = 1 - \frac{|A - h_1(S)|}{|A|} \leq \frac{|S|}{|A|} = \frac{1}{d} \\
\log d & = l - \log \alpha_f - \log n \geq l - \log n \\
P(E_2) 
	& \leq \alpha^{l - \log n - \log \log n + \log \log \left( \frac{1}{\alpha} \right)} \\
	& \leq \left( \frac{1}{d} \right)^{\log d - \log \log n + \log \log d} \\
	& \leq \left(\frac{r}{\log r}\right)^{\log \log n - \log d - \log \log d} \\
	& \leq \left(\frac{r}{\log r}\right)^{\log \log n - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} \\
\end{split}
\end{displaymath}

The verification of validity of choice for the variable $t$ follows:
\begin{displaymath}
\begin{split}
c_\epsilon \frac{2^l}{n} \log \left( \frac{2^l}{n} \right) 
	& \leq 2 c_\epsilon \alpha_f \frac{r}{\log r} \log \left( \frac{2 \alpha_f r}{\log r} \right) \\
	& \leq 2 c_\epsilon \alpha_f \frac{r}{\log r} \log r = 2 c_\epsilon \alpha_f r = t
\end{split}
\end{displaymath}

Thus $P(lpsl \geq 2 \alpha_f c_\epsilon r) \leq \frac{1}{1 - \epsilon} \left(\frac{r}{\log r}\right)^{\log \log n - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)}$. From this fact we obtain a bound on the expected longest length.

\begin{displaymath}
\begin{split}
E lpsl 
	& = \int\limits_0^\infty P(lpsl \geq t) dt \leq 2 c_\epsilon 4 + \int\limits_{8 c_\epsilon}^\infty P(lpsl \geq t) dt \\ 
	& = 8c_\epsilon + 2 c_\epsilon \int\limits_{4}^\infty P(lpsl \geq 2c_\epsilon r) dr \\
	& = 2c_\epsilon \left(4 + \frac{1}{1 - \epsilon} \int\limits_{4}^\infty \min\left(1, \left(\frac{r}{\log r}\right)^{\log \log n - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)}\right) dr \right) \\ 
	& \leq 2c_\epsilon \left( 4 + \frac{1}{1-\epsilon}\left( 2 \log n \log \log n - 4 + I \right) \right) \\
\end{split}
\end{displaymath}

For the integral $I$ estimation for simplicity we will denote the value $d = \frac{r}{\log r}$.
\begin{displaymath}
I = \int\limits_{2 \log n \log \log n}^\infty d ^ {\log \log n - \log d - \log \log d} dr \leq 14
\end{displaymath}

The whole bound looks like:
\begin{displaymath}
E lpsl \leq \frac{4c_\epsilon}{1 - \epsilon} \log n \log \log n + 2 c_\epsilon \left( \frac{I - 4}{1 - \epsilon} + 4 \right) \textit{.}
\end{displaymath}

The estimate of integral $\int\limits_{4}^\infty \left(\frac{r}{\log r}\right)^{\log \log n - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} dr$ remains to be unveiled. First we must realise that the exponent must be negative in order to get the integrand below 1. And it is sufficient to have $\log \left( \frac{r}{\log r} \right) > \log \log n$. Our choice is to put $r > 2 \log n \log \log n$:
\begin{displaymath}
\begin{split}
d & \geq \log n \\
\frac{r}{\log r} & \geq \log n \\
\frac{r}{\log r} & \geq \frac{2 \log n \log \log n}{1 + \log \log n + \log \log \log n} \\
	& = \frac{2 \log n}{1 + \frac{1}{\log \log n} + \frac{\log \log \log n}{\log \log n}} \\
	& \geq \log n \\
\end{split}
\end{displaymath}

Which is valid for $n \geq 16$. The value of $I$ is determined in the similar fashion as in the previous case. The best estimate achieved is 538 $\log n \log \log n$ + 2~897.
\end{proof}

\section{Better hashing schemes}
The expected length of the longest chain gives us the generic hint when we should rehash the table to guarantee the worst case time for the member operation. The lower the value $E lpsl$ is the better worst case bound is achieved. The probability of event that there is chain exceeding the value $c E lpsl$ is bounded by $\frac{1}{c}$. This is a straightforward use of the Markov inequality. 

The probability bound of existence of a long chain has two important consequences:
\begin{itemize}
\item At most the fraction $\frac{1}{c}$ of all the functions cause long chains; longer than $c E lpsl$. The probability of rehashing for the uniform function's choice is then $\frac{1}{c}$.
\item When rehashing; when a long chain appeared; to guarantee the bound of $c E lpsl$ the probability of finding a suitable function is $1 - \frac{1}{c}$. Note that this holds under the assumption of the uniform choice of an universal function.
\end{itemize}

Mentioned probability bound computed from the expected value can be improved by direct use of the cumulative density function of the variable $lpsl$. We already found and consequently improved it in the previous theorems. To sum up we can say that the approach with the expected length is more general but achieves worse performance. Note that whenever we know the probability density function we are able to compute the expected value.

\section{Better bound estimate for the systems of linear functions}
\label{section-linear-systems-linear-amount-constant-estimate}
From now on we concentrate on obtaining a bound on the length of the chain if we allow the probability of rehashing $\frac{1}{c}$. The corresponding bound is computed from the density function.

\begin{displaymath}
\begin{split}
P(lpsl \geq 2\alpha_f c_\epsilon r) & \leq \frac{1}{c} \\ 
\frac{1}{1 - \epsilon}\left(\frac{r}{\log r}\right)^{\log \log n - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} & \leq \frac{1}{c}
\end{split}
\end{displaymath}
Let us remark that the probability estimate works for $r \geq 4$ only, proof of theorem \ref{elpsl-hashing-linear-amount}.

Let $d = k \log n$ and $d \leq \frac{r}{\log r}$ for a positive constant $k$. When the exponent of function $d^{\log \log n -\log d - \log \log d}$ is negative it is decreasing and the previous inequality holds if
\begin{displaymath}
d ^ {\log \log n - \log d -\log \log d} \leq \frac{1 - \epsilon}{c}\textit{.}
\end{displaymath}

As first for the exponent we have:
\begin{displaymath}
\begin{split}
\log & \log n - \log (k \log n) - \log \log (k \log n) \\
	& = \log \log n - \log k - \log\log n - \log \log (k \log n) \\
	& \leq - \log k - \log \log n \\
\end{split}
\end{displaymath}
The last inequality holds for $\log (k \log n) \geq 1$ and this is equivalent to $n \geq 2^{\frac{2}{k}}$.

We selected $d$ as a lower bound for $\frac{r}{\log r}$. Now we need to find a suitable value for $r$. Putting $r = 2 d \log d$ we need to confirm $d$ is a lower bound. For every $d$ greater or equal to $2$ the estimate works
\begin{displaymath}
\frac{r}{\log r} = \frac{2 d \log d}{1 + \log d + \log \log d} \geq d \textit{.}
\end{displaymath}
The assumption for the probability estimate used was $r \geq 4$ which gets fulfilled by choosing $d \geq 2$ as well.

And we explicitly compute the dependence of $r$ on $n$:
\begin{displaymath}
r = 2 d \log d = 2 k \log n (\log k + \log \log n) = 2 k \log n \log \log n + 2 k \log k \log n
\end{displaymath}



Since $d = k \log n$ for the probability we want:
\begin{displaymath}
\begin{split}
d ^ {\log \log n - \log d -\log \log d} & \leq \frac{1 - \epsilon}{c} \\
\left(k \log n\right)^{-\log k -\log \log n} & \leq \frac{1 - \epsilon}{c} \\
\end{split}
\end{displaymath}
This gives us the lowest $n$ when our bound is valid. 

Also note that all the assumptions for the claims used are met. The bound for the fixed probability $\frac{1}{c}$ when the table is rehashed should be optimised. This bound is obtained as:
\begin{displaymath}
2 \alpha_f c_\epsilon r = 4 c_\epsilon k \alpha_f (\log n \log \log n + \log k \log n)
\end{displaymath}
And the best estimate is for the minimum $c_\epsilon$ which from the above section is 17.31 for the $\epsilon = 0.8967$.

And for example by fixing $k = 1$ and $c = 2$ with the best $\epsilon$ one gets:
\begin{displaymath}
\begin{split}
\left(\frac{1}{\log n}\right)^{\log \log n} & \leq \frac{1 - \epsilon}{2} \\
\end{split}
\end{displaymath}

The estimate is certainly valid for $n \geq 32$. The bound's multiplicative constant without the load factor is $17.31 . 1 . 4$ which is less than 70. This bound becomes better than linear ($n$) for $n$ approximately equal to 3 000. But we were not forced to using $k = 1$, we assumed only the positiveness of $k$. So by choosing a lower value we obtain a better bound. The smaller value of $k$ postpones the validity of our estimates to greater values of $n$.

For small hash tables the best bound is achieved when the value of $n$ when our estimates are valid is the same as the value obtained by the above estimate. This value is for $k = 0.2766$, $n \geq 572$ and the multiplicative constant is $19.152$. This value has also been computed by a simple computer program.
