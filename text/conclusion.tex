\chapter{Conclusion}
In this work we present a model of universal hashing which preserves the constant expected length of a chain. The running time of the find operation is then $O(1)$, too. The model uses the system of linear transformations and exploits its remarkable properties shown in \cite{DBLP:journals/jacm/AlonDMPT99}. In addition, these results are substantially improved so that they allow the construction of the model. We are also able to show that the expected amortised running time of the insert and delete operations is constant. Not only that, our model bounds the worst case running time of the find operation in $O(\log m \log \log m)$. 

Because the model is based on the system of linear transformations, the time to compute the hash value of an element is worsened. The solution that we propose is to store once computed hash values within the object. This optimisation takes the advantage of the warranties provided by the model, if the find operation is dominant. 

\section{Future Work}
There are many ways how to improve the model. For example, we can go the way of the tighter estimates. Although, it may also be more interesting to describe the behaviour of double hashing when it is used with a universal class of functions. Combined with the system of linear transformations it may be possible to obtain a similar worst case bound without violating the expected running times. 

Our next option is to use ideas of perfect hashing. Every chain may be represented by a hash table allowed to have a small load factor. Whenever the elements in the bucket can not be accessed in a constant time, then it might be possible to rehash only the small table instead of the large one. This approach may bring another optimisation.

Another brilliant idea comes from the area of load balancing. We can hash by two functions simultaneously and a newly stored element is placed into a smaller bucket. The find operation has to search in both buckets associated with an element. However, as stated in \cite{1076315} the expected worst case time in classic hashing is then substantially better and the expected complexity is preserved.

Unfortunately, the thesis does not show the experimental results. A high quality benchmark of the model is required. The benchmark needs to be performed with and without the mentioned cache optimisation. This should show the influence of the linear system on the running times. The benchmark has to show when the warranty is needed in dependence on the operations composition and the input distribution. If we try using inputs created by a random number generator, then they are uniformly distributed. So the obtained chains are short even for classic hashing. On the other hand such good inputs are seldom present. The real inconvenient inputs should be another output of the benchmark.When the chains are longer and the find operation is the most frequent one, then it is convenient to have a worst case warranty especially for large number of stored elements. The question is, how frequent the find operation has to be when compared to the modifying ones.

Of course, usage of simpler classes brings faster times of computing the hash codes. Linear classes are quite similar to each other. Maybe we could find a correspondence allowing the results found for the class of linear transformations to be brought into a faster class.

Models providing a reasonable worst case warranty with a good expected time complexity may be a suitable solution for various set representation problems. Current models of hashing may provide such qualities when enriched by simple rules. Also the approach of relaxing the models providing warranties may be helpful when achieving similar results.
