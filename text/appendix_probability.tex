\chapter{Facts Regarding Probability}
In the appendix we summarise some facts regarding the probability theory. We show the basic definitions and prove the statements used in the work.

First we discuss the discrete probability and discrete random variables. Then we move to continuous random variables. Finally we show the Markov's and the Chebyshev's inequality.

Whenever we perform a \emph{random experiment} we get a random outcome -- a \emph{random event} occurred. The probability theory models the uncertainty included in random experiments and deals with the probabilities of the random events. For example if the coin toss is the random experiment, then the set $\{\text{roll}, \text{die}\}$ contains all the possible outcomes. If the coin is fair, they share the same probability of occurrence.

\begin{definition}[Probability space, elementary event]
We refer to the set $\Omega$ consisting of all the possible outcomes of a random experiment, $\Omega = \{ \omega_1, \dots, \omega_n \}$, as to the \emph{probability space}. Its elements $\omega_i$, $i \in \{1, \dots, n\}$, are the \emph{elementary events}. The probability of every elementary event $\omega_i$, $i \in \{1, \dots, n\}$, equals $p_i$, $0 \leq p_i \leq 1$, and is written as: \[ \Prob{\omega_i} = p_i \text{.} \] The sum of the probabilities of the elementary events must be equal to one:
\[
\displaystyle\sum_{i = 1}^{n} p_i = 1 \text{.}
\]
\end{definition}

In general the probabilities of the elementary events need not to be the same. However, this assumption is frequently satisfied and then $p_i = \frac{1}{n}$ for $i = 1, \dots, n$.

\begin{definition}[Event, probability]
The set $A = \{a_1, \dots, a_m \}\subseteq \Omega$, containing only elementary events, is called a \emph{compound event}. The \emph{probability of the compound event $A$} is \[ \Prob{A} = \displaystyle\sum_{\omega \in A} \Prob{\omega} \text{.} \] If the probabilities of the elementary events are the same, then 
\[
	\Prob{A} = \frac{|A|}{|\Omega|} = \frac{m}{n} \text{.}
\]
\end{definition}

\begin{definition}[Complementary event, the certain event, the impossible event]
Let $A \subseteq \Omega$ be an event. The event $\Omega - A$ is the \emph{complementary event} of the event $A$. The \emph{certain event} is the probability space $\Omega$ and its complementary event $\emptyset$ is the \emph{impossible event}.
\end{definition}

\begin{corollary}[Probability of the complementary event]
If $A \subseteq \Omega$ is an event then the probability of the complementary event is 
\[ 
	\Prob{\Omega - A} = 1 - \Prob{A} \text{.} 
\] 
\end{corollary}
\begin{proof}
Follows from the definition of probability:
\[
\begin{split}
1 
	& = \Prob{\Omega} = \Prob{(\Omega - A) \cup A} \\
	& = \displaystyle\sum_{\omega \in \Omega - A} \Prob{\omega} + \displaystyle\sum_{\omega \in A} \Prob{\omega} \\
	& = \Prob{\Omega - A} + \Prob{A} \text{.}
\end{split}
\]
\end{proof}

We can compute the size of the set, consisting of elementary events, if we know the event's probability and the size of the probability space.

\begin{lemma}
Let $A \subseteq \Omega$ be an event such that $\Prob{A} = p$. If the probabilities of the elementary events are the same, then $|A| = p |\Omega|$.
\end{lemma}
\begin{proof}
This statement is a direct use of the definition of the probability of the event $A$.
\[
p = \Prob{A} = \frac{|A|}{|\Omega|}
\]
\end{proof}

\begin{definition}[Intersection]
Let $A, B \subseteq \Omega$ be events. The \emph{event that both $A$ and $B$ occur} is denoted by $A, B$ and equals the event $A \cap B$. The probability of the event $A, B$ is
\[
\Prob{A, B} = \Prob{A \cap B} \text{.}
\]
\end{definition}

\begin{definition}[Disjoint events]
Let $A, B \subseteq \Omega$ be events. Events $A$ and $B$ are \emph{disjoint} if $A \cap B = \emptyset$. The corresponding compound probability then equals
\[
\Prob{A, B} = \Prob{\emptyset} = 0 \text{.}
\]
\end{definition}

\begin{definition}[Independent events]
Let $A, B \subseteq \Omega$ be events. Events $A$ and $B$ are \emph{independent} if
\[
\Prob{A, B} = \Prob{A}\Prob{B} \text{.}
\]
\end{definition}

The motivation of the following definition is a way how to compute the probability of event $A$ inside the restricted probability space $B$.
\begin{definition}[Conditional probability]
Let $A, B \subseteq \Omega$ be events and assume that $\Prob{B} \neq 0$. Then the \emph{conditional probability} of event $A \mid B$ is defined as
\[
\Prob{A \mid B} = \frac{\Prob{A, B}}{\Prob{B}} \text{.}
\]
\end{definition}

\begin{lemma}
\label{lemma-conditional-probability-event-estimate}
If $A, B \subseteq \Omega$ are non-disjoint events such that $\Prob{B} \neq 0$, then $\Prob{B} \leq \frac{\Prob{A}}{\Prob{A \mid B}}$.
\end{lemma}
\begin{proof}
From our assumptions we have that the conditional probability of the event $A \mid B$ is defined and positive. To prove the lemma the definition of the conditional probability and the straightforward bound on the compound probability, $\Prob{A, B}~\leq~\Prob{A}$, is used
\[
\Prob{B} = \frac{\Prob{A, B}}{\Prob{A \mid B}} \leq \frac{\Prob{A}}{\Prob{A \mid B}} \text{.}
\]
\end{proof}

\begin{remark}
Let $A, B \subseteq \Omega$ be events with $B \neq \emptyset$. The events are independent if and only if $\Prob{A \mid B} = \Prob{A}$.
\end{remark}
\begin{proof}
If we assume the independence of the two events we have
\[
\Prob{A \mid B} = \frac{\Prob{A, B}}{\Prob{B}} = \frac{\Prob{A}\Prob{B}}{\Prob{B}} = \Prob{A} \text{.}
\]
The reverse implication holds since
\[
\Prob{A, B} = \Prob{A \mid B}\Prob{B} = \Prob{A}\Prob{B} \text{.}
\]
\end{proof}

\begin{lemma}{Properties of Probability}
\begin{enumerate}
\item[(1)] $\Prob{\emptyset} = 0$
\item[(2)] $\Prob{\Omega} = 1$
\item[(3)] $0 \leq \Prob{A} \leq 1$
\item[(4)] $\Prob{A \cup B} = \Prob{A} + \Prob{B} - \Prob{A \cap B}$
\end{enumerate}
\end{lemma}

\begin{theorem}[The Law of Total Probability]
\label{theorem-law-of-total-probability}
Let $A \subseteq \Omega$ be an event and $\Omega_1, \dots, \Omega_k$ be a decomposition of the probability space $\Omega$ such that it does not contain the impossible event. Then
\[
\Prob{A} = \displaystyle\sum_{i = 1}^{k} \Prob{A \mid \Omega_i} \Prob{\Omega_i} \text{.}
\]
\end{theorem}
\begin{proof}
We perform a simple computation to decompose the event $A$. The decomposed event is then rewritten using the definition of the conditional probability.
\[
\begin{split}
\Prob{A}
	& = \Prob{A \cap \Omega} \\
	& = \displaystyle\sum_{i = 1}^{k} \Prob{A, \Omega_i} \\
	& = \displaystyle\sum_{i = 1}^{k} \Prob{A \mid \Omega_i} \Prob{\Omega_i} \\
\end{split}
\]
\end{proof}

\begin{definition}[Discrete random variable]
Let $\Omega$ be a probability space. Then \emph{discrete random variable} $X$ is a function $X: \Omega \rightarrow \mathbb{N}$. Probability of the event $X = x$ equals
\[
\Prob{X = x} = \displaystyle\sum_{\substack{\omega \in \Omega \\ X(\omega) = x}} \Prob{\omega} \text{.}
\]
\end{definition}

In the continuous probability we usually assume that the vector space $\Omega$ is an uncountable set.
\begin{definition}[Continuous random variable, probability density function, cumulative probability density function]
\emph{Continuous random variable} $X$ is a function $X: \Omega \rightarrow \mathbb{R}$. 

Function $F: \mathbb{R} \rightarrow \left[0, 1\right]$ is the \emph{cumulative probability density function} of the random variable $X$ if the following conditions are satisfied
\begin{enumerate}
\item[(1)] $\lim\limits_{t \rightarrow -\infty} F(t) = 0$,
\item[(2)] $\lim\limits_{t \rightarrow \infty} F(t) = 1$.
\item[(3)] The function $F$ is non-decreasing.
\item[(4)] $F(t) = \Prob{X \leq t} $ for every $t \in \mathbb{R}$.
\end{enumerate}

Function $f: \mathbb{R} \rightarrow \mathbb{R}^{*}$ such that $f(t) = F'(t)$ for every $t \in \mathbb{R}$ is the \emph{probability density function} of the random variable $X$. The probability density function $f$ satisfies that
\begin{enumerate}
\item[(1)] $\int\limits_{-\infty}^{\infty} f(t) \ dt = 1$,
\item[(2)] $\int\limits_{-\infty}^{t} f(x) \ dx = F(t)$.
\end{enumerate}
\end{definition}

More formal definitions of random variables use the Lebesgue measure and Borel sets to measure the probability $\Prob{X = x}$. However previous definitions are sufficient for our needs.

If $X$ is a continuous random variable with the cumulative density function $F$ then the probability of $X \in \left[a, b\right]$ equals
\[
\Prob{a \leq X \leq b} = F(b) - F(a) = \int\limits_{a}^{b} f(x) \ dx \text{.}
\]

The motivation for the definition of the probability density function is the fact that it corresponds to the probability of $X$ being equal to a singleton. We see that for every $\epsilon > 0$ value $\epsilon f(b)$ approximates $\Prob{b - \epsilon \leq X \leq b}$, if it is small enough. However for continuous variables the event $X = b$ is impossible unless $F$ is in-continuous in $b$ and thus $f(b) = \infty$.	

After observing the previous fact we can rewrite $F(t)$ to the form:
\[
F(t) = \Prob{X \leq t} = \int\limits_{-\infty}^{t} \Prob{X = t} dt \text{.}
\]

\begin{definition}[Expected value]
Let $X$ be a discrete random variable. Its \emph{expected value}, $\Expect{X}$, is defined as
\[
\Expect{X} = \displaystyle\sum_{x \in \mathbb{N}} x \Prob{X = x} \text{.}
\]

In the continuous case, assume that $X$ is a continuous random variable, then its \emph{expected value}, $\Expect{X}$, is defined as
\[
\Expect{X} = \int\limits_{-\infty}^{\infty} x \Prob{X = x} dx \text{.}
\]
\end{definition}

\begin{definition}[Variance]
Let $X$ be a random variable. Its variance, $\Variance{X}$, is defined as \[ \Variance{X} = \Expect{(X - \Expect{X}) ^ 2} \text{.} \]
\end{definition}

We only show the basic properties of the expected value and variance without the proof. 
\begin{lemma}
Assume that $X$ and $Y$ are random variables and $a, b, c \in \mathbb{R}$. Then
\label{lemma-expected-value-properties}
\begin{enumerate}
\item[(1)] $\Expect{aX + bY + c} = a\Expect{X} + b\Expect{Y} + c$
\item[(2)] $\Variance{aX + bY + c} = a ^ 2 \Variance{X} + b ^ 2 \Variance{Y}$
\end{enumerate}
\end{lemma}

\begin{definition}[Expected value of a function of a random variable]
Let $X$ be a continuous random variable and $g: \mathbb{R} \rightarrow \mathbb{R}$ be a function. The \emph{expected value} of the random variable $Y = g(X)$ is defined as
\[
\Expect{Y} = \int\limits_{-\infty}^{\infty} y\Prob{Y = y} dy = \int\limits_{-\infty}^{\infty} g(x)\Prob{X = x} dx \text{.}
\]
\end{definition}

\begin{lemma}
\label{lemma-expect-probability}
Let $X, Y$ be random variables and $t \in \mathbb{R}$. For every $z \in [0, 1]$ set $\omega_t(z) = \{y \in \mathbb{R} \setdelim z = \Prob{X = t \mid Y = y} \}$. Let $Z_t$ be a random variable bounded in the interval $[0, 1]$ such that \[\Prob{Z_t = z} = \int\limits_{\omega_t(z)} \Prob{Y = y} dy \text{.} \] Then $\Prob{X = t} = \Expect{Z_t}$.
\end{lemma}
\begin{proof}
By the Law of Total Probability we state the following fact:
\[
\begin{split}
\Prob{X = t} 
	& = \int\limits_{-\infty}^{\infty} \Prob{X = t, Y = y} dy \\
	& = \int\limits_{-\infty}^{\infty} \Prob{X = t \mid Y = y} \Prob{Y = y} dy \\
	& = \int\limits_{0}^{1} z \Prob{Z_t = z} dz \\
	& = \Expect{Z_t} \text{.}
\end{split}
\]

So the expected value of the variable $Z$ equals the probability $\Prob{X = t}$.
\end{proof}

In Lemma \ref{lemma-random-variable} we refer to the modification of Lemma \ref{lemma-expect-probability} for $X \geq t$. In this modified version we assume that variable $Z = \Prob{X \geq t \mid Y = y}$. A straightforward inspection proves the consequence.

The following two theorems, the Markov's \cite{557945} and Chebyshev's \cite{pFEL66a} inequalities, are well known and we often refer to them in the work. Let us note that various improvements for the higher moments hold as well.
\begin{theorem}[Markov's inequality]
\label{theorem-markov-inequality}
Let $X$ be a random variable and $t \in \mathbb{R}^{+}$. Then the probability of the event $X \geq t$ is bounded as
\[
\Prob{|X| \geq t} \leq \frac{\Expect{|X|}}{t} \text{.}
\]
\end{theorem}
\begin{proof}
For the indicator $\Indicator(|X| \geq t)$ we have that $t\Indicator(|X| \geq t) \leq |X|$. If $|X| \geq t$, than $\Indicator(|X| \geq t) = 1$ and it follows that $t\Indicator(|X| \geq t) = t \leq |X|$. If $|X| < t$, then $0 = \Indicator(|X| \geq t) \leq |X|$.

\[
\begin{split}
\Expect{|X|} 
	& = \int\limits_{0}^{\infty} |X| \Prob{|X| = x} dx \\
	& \geq \int\limits_{0}^{\infty} t \Indicator(|X| \geq t) \Prob{|X| = x} dx \\
	& = t \int\limits_{0}^{\infty} \Indicator(|X| \geq t) \Prob{|X| = x} dx \\
	& = t \Prob{|X| \geq t} \text{.}
\end{split}
\]
\end{proof}

\begin{theorem}[Chebyshev's inequality]
\label{theorem-chebyshev-inequality}
If $X$ is a random variable, then \[ \Prob{|X - \Expect{X}| \geq \epsilon} \leq \frac{\Variance{X}}{\epsilon ^ 2} \text{.} \]
\end{theorem}
\begin{proof}
First observe that the event $|X - \Expect{X}| \geq \epsilon$ is equivalent to the event $(X - \Expect{X}) ^ 2 \geq \epsilon ^ 2$. The theorem follows from the Markov's inequality used for the latter event and from the definition of variance. 
\[
\begin{split}
\Prob{|X - \Expect{X}| \geq \epsilon} 
	& = \Prob{(X - \Expect{X}) ^ 2 \geq \epsilon ^ 2} \\
	& \leq \frac{\Expect{(X - \Expect{X}) ^ 2}}{\epsilon ^ 2} \\
	& = \frac{\Variance{X}}{\epsilon ^ 2} \text{.}
\end{split}
\]
\end{proof}

From the Chebyshev's inequality it follows that the average converges to the expected value. This fact is commonly referred to as the Weak Law of Large Numbers.
\begin{theorem}[Weak Law of Large Numbers]
\label{theorem-weak-law-of-large-numbers}
Let $n \in \mathbb{N}$, $\epsilon > 0$ and $X_1$, \dots, $X_n$ be independent identically distributed random variables. Then 
\[
\Prob{\left| \frac{\sum\displaylimits_{i = 1}^{n} X_i}{n} - \Expect{X_1} \right| \geq \epsilon} \leq \frac{\Variance{X_1}}{n\epsilon ^ 2} \text{.}
\]
\end{theorem}
\begin{proof}
Define the average of $X_1, \dots, X_n$ as the random variable $\bar{X} = \frac{\sum\displaylimits_{i = 1}^{n} X_i}{n}$. From the properties of the expected value and variance stated in Lemma \ref{lemma-expected-value-properties} it follows that 
\[
\begin{split}
& \Expect{\bar{X}} = \Expect{X_1} \\
& \Variance{\bar{X}} = \frac{\Variance{X_1}}{n} \text{.}
\end{split}
\]

The Chebyshev's inequality used for the random variable $\bar{X}$ yields the required result.
\end{proof}

Following lemma enables us to compute the expected value of a continuous random variable if we know its cumulative probability density function.
\begin{lemma}
\label{lemma-expected-value-cumulative}
Let $X$ be a continuous random variable taking only non-negative values. Let $F: \mathbb{R}_{0}^{+} \rightarrow \left[0, 1\right]$ be its cumulative probability density function. Then
\[
	\Expect{X} = \int\limits_{0}^{\infty} 1 - F(x) \ dx
\]
\end{lemma}
\begin{proof}
From the definition of the expected value and the cumulative probability density function we have that
\[
\begin{split}
\Expect{X} 
	& = \int\limits_{0}^{\infty} t\Prob{X = t} dt \\
	& = \int\limits_{0}^{\infty} \int\limits_{0}^{t} \Prob{X = t} dx \  dt \\
	& = \int\limits_{0}^{\infty} \int\limits_{x}^{\infty} \Prob{X = t} dt \  dx \\
	& = \int\limits_{0}^{\infty} \Prob{X \geq x} dx \\
	& = \int\limits_{0}^{\infty} 1 - F(x) \ dx \text{.}
\end{split}
\]
\end{proof}
