\chapter{Basic facts regarding probability}
In this appendix we summarise some facts regarding the probability theory. We show basic definitions and prove statements used in the work.

First we discuss the discrete probability and discrete random variables. Then we move to continuous random variables and finally show the Markov's and Chebyshev's inequality.

Whenever we perform a \emph{random experiment} we get a random outcome. The probability theory models the uncertainty included in random experiments and deals with the probabilities of such results. For example if the coin toss is our random experiment then the set $\{\text{roll}, \text{die}\}$ contains all the possible outcomes. If the coin is fair probability $\frac{1}{2}$ is the same for both of them.

\begin{definition}[Probability space, elementary event]
We refer to the set $\Omega$ consisting of all the possible outcomes of a random experiment, $\Omega = \{ \omega_1, \dots, \omega_n \}$ as to the \emph{probability space}. Its elements $\omega_i$, $1 \leq i \leq n$ are \emph{elementary events}. The probability of an elementary event $\omega_i$, $1 \leq i \leq n$ equals $p_i$, $0 \leq p_i \leq 1$ and is written as: \[ \Prob{\omega_i} = p_i \text{.} \] The sum of the probabilities of the elementary events must be equal to one:
\[
\displaystyle\sum_{i = 1}^{n} p_i = 1 \text{.}
\]
\end{definition}

In general the probabilities of elementary events need not to be the same but this assumption is frequently satisfied. Then $p_i = \frac{1}{n}$ for $1 \leq i \leq n$.

\begin{definition}[Event, probability]
Set $A = \{a_1, \dots, a_m \}\subseteq \Omega$, containing only elementary events, is called a \emph{compound event}. The \emph{probability of the compound event $A$} is \[ \Prob{A} = \displaystyle\sum_{\omega \in A} \Prob{\omega} \text{.} \] If the probabilities of the elementary events are the same then 
\[
	\Prob{A} = \frac{|A|}{|\Omega|} = \frac{m}{n} \text{.}
\]
\end{definition}

\begin{definition}[Complementary event, certain event, impossible event]
Let $A \subseteq \Omega$ be an event. The set $\Omega - A$ is the \emph{complementary event} to the event $A$. The \emph{certain event} is the probability space $\Omega$ and its complementary event $\emptyset$ is the \emph{impossible event}.
\end{definition}

\begin{corollary}[Probability of a complementary event]
If $A \subseteq \Omega$ is an event then the probability of the complementary event is 
\[ 
	\Prob{\Omega - A} = 1 - \Prob{A} \text{.} 
\] 
\end{corollary}
\begin{proof}
Follows from the definition of probability:
\[
\begin{split}
1 
	& = \Prob{\Omega} = \Prob{(\Omega - A) \cup A} \\
	& = \displaystyle\sum_{\omega \in \Omega - A} \Prob{\omega} + \displaystyle\sum_{\omega \in A} \Prob{\omega} \\
	& = \Prob{\Omega - A} + \Prob{A} \text{.}
\end{split}
\]
\end{proof}

We can compute the size of the set, consisting of elementary events, if we know the event's probability and size of the probability space.

\begin{lemma}
Let $A \subseteq \Omega$ be an event such that $\Prob{A} = p$. If the probabilities of the elementary events are the same, then $|A| = p |\Omega|$.
\end{lemma}
\begin{proof}
This statement is a direct use of the definition of probability of the event $A$.
\[
p = \Prob{A} = \frac{|A|}{|\Omega|}
\]
\end{proof}

\begin{definition}[Compound event, compound probability]
Let $A, B \subseteq \Omega$ be events. The \emph{compound event} is denoted by $A, B$ and equals the event $A \cap B$. The \emph{compound probability} of the event $A, B$ is
\[
\Prob{A, B} = \Prob{A \cap B} \text{.}
\]
\end{definition}

\begin{definition}[Disjoint events]
Let $A, B \subseteq \Omega$ be two events. Events $A$ and $B$ are \emph{disjoint} if $A \cap B = \emptyset$ and the corresponding compound probability equals
\[
\Prob{A, B} = \Prob{\emptyset} = 0 \text{.}
\]
\end{definition}

\begin{definition}[Independent events]
Let $A, B \subseteq \Omega$ be two events. Events $A$ and $B$ are \emph{independent} if
\[
\Prob{A, B} = \Prob{A}\Prob{B} \text{.}
\]
\end{definition}

The motivation of the following is to compute the probability of event $A$ inside the restricted probability space $B$.
\begin{definition}[Conditional probability]
Let $A, B \subseteq \Omega$ be two events and assume that $\Prob{B} \neq 0$. Then the \emph{conditional probability} of event $A \mid B$ is defined as
\[
\Prob{A \mid B} = \frac{\Prob{A, B}}{\Prob{B}} \text{.}
\]
\end{definition}

\begin{lemma}
\label{lemma-conditional-probability-event-estimate}
If $A, B \subseteq \Omega$ are two non-disjoint events such that $\Prob{B} \neq 0$, then $\Prob{B} \leq \frac{\Prob{A}}{\Prob{A \mid B}}$.
\end{lemma}
\begin{proof}
From our assumptions we have that the conditional probability of the event $A \mid B$ is defined and positive. To prove the lemma the definition of the conditional probability and the straightforward bound on the compound probability, $\Prob{A, B}~\leq~\Prob{A}$, is used
\[
\Prob{B} = \frac{\Prob{A, B}}{\Prob{A \mid B}} \leq \frac{\Prob{A}}{\Prob{A \mid B}} \text{.}
\]
\end{proof}

We summarise the basic properties of probability.
\begin{itemize}
\item $\Prob{\emptyset} = 0$
\item $\Prob{\Omega} = 1$
\item $0 \leq \Prob{A} \leq 1$
\item $\Prob{A \cup B} = \Prob{A} + \Prob{B} - \Prob{A \cap B}$
\end{itemize}

\begin{theorem}[The Law of Total Probability]
\label{theorem-law-of-total-probability}
Let $A \subseteq \Omega$ be an event and $\Omega_1, \dots, \Omega_k$ be a decomposition of the probability space $\Omega$ such that it does not contain the impossible event. Then
\[
\Prob{A} = \displaystyle\sum_{i = 1}^{k} \Prob{A \mid \Omega_i} \Prob{\Omega_i} \text{.}
\]
\end{theorem}
\begin{proof}
We perform a simple computation to decompose the event $A$. The decomposed event is then rewritten using the definition of the conditional probability.
\[
\begin{split}
\Prob{A}
	& = \Prob{A \cap \Omega} \\
	& = \displaystyle\sum_{i = 1}^{k} \Prob{A, \Omega_i} \\
	& = \displaystyle\sum_{i = 1}^{k} \Prob{A \mid \Omega_i} \Prob{\Omega_i} \\
\end{split}
\]
\end{proof}

\begin{definition}[Discrete random variable]
Let $\Omega$ be a probability space. Then \emph{discrete random variable} $X$ is a function $X: \Omega \rightarrow \mathbb{N}$. Probability of the event $X = x$ equals
\[
\Prob{X = x} = \displaystyle\sum_{\substack{\omega \in \Omega \\ X(\omega) = x}} \Prob{\omega} \text{.}
\]
\end{definition}

\begin{definition}[Continuous random variable, probability density function, cumulative probability density function]
\emph{Continuous random variable} $X$ is a function $X: \mathbb{R} \rightarrow \mathbb{R}$. 

Function $F: \mathbb{R} \rightarrow \left[0, 1\right]$ is the \emph{cumulative probability density function} of the random variable $X$ if the following conditions are satisfied
\begin{itemize}
\item $\lim\limits_{t \rightarrow -\infty} F(t) = 0$,
\item $\lim\limits_{t \rightarrow \infty} F(t) = 1$.
\item Function $F$ is non-decreasing.
\item $F(t) = \Prob{X \leq t} $ for every $t \in \mathbb{R}$.
\end{itemize}

Function $f: \mathbb{R} \rightarrow \mathbb{R}^{*}$ such that $f(t) = F'(t)$ for every $t \in \mathbb{R}$ is the \emph{probability density function} of the random variable $X$. The probability density function $f$ satisfies that
\begin{itemize}
\item $\int\limits_{-\infty}^{\infty} f(t) dt = 1$,
\item $\int\limits_{-\infty}^{t} f(x) dx = F(t)$.
\end{itemize}
\end{definition}

More formal definitions of random variables use the Lebesgue measure and Borel sets to measure the probability $\Prob{X = x}$. However previous definitions are sufficient for our needs.

If $X$ is a continuous random variable with cumulative density function $F$ then the probability of $X \in \left[a, b\right]$ equals
\[
\Prob{a \leq X \leq b} = F(b) - F(a) \text{.}
\]

The motivation behind probability density function is that it corresponds to the probability of $X$ being equal a singleton.
\[
\Prob{a \leq X \leq b} = F(b) - F(a) = \int\limits_{a}^{b} f(x) dx
\]
We see that for every $\epsilon > 0$ value $\epsilon f(b)$ approximates $\Prob{b - \epsilon \leq X \leq b}$ when epsilon is small enough. However for continuous variables the event $X = b$ is impossible unless $F$ is in-continuous in $b$ and thus $f(b) = \infty$.	

After observing the previous fact we can rewrite $F(t)$ to the form:
\[
F(t) = \Prob{X \leq t} = \int\limits_{-\infty}^{t} \Prob{X = t} dt \text{.}
\]

\begin{definition}[Expected value]
Let $X$ be a discrete random variable. Its \emph{expected value}, $\Expect{X}$, is defined as
\[
\Expect{X} = \displaystyle\sum_{x \in \mathbb{N}} x \Prob{X = x} \text{.}
\]

In the continuous case assume that $X$ is a continuous random variable, then its \emph{expected value}, $\Expect{X}$, is defined as
\[
\Expect{X} = \int\limits_{-\infty}^{\infty} x \Prob{X = x} dx \text{.}
\]
\end{definition}

\begin{definition}[Variance]
Let $X$ be a random variable. Its variance, $\Variance{X}$, is defined as \[ \Variance{X} = \Expect{(X - \Expect{X}) ^ 2} \text{.} \]
\end{definition}

We only show the basic properties of the expected value and variance without the proof. 
\begin{lemma}
Assume that $X$ and $Y$ are random variables and $a, b, c \in \mathbb{R}$. Then
\label{lemma-expected-value-properties}
\begin{itemize}
\item $\Expect{aX + bY + c} = a\Expect{X} + b\Expect{Y} + c$
\item $\Variance{aX + bY + c} = a ^ 2 \Variance{X} + b ^ 2 \Variance{Y}$
\end{itemize}
\end{lemma}

\begin{definition}[Expected value of a function of a random variable]
Let $X$ be a continuous random variable and $g: \mathbb{R} \rightarrow \mathbb{R}$ be a function. The \emph{expected value} of the random variable $Y = g(X)$ is defined as
\[
\Expect{Y} = \int\limits_{-\infty}^{\infty} y\Prob{Y = y} dy = \int\limits_{-\infty}^{\infty} g(x)\Prob{X = x} dx \text{.}
\]
\end{definition}

\begin{lemma}
\label{lemma-expect-probability}
Let $X, Y$ be random variables, $t \in \mathbb{R}$ and $Z$ be a random variable having value $\Prob{X = t \mid Y = y}$ with the probability $\Prob{Y = y}$. Then $\Prob{X = t}$ can be computed as
\[
\Prob{X = t} = \Expect{Z} \text{.}
\]
\end{lemma}
\begin{proof}
By the Law of Total Probability we state the following fact:
\[
\begin{split}
\Prob{X = t} 
	& = \int\limits_{-\infty}^{\infty} \Prob{X = t, Y = y} dy \\
	& = \int\limits_{-\infty}^{\infty} \Prob{X = t \mid Y = y} \Prob{Y = y} dy \\
	& = \int\limits_{-\infty}^{\infty} z \Prob{Z = z} dz \\
	& = \Expect{Z} \text{.}
\end{split}
\]

The expected value of set then equals the wanted probability.
\end{proof}

We refer to the modification of the previous lemma for $X \geq t$ in Lemma \ref{lemma-random-variable}. In the modified version we assume that variable $Z = \Prob{X \geq t \mid Y = y}$. A straightforward inspection proves the lemma.

The following two theorems, Markov's \cite{557945} and Chebyshev's \cite{pFEL66a} inequalities, are well known and we often refer to them in the work. Let us note that similar improvements for the higher moments hold as well.
\begin{theorem}[Markov's inequality]
\label{theorem-markov-inequality}
Let $X$ be a random variable and $t \in \mathbb{R}^{+}$. Then the probability of the event $X \geq t$ is bounded as
\[
\Prob{|X| \geq t} \leq \frac{\Expect{|X|}}{t} \text{.}
\]
\end{theorem}
\begin{proof}
For the indicator $\Indicator(|X| \geq t)$ we have that $t\Indicator(|X| \geq t) \leq |X|$. If $|X| \geq t$ than $\Indicator(|X| \geq t) = 1$ and it follows that $t\Indicator(|X| \geq t) = t \leq |X|$. If $|X| < t$ then $0 = \Indicator(|X| \geq t) \leq |X|$.

\[
\begin{split}
\Expect{|X|} 
	& = \int\limits_{0}^{\infty} |X| \Prob{|X| = x} dx \\
	& \geq \int\limits_{0}^{\infty} t \Indicator(|X| \geq t) \Prob{|X| = x} dx \\
	& \geq t \int\limits_{0}^{\infty} \Indicator(|X| \geq t) \Prob{|X| = x} dx \\
	& = t \Prob{|X| \geq t} \\
\end{split}
\]
\end{proof}

\begin{theorem}[Chebyshev's inequality]
\label{theorem-chebyshev-inequality}
If $X$ is a random variable, then \[ \Prob{|X - \Expect{X}| \geq \epsilon} \leq \frac{\Variance{X}}{\epsilon ^ 2} \text{.} \]
\end{theorem}
\begin{proof}
First observe that the event $|X - \Expect{X}| \geq \epsilon$ is equivalent to the event $(X - \Expect{X}) ^ 2 \geq \epsilon ^ 2$. The theorem follows from the Markov's inequality used for the latter event and from the definition of variance. 
\[
\begin{split}
\Prob{|X - \Expect{X}| \geq \epsilon} 
	& = \Prob{(X - \Expect{X}) ^ 2 \geq \epsilon ^ 2} \\
	& \leq \frac{\Expect{(X - \Expect{X}) ^ 2}}{\epsilon ^ 2} \\
	& = \frac{\Variance{X}}{\epsilon ^ 2} \text{.}
\end{split}
\]
\end{proof}

From the Chebyshev's inequality it follows that the average converges to the expected value. This fact is commonly referred to as the Weak Law of Large Numbers.
\begin{theorem}[Weak Law of Large Numbers]
\label{theorem-weak-law-of-large-numbers}
Let $n \in \mathbb{N}$, $\epsilon > 0$ and $X_1$, \dots, $X_n$ be identically independent distributed random variables. Then 
\[
\Prob{\left| \frac{\sum\displaylimits_{i = 1}^{n} X_i}{n} - \Expect{X_1} \right| \geq \epsilon} \leq \frac{\Variance{X_1}}{n\epsilon ^ 2} \text{.}
\]
\end{theorem}
\begin{proof}
Set the random variable $\bar{X} = \frac{\sum\displaylimits_{i = 1}^{n} X_i}{n}$. From the properties of the expected value and variance stated in Lemma \ref{lemma-expected-value-properties} it follows that 
\[
\begin{split}
& \Expect{\bar{X}} = \Expect{X_1} \\
& \Variance{\bar{X}} = \frac{\Variance{X_1}}{n} \text{.}
\end{split}
\]

The Chebyshev's inequality used for the random variable $\bar{X}$ yields the required result.
\end{proof}

Following lemma enables us to compute the expected value of a continuous random variable if we know its cumulative probability density function.
\begin{lemma}
\label{lemma-expected-value-cumulative}
Let $X$ be a continuous random variable taking only non-negative values. Let $F: \mathbb{R}_{0}^{+} \rightarrow \left[0, 1\right]$ be its cumulative probability density function. Then
\[
	\Expect{X} = \int\limits_{0}^{\infty} 1 - F(x) dx
\]
\end{lemma}
\begin{proof}
From the definition of the expected value and the cumulative probability density function we have that
\[
\begin{split}
\Expect{X} 
	& = \int\limits_{0}^{\infty} t\Prob{X = t} dt \\
	& = \int\limits_{0}^{\infty} \int\limits_{0}^{t} \Prob{X = t} dx \  dt \\
	& = \int\limits_{0}^{\infty} \int\limits_{x}^{\infty} \Prob{X = t} dt \  dx \\
	& = \int\limits_{0}^{\infty} \Prob{X \geq x} dx \\
	& = \int\limits_{0}^{\infty} 1 - F(t) dx \text{.}
\end{split}
\]
\end{proof}
