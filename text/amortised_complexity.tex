\chapter{Amortised complexity of a hashing scheme}

% TODO: ktore schemy nevedia delete, alebo nie je znamene, zmienit dynamizaciu
The hashing scheme we propose later in this chapter is a solution to the set representation problem. Every such solution provides us basic with some operations. These allow obtaining an information if an element is stored within the set, accessing the stored elements and inserting the element. Some schemes do not implement the element removal or the efficient implementation is not known.

% TODO: definicia vahovo vyvazeneho stromu.
The must crucial are the operations' running times. Various solutions to different problems prefer different operations. For instance some applications modify stored data rarely but the other storages are used for data that is almost never queried. So the running times of some operations are considered more important. In the case of a simple array we have $O(1)$ time for the find procedure and $O(n)$ insertion or deletion time in the canonical case. Better bounds can be obtained using the amortised complexity. Routines of balanced trees' are typically bounded by the logarithmic time. As already mentioned the right answer what is a good choice for data structure that should be used lies in the estimated structure of operations. This choice can be an asymptotic improvement, too. Anyway this does not change the fact that the short running times are welcomed.

We analyse the running times of the universal hashing first by mentioning the known facts. Then we extend this analysis to the universal hashing using the system of linear transformations over vector spaces. We will guarantee the worst case complexity of the find operation.

\section{Expected time of the find operation in the classical universal hashing}
In this section we assume that the used system of hashing functions is at least $c$-universal. The running time of the find operation is proportional to the number of collisions in the bucket where the searched element should be placed according to its hash value. The obvious worst case is $O(n)$ where $n$ is the number of elements hashed. The universal hashing gives a far better expected estimate, $O(1)$.

\begin{theorem}
\label{theorem-expected-chain-length-universal}
Expected time of the find operation in the universal hashing using a $c$-universal system is $O(1)$ if the load factors are upper bounded.
\end{theorem}
\begin{proof}
The time of the find operation is given by the length of the chain into which the searched element falls. We must also add the time for hashing the element but this can be typically done in one unit of time. The expected length of a chain containing the element $x \in U$ must be computed. The expectation is taken over the family of hash functions and the chain's length is the sum of indicator variables:
\begin{displaymath}
\frac{\sum\displaylimits_{h \in H} \sum\displaylimits_{y \in S - \lbrace x \rbrace} P(h(x) = h(y))}{|H|} = \frac{\sum\displaylimits_{h \in H} \sum\displaylimits_{y \in S - \lbrace x \rbrace} \frac{c}{m}}{|H|} = \frac{cn}{m}
\end{displaymath}

Since the element $x$ may be present in the set $S$ the time is bounded by the expression $1 + c\alpha$ where $\alpha$ is the table's load factor. The typical load factor is lower than 1, we only assumed that it is bounded. It is still enough to see that the expected time of every operation is still in $O(1)$.
\end{proof}

\section{Effects of trimming long chains on the universal system}
The model of hashing that we present guarantees the worst case bound on the length of the longest chain and thus the time of all operations. If we knew the value of $\Expect{\lpsl}$ we would know when to stop inserting into a long chain and rehash whole table. The algorithm would start finding more suitable function which does create such long chains. We would also be able to set this limit according to the probability of creating long chain.

Some formal computations are performed to support these ideas. The fact that $\Prob{\lpsl > k \Expect{\lpsl}} \leq \frac{1}{k}$ is obtained by a direct use of the Markov inequality. For instance this means that less than half of all functions create longest chains longer than $2 \Expect{\lpsl}$. In the case of linear transformations $2\Expect{\lpsl} \leq \frac{8 c_\epsilon}{1 - \epsilon} \log m \log \log m$ for every $0 < \epsilon < 1$ where $m$ is the table's size. Situation when a long chain occurs ends by rehashing of the whole table and finding a new suitable function. Chosen mapping creates shorter chains only.

If the statement that function $h$ creates a long chain means that there is a chain of length longer than $k \Expect{\lpsl}$ then:
\[
|\lbrace h \in H \mid \textit{ h does not create a long chain} \rbrace| 
	= \left(1 - \Prob{\lpsl > k \Expect{\lpsl}}\right) |H| 
	\geq \frac{|H|}{k} \text{.}
\]

Regarding that every function is chosen uniformly and the unsuitable ones are discarded we still perform uniform selection but now among functions that does not create long chains. Note that the restriction on the functions of the universal system is done by using the information about the hashed set. In the next calculation we observe that a smaller but still universal system of functions.
\[
\begin{split}
& \Prob{h(x) = h(y) \text{ with trimming}} \\
	& \qquad =  \frac{|\{ h \in H \mid h(x) = h(y) \wedge \textit{ h does not create long chains} \}|}{|\lbrace h \in H \mid \textit{ h does not create long chains} \rbrace|} \\
	& \qquad \leq \frac{k |\{ h \in H \mid h(x) = h(y) \wedge \textit{ h does not create long chains} \}|}{|H|} \\ 
	& \qquad \leq \frac{k |\{ h \in H \mid h(x) = h(y) \}|}{|H|} \\
	& \qquad = k \Prob{h(x) = h(y) \text{ without trimming}} \\
\end{split}
\]

Last inequality combined with $c$-universality of the original system gives $ck$-universality of the resulting system. Similar restrictions on the other universal systems may be used. For such systems probability of collision of two (or more) elements is estimated by their strong $c$-universality or simple $c$-universality. Resulting probability is $k$ times the original one as stated in the next theorem.

\begin{theorem}
Let $H$ be an universal system of hash functions mapping universe $U$ to a hash table $B$. Let $k > 1$ be the trim rate and define $m = |B|$. Then for every hashed set $S \subset U$ system of functions 
\[
	H_S = \{ h \in H \setdelim \lpsl(h) \leq k \Expect{\lpsl} \}
\]
is still universal with the constant of universality multiplied by the factor of $k$.
\[
	\Prob{h(x) = h(y) \text{ for } h \in H_S} \leq k \Prob{h(x) = h(y) \text{ for } h \in H} \text{.}
\]
\end{theorem}
In the probability statement we used the choice over function $h \in H_S$.
\begin{proof}
We use the just derived probability bound.
\[
\begin{split}
\Prob{h(x) = h(y) \text{ for } h \in H_S} 
	& = \Prob{h(x) = h(y) \text{ with trimming}} \\
	& \leq k \Prob{h(x) = h(y) \text{ without trimming}} \\
	& = \Prob{h(x) = h(y) \text{ for } h \in H} \\
\end{split}
\]
\end{proof}

Next statement summarises results for the systems of linear transformations inspected by us.

\begin{corollary}
\label{corollary-trimming-linear}
For every trimming rate and hashed set the system of linear transformations, $LT(U, B)$, is still $k$-universal.
\end{corollary}
\begin{proof}
Since system of linear transformations is $1$-universal as seen in remark \ref{remark-system-of-linear-transformations} and from the previous statement it is clear that the system $LT(U, B)_S$ is still $k$-universal.
\end{proof}

The previous computations show us that the expected chain length is still constant but can be larger according to our choice of $k$. Of course by selecting greater length of the longest chain we will omit less mappings. The lower the value of trimming rate is the better expected results are obtained but the warranty for the worst case is adequately worsened.

The most interesting idea of the trimming is an adaptation of the universal class of functions to hashed sets.

\section{Hashing using the family of linear transformations}

The model we propose is a slight modification of the simplest universal hashing scheme with separated chains:
\begin{itemize}
\item We use the universal family of linear transformations from the $U = Z^2_l$ to the space $Z^2_f$. We may imagine this as hashing $l$-bit binary numbers to the space of $f$-bit binary numbers. We will refer to the size of the hashing table as $m = 2^k$.
\item \textbf{Chain limit rule:} when there is a chain longer than a limiting value $l(m)$ then the whole table is rehashed.
\item \textbf{Load factor rule:} if the only operations performed are insert and find the load factor of the table is maintained in the interval $\left[0.5, 1\right]$. We need the interval $\left[0.25, 1\right]$ if delete is allowed, too. If the load factor rule is violated whole table is rehashed into a greater or smaller table respectively without violating the second property.
\end{itemize}

The choice of the limiting value, $l(m)$, for the table of size $m$ is done in the following manner. We would like to guarantee the worst case limit even when the table is fully loaded. So we use the results of the section \ref{section-linear-systems-linear-amount-constant-estimate} for the $\alpha = 1$ when the delete operations are not allowed and $\alpha = 1.5$ when they are. The value $l(m)$ is chosen so that the probability of failing to find a suitable function is lower than a prescribed value $p$. This probability bound certainly works for the lower values of $\alpha$. In the mentioned section we computed the limit for $p = 0.5$ as $19.152 \log m \log \log m$ when $\alpha = 1$ and $28.728 \log m \log \log m$ for $\alpha = 1.5$.

Because the estimates used work only with the initial size of the table greater than $572$ we will use the initial size of $m$ = 1 024 elements. Since not every hash table grows to a size of 1 024 elements this overhead may be unacceptable. In this case we can turn off the rehash operation caused by the chain limit rule violation when the table is smaller than 1 024 buckets. Expected amortised time complexity of the introduced scheme is about to be analysed.

\begin{lemma}
\label{lemma-linear-transformations-tries}
If the hash function for a table with the size of $m$ elements and with the chain limit setup $P(lpsl \geq l(m)) \leq p$ is sought the expected number of tries is $\frac{1}{(1 - p)^2}$.
\end{lemma}
\begin{proof}
The estimate of failing $k$ times is bounded by the geometric distribution and this probability is certainly less or equal than $p^k$. So the expected time of success is given by:
\begin{displaymath}
\sum_{i = 0}^{\infty} (i + 1)p^i = \sum_{i = 0}^{\infty}p^i + \sum_{i = 0}^{\infty}ip^i = \frac{1}{1 - p} + \frac{p}{(1- p)^2} = \frac{1}{(1 - p)^2}
\end{displaymath}
\end{proof}

\begin{theorem}
Let $m$ be the start size of a hash table confirming to our model where the delete operation is not allowed.  Also assume that the number of insert operations done is $2^k m $ for $k \in \mathbb{N}$. Then the expected amortised complexity of every performed operation find or insert is $O(1)$. The expectation is taken over the uniform choice of a hash function. 
\end{theorem}
\begin{proof}
For the amortised complexity analysis we use the potential method. The potential used here is the number $\alpha m - m$. This is the negative value of the number of remaining subsequent insert operations which would make the load factor reach one. Whenever an real insert is done we check if the prolonged chain is not longer than the limiting value or the load factor is not greater than $1$. So there are four cases to be analysed.
\begin{itemize}
\item Find operation is performed. From \ref{theorem-expected-chain-length-universal} and \ref{corollary-trimming-linear} we know it takes $O(1)$ time only.

\item No rehash is needed. Then from the previous theorem (\ref{theorem-expected-chain-length-universal}) the operation itself took only $O(1)$ time in the expected case. The potential is also increased by one. So the operation consumed $O(1)$ amortised time.

\item The rehash is performed because the load factor exceeds the value of $1$. The rehash itself takes the time $O(m)$ and the size of the table is doubled. Thus the resulting potential is $-m$ and the starting potential was $0$. So the operation took only a $O(1)$ time because of the first insertion.

\item Suppose the rehash operation is needed because of violation of chain limit rule. We seek for a new function satisfying the rule without resizing the table. A single rehash operation takes the time of $O(m)$ operations. In the following we refer to two immediate rehashes caused by the load factor rule as cycle. We need to find out how many rehash operations caused by the violation of the chain limit rule can occur in one cycle. 

Finding a suitable function for the hashed elements during the cycle is remarkably similar to finding it at the cycle's end. This is caused by setting the limit dependent on the size of the hash table only. We can see the process in the following manner. When we select functions from the family $H$ we can ask if the function is fine for a set $S_e$ that will be represented at the end of the cycle. The limits and probability bounds remain the same, $|S| < |S_e| = m$. Just remark that we do not know the hashed set $S_e$ in advance. Because universal hashing has no assumptions on a hashed set $S_e$\footnote{It only needs to be a subset of the universe $U$.} and it contains all currently hashed elements $S \subset S_e$ we can afford this simplification. After $\frac{1}{(1-p)^2}$ such choices we are expected to find a right function. The expected number of rehashes in a single cycle is then bounded by the same expression. 

Notice that the number of inserts to get the table from the size $m$ when $\alpha = 0.5$ to the value of $\alpha$ equal to one is $m$. The expected time for the limit rule caused operations is $\frac{1}{(1 - p)^2}O(m)$ and if this amount of time is distributed uniformly along the cycle of $m$ inserts we are still at $O(1)$ amortised time per operation.
\end{itemize}
\end{proof}

To be perfectly honest and to finish the proof we have an issue to care about. In the last case we distributed the time evenly throughout the cycle. If the number of inserts is not a power of two we would distribute possibly long time over a non-complete cycle and would not obtain $O(1)$ time in it. In the previous cycles our complexity estimates hold. 

Amortised time of insert operation then converges to the $O(1)$ time with a growing number of operations. The expected time of find is from the theorem \ref{theorem-expected-chain-length-universal} and it obviously does not change the potential.


One can find a potential to prove previous result more formally. We use this approach when the delete operation is also allowed.

\begin{theorem}
The expected amortised complexity of the operations find, insert and delete converges to $O(1)$. The expectation is taken over an uniform choice of a hash function.
\end{theorem}
\begin{proof}
In this proof we need to distinguish two types of the operation cycles. First we need to distribute the work of the load factor rule and that of the chain limit rule as well. In the previous theorem these cycles were the same. Also remember the difference that the limiting value is computed for the $\alpha = 1.5$.

In advance we must mention that the time of the find operation is proportional to the chain length. This value is expected to be constant and the operation does not change the potential. The following analysis is simplified by omitting the find operation.

Let the sequence $O = (o_i)_{i=1}^o$ denote the sequence of operations performed starting from the empty table, $o_i \in \lbrace Insert, Delete \rbrace$ and $o$ is the number of operations performed. Two later definitions will make the following analysis more clear.

\begin{definition}[$\alpha$-cycle]
The $\alpha$-cycle contains the operations between two immediate rehash operations caused by violations of the load factor rule independently on its cause - lower or upper bound. 
\end{definition}
Also note that the $\alpha$ is now in the interval $\left[0.25, 1\right]$. Whenever the rehash operation is executed the table size is chosen to fit the $\alpha$ as nearest $0.5$ as possible. The $\alpha$-cycle is defined to amortise the work caused by the load factor rule.

\begin{definition}[l-cycle]
The l-cycles are the partitioning of the sequence $O$ such that every l-cycle ends
\begin{itemize}
\item when the rehash operation caused by the load factor rule is needed or
\item we performed $0.5 m$ insert operations from the start of the l-cycle the load factor did not reach the value $1$.
\end{itemize}
\end{definition}
The l-cycle allows us to analyse the work caused by the chain limit rule.

The potential consists of two parts. Above all we want a simple insertion and deletion to take $O(1)$ time only. So the potential consists of the part $p_1 = 2i_{\alpha} + 4d_{\alpha}$ where $i_{\alpha}$ is the number inserts and $d_{\alpha}$ is the number of delete operations in the current $\alpha$-cycle. 

Next we need to distribute the work caused by the chain limit rule. Let $e$ be the expected number of rehash operations, equally fails, when finding a good function. The second part of the potential is $p_2 = 2ei_{l} + (ce - r) m$ where $i_l$ is the number of insertions in the current l-cycle, $r$ is the number of rehash operations caused by the chain rule violation and $c$ denotes the number of cycles occurred so far. The overall potential is combined as $p = 2p_1 + p_2$.

The analysis of the delete operation is simpler. When a deletion is performed we have just two cases:
\begin{itemize}
\item Simple deletion takes expected $O(1)$ time. We search in a chain of expected length $O(1)$ and the potential is increased by $8$.
\item The $\alpha$ violates the lower value. Now remark the number of deletions performed in the current cycle. The table has the size of $m$ slots in the beginning with load factor $0.5$. In the end the load factor is lowered to $0.25$. This work can be done by at least $0.25 m$ delete operations. The rehashing takes $O(m)$ time and the potential is also lowered by $m$ at least. Let us discuss the potential change. At first the difference of the $2p_1$ part of the potential is $2m$ at least. The second part gets increased by at most $m$ since a new l-cycle is started and the $i_l$ is zeroed.
\end{itemize}

The analysis of the first two cases of the insert operation remains similar as in the previous proof:
\begin{itemize}
\item Suppose no rehash operation after an insert was performed. The searched chain had the constant expected length and the potential was increased by $4 + 2e$. So the amortised time of the operation is constant.
\item If the load factor exceeds the upper bound there were at least $0.5 m$ insertions in the $\alpha$-cycle. The potential is certainly greater or equal $2m$. The rehash operation is expected to take $O(m)$ time. The resulting potential is lowered by at least $m$ since and a new l-cycle started.
\item Insert operation is the last in the l-cycle. Then there are at least $0.5 m$ insertions and the accumulated potential denoted by variable $p_2$ is equal to $em + (ce - r)m$. Since the new l-cycle begins the $i_l$ term is set to zero and the potential is moved to the term $((c + 1)e -r)m$.
\item Only the insert operations violating the chain limit rule remain. The time to rehash the table is $O(m)$ times the number of fails. The part $p_2$ was decreased by the same value since the variable $r$ increased by the number of fails. The expected number of fails in one l-cycle is equal to $e$ because the load factor is maintained lower than 1. The $0.5 m$ insert operations of the l-cycle can cause the increase at most to $1.5$ and the limiting value is chosen for the $\alpha = 1.5$.
\end{itemize}

The fraction $\frac{(ce - r)m}{o}$ converges to $1$ with the growing number of operations, $o$. This can be seen from the weak law of large numbers.
\end{proof}
