\chapter{The Model of Universal Hashing}
\label{chapter-proposed-model}

The hashing scheme we propose later in this chapter is a solution to the set representation problem. Solution of this problems usually provide some basic operations such as \emph{Insert}, \emph{Delete} and \emph{Find}. These operations allow querying if an element is stored within the represented set, accessing the stored elements and inserting the element. Some schemes, e.g. double hashing, do not implement the element removal at all or the efficient implementation is not known.

The must important are the operations' running times. Various solutions to many algorithmic problems prefer different operations when representing sets. For instance some applications query the stored data rarely, e.g. log files. On the other hand, other applications of the set representation problem may store the data that is almost never changed -- find operation is preferred. Therefore the running times of only selected operations are considered crucial. 

In the case of a simple array we have $O(1)$ time for the find procedure provided that we know an element's index. But insertion or deletion may take $O(n)$ time. Better bounds for dynamic arrays can be obtained using the amortised complexity. Another example are balanced trees, they have running times typically bounded by the logarithmic time. As already mentioned the right answer what data structure should be used lies in the estimated structure of the operations. Making the right choice can be an asymptotic improvement. Anyway, this does not change the fact that the short running times are welcomed.

We analyse the running times of the universal hashing, first by mentioning the known facts. Then, we extend this analysis to the universal hashing using the system of linear transformations over vector spaces. Finally, we propose a model that guarantees the worst case complexity of the find operation.

\section{Time Complexity of Universal Hashing}
In this section we assume, that the system of hashing functions we use is $c$-universal. The running time of the find operation is certainly proportional to the length of the chain of an element's bucket. The obvious worst case time is $O(n)$ where $n$ is the number of elements hashed. The universal hashing gives a far better expected estimate, $O(1)$.

Recall the definitions and notation from Chapter \ref{chapter-hashing}. The load factor of a hash table is denoted by $\alpha = \frac{n}{m}$. The value $n$ denotes the size of the represented set and $m$ is the size of the table.

\begin{theorem}
\label{theorem-expected-chain-length-universal}
Assume that we represent set $S$ in a hash table using a $c$-universal class of functions $H$. Let the table's load factor $\alpha$ be upper bounded. Then the expected length of a chain is lower or equals $c \alpha$.
\end{theorem}
\begin{proof}
We find the expected length of a chain containing arbitrary element $x \in U$. The expectation is taken over the choice of a function from the universal family of functions. From the definition of the expected value we have that
\begin{displaymath}
\begin{split}
\Expect{\psl} 
	& = \frac{\sum\displaylimits_{h \in H} psl(h(x), h) }{|H|} \\
	& = \frac{\sum\displaylimits_{h \in H} \sum\displaylimits_{y \in S} I(h(x) = h(y))}{|H|} \\
	& = \frac{\sum\displaylimits_{y \in S} \sum\displaylimits_{h \in H} I(h(x) = h(y))}{|H|}  \\
	& = \sum\displaylimits_{y \in S} \Prob{h(x) = h(y)} \\
	& = \frac{cn}{m} = c \alpha \text{.}
\end{split}
\end{displaymath}
\end{proof}

\begin{corollary}
If we hash using a $c$-universal class and $\alpha$ denotes the table's load factor, then the expected time of the find operation is $1 + c\alpha$.
\end{corollary}
\begin{proof}
The running time of the find operation is proportional to the length of the chain where the searched element belongs. In the worst case the operation must iterate the whole chain. We must also add the unit time for retrieving the element's hash value and for checking if the chain is not empty. From previous lemma it follows that the expected length of a chain is $c\alpha$. Remark, that we have no other assumption on the distribution of the input. The expected running time is then bounded by $1 + c\alpha$. 
\end{proof}

\begin{corollary}
\label{corollary-find-time}
The expected time of the find operation in every model of universal hashing is $O(1)$ when the load factor is bounded.
\end{corollary}
\begin{proof}
Follows from the previous corollary.
\end{proof}

\section{Consequences of Trimming Long Chains}
The model of hashing we propose guarantees the worst case bound on the length of the longest chain. Hence it bounds the running times of the operations. Knowledge of the value $\Expect{\lpsl}$ provides a \emph{limit} when to stop inserting into a long chain. If a long chain is found, then the whole table is rehashed. In this situation the algorithm seeks a more suitable function; a function that does create such long chains. This \emph{bound} corresponds to the probability of the existence of a long chain. In fact we set it according to this probability. In this section we examine consequences of such \emph{limits} on models of universal hashing.

Following computations support and motivate our later statements and ideas. The fact that \[ \Prob{\lpsl > k \Expect{\lpsl}} \leq \frac{1}{k} \] may be obtained by direct use of the Markov's inequality. This fact, for instance, means that less than half of all the universal functions create longest chains longer than $2 \Expect{\lpsl}$ for arbitrary stored set. For the system of linear transformations we choose the \emph{limit} as $2\Expect{\lpsl} \leq \text{1 076} \log m \log \log m + 88$. This \emph{bound} follows from Corollary \ref{corollary-best-elpsl}.

Expected length of the longest chain gives us a generic hint when the table should be rehashed if the worst case time for the find operation is guaranteed. The lower the value $\Expect{\lpsl}$, or the tighter its estimate is, the better worst case \emph{limit} is achieved.

Mentioned probability bound computed from the expected value can be further improved. If the probability density function of the random variable $\lpsl$ is known, then it may be used directly. For the system of linear mappings we have already discovered it. In addition, we have consequently improved it in. To sum up, the approach with the expected length is more general but achieves worse performance. However, it is more general because whenever we know the probability density function, then we should able to find the expected value.

\begin{definition}[Chain length limiting function, long chain, $p$-trimmed-system, trimming rate]
Let $H$ be a universal system mapping an universe $U$ to a hash table $B$. Let $m$ be the size of the hash table, $\alpha$ be its load factor and $S \subset U$ be the stored set. 

Then function $l: \mathbb{N} \times \mathbb{R}_{0}^{+} \rightarrow \mathbb{N}$ of variables $m$ and $\alpha$, $l(m, \alpha)$, is a \emph{chain length limiting function}. 

We say that function $h \in H$ \emph{creates a long chain} if there is a chain of length longer than limiting value $l(m, \alpha)$.

Moreover let $p \in (0, 1)$ be probability such that $\Prob{\lpsl \geq l(m, \alpha)} \leq p$, then system $H_p = \{ h \in H \setdelim h \text{ does not create a long chain} \}$ is called a \emph{$p$-trimmed system}. The probability $p$ is called the \emph{trimming rate}.
\end{definition}

Probability bound $p \in (0, 1)$ of the existence of a long chain has important consequences for the model that limits chains.
\begin{itemize}
\item At most $p|H|$ of all the functions in the original universal system $H$ create long chains -- longer than the prescribed limit $l(m, \alpha)$. 
\item The probability that the table needs to be rehashed, equivalently probability of selecting an inconvenient function, is lower than $p$ provided the uniform choice of a hash function.
\item During rehashing caused by an occurrence of a long chain, the probability of finding a suitable function is at least $1 - p$ when assuming the uniform choice of a hash function.
\end{itemize}

\begin{lemma}
\label{lemma-size-of-trimmed-system}
If $H_p$ is a $p$-trimmed system, then $|H_p| \geq (1 - p)|H|$.
\end{lemma}
\begin{proof}
Simply use the definition of $H_p$ and that of the trimming rate $p$:
\[
\begin{split}
|H_p|
	& = |\lbrace h \in H \setdelim \text{ h does not create a long chain} \rbrace| \\
	& =\Prob{\lpsl < l(m, \alpha)} |H| \\
	& = \left(1 - \Prob{\lpsl \geq l(m, \alpha)}\right) |H| \\
	& \geq (1 - p)|H| \text{.}
\end{split}
\]
\end{proof}

Regarding that every function is chosen uniformly and the unsuitable ones are discarded, we still perform the uniform selection of a hash function. The choice is now restricted to the functions that do not create long chains; to the class $H_p$. Note that the restriction to the functions of the original universal system $H$ comes from an information about the stored set.

Previous remarks are quite interesting. So now, we ask, if it is possible to use the family $H_p$ as a universal one.
\begin{theorem}
\label{theorem-p-trimmed-is-universal}
Let $H$ be a $c$-universal system of hash functions, $U$ be a universe and $B$ be a hash table. Let $p \in (0, 1)$ be the trimming rate and set $m = |B|$. Then the system of functions $H_p$ is $\frac{c}{1 - p}$-universal. Equivalently:
\[
	\Prob{h(x) = h(y) \text{ for } h \in H_p} \leq \frac{1}{1 - p} \Prob{h(x) = h(y) \text{ for } h \in H} \text{.}
\]
\end{theorem}
\begin{proof}
From lemma \ref{lemma-size-of-trimmed-system} and from the assumption of $c$-universality of the original system it follows that 
\[
\begin{split}
& \Prob{h(x) = h(y) \text{ for } h \in H_p}  \\
	& \qquad =  \frac{|\{ h \in H \mid h(x) = h(y) \wedge \textit{ h does not create long chains} \}|}{|\lbrace h \in H \mid \textit{ h does not create long chains} \rbrace|} \\
	& \qquad \leq \frac{|\{ h \in H \mid h(x) = h(y) \wedge \textit{ h does not create long chains} \}|}{(1 - p)|H|} \\ 
	& \qquad \leq \frac{|\{ h \in H \mid h(x) = h(y) \}|}{(1 - p)|H|} \\
	& \qquad = \frac{1}{1 - p} \Prob{h(x) = h(y) \text{ for } h \in H} \\
	& \qquad \leq \frac{c}{(1 - p) m} \\
\end{split}
\]

Hence the system $H_p$ is $\frac{c}{1 - p}$-universal.
\end{proof}

Similar restrictions for the other universal systems may be found. Resulting probability is always $\frac{1}{1 - p}$ times higher than the original one following from the strong $k$-universality, in this case.

Next statement summarises results for the systems of linear transformations.

\begin{corollary}
\label{corollary-trimming-linear}
For every trimming rate $0 < p < 1$ the $p$-trimmed system of linear transformations, $LT(U, B)_p$, is $\frac{1}{1 - p}$-universal.
\end{corollary}
\begin{proof}
System of linear transformations is $1$-universal as seen in Remark \ref{remark-system-of-linear-transformations}. The fact then follows from the previous theorem.
\end{proof}

Theorem \ref{theorem-expected-chain-length-universal} implies that the expected chain length still remains constant provided that the load factor $\alpha$ is bounded. It naturally gets larger and certainly depends on our choice of $p$. The looser the value of the trimming rate is, the better expected results are obtained. However, for benevolent choices of $p$ the worst case warranty is adequately worsened.

Every limit $l(m, \alpha)$ comes with an associated trimming rate -- probability of the event $\lpsl > l(m, \alpha)$. This probability determines the expected number of tries to find a suitable function, as stated in the next lemma.

\begin{lemma}
\label{lemma-linear-transformations-tries}
Let $l$ be a chain length limiting function and $p$ be the trimming rate, \[ \Prob{\lpsl \geq l(m, \alpha)} \leq p \text{.} \] Then the expected number of tries to find a function, which does not create a long chain, is $\frac{1}{(1 - p)^2}$ and the variance of this number is finite.
\end{lemma}
\begin{proof}
The estimate of failing $k$ times in a row is clearly bounded by the geometric distribution with  the parameter $p$. The probability of the failure is thus less or equals $p ^ k$. The expected time of success is then given by:
\[
\sum_{i = 0}^{\infty} (i + 1)p^i = \sum_{i = 0}^{\infty}p^i + \sum_{i = 0}^{\infty}ip^i = \frac{1}{1 - p} + \frac{p}{(1- p)^2} = \frac{1}{(1 - p)^2} \text{.}
\]

Now we estimate the variance:
\[
\begin{split}
\sum_{i = 0}^{\infty} \left(i + 1 - \frac{1}{(1 - p) ^ 2}\right) ^ 2  
	& = \sum_{i = 0}^{\infty} \left(i + 1\right) ^ 2 p ^ i - \sum_{i = 0}^{\infty} \frac{2(i + 1)p ^ i}{(1 - p) ^ 2} +   \sum_{i = 0}^{\infty} \frac{p ^ i}{(1 - p) ^ 4} \\
	& = \frac{1}{p}\left(\sum_{i = 0}^{\infty} i ^ 2 p ^ i - 1 \right) - \frac{2}{(1 - p) ^ 4} + \frac{1}{(1 - p) ^ 5} \\
	& = \frac{1}{p}\left(\frac{p(1 + p)}{(1 - p) ^ 3} - 1\right) - \frac{2}{(1 - p) ^ 4} + \frac{1}{(1 - p) ^ 5}
	\text{.}
\end{split}
\]
\end{proof}

The most interesting and the most important idea of trimming is that every $p$-trimmed system is an adaptation of the original class to the stored set. We showed that in the expected case we need a constant number of tries to find a suitable function -- a function from the family $H_p$. In addition from Theorem \ref{theorem-p-trimmed-is-universal} it follows that the $p$-trimmed systems are still $\frac{c}{1 - p}$-universal where $c$ is the universality constant of the original system.

\section{Chain Length Limit}
\label{section-linear-systems-linear-amount-constant-estimate}
From now on we concentrate on obtaining a tightest possible limit on the chain length for a given trimming rate $p \in (0, 1)$. The corresponding bound is determined from the density function shown in Remark \ref{remark-lpsl-pdf-linear-amount}. This limit is used later in model from Section \ref{section-proposed-model}.

\begin{theorem}
\label{theorem-model-chain-limit-rule}
Let $T: \vecspace{u} \rightarrow \vecspace{b}$ be a random uniformly chosen linear transformation, $m = 2 ^ b$ and $S \subset \vecspace{u}$.
\begin{itemize}
\item If $|S| = 1.5 m$, then set $l(m) = 57.29 \log m \log \log m$.
\item If $|S| = m$, then set $l(m) = 47.63 \log m \log \log m$.
\end{itemize}
In either case $\Prob{\lpsl > l(m)} \leq 0.5$.

For $\alpha \in (0, \infty)$ and $p \in (0, 1)$ the chain limit $l(m) = a \log \log m + b \log m$ for $a, b \in \mathbb{R}$ depending only $\alpha$ and $p$.
\end{theorem}
\begin{proof}
First set $f(x) = x ^ {\log b - \log \alpha - \log x - \log \log x}$. From Corollary \ref{corollary-f1} it follows that the function $f(x)$ is decreasing in the interval $\left[2, \infty \right]$.

If the assumptions of Remark \ref{remark-lpsl-pdf-linear-amount} are satisfied we have that 
\[
	\Prob{\lpsl > 2 \alpha c_\epsilon r} \leq \frac{1}{1 - \epsilon} f\left(\frac{r}{\log r}\right) \text{.}
\]

We find the minimal value of $r$ such that $\frac{1}{1 - \epsilon} f\left(\frac{r}{\log r}\right) \leq p$. By setting the value of the variable $r$ we also obtain the chain limit confirming to the the prescribed trimming rate $p$. 

Our next step is to define a lower bound $d$, $d \geq 2$, of the expression $\frac{r}{\log r}$. Since $d \leq \frac{r}{\log r}$ we have that $f(d) \geq f\left(\frac{r}{\log r}\right)$ because the function $f$ is decreasing in the interval $[2, \infty)$. Whenever we have a value of the variable $d$ such that $f(d) \leq (1 - \epsilon) p$, then $f\left(\frac{r}{\log r}\right) \leq (1- \epsilon) p$. If we manage to find the minimal value of $r$ from $d \leq \frac{r}{\log r}$, then we set the chain limit to $2 \alpha c_\epsilon r$ and the trimming rate $p$ is achieved as well. 

First we show a way how to estimate the value of the variable $r$ for a given $d \geq 2$. 
\begin{claim}
\label{claim-choose-r}
If $d \geq 2$ and $r = 2d \log d$, then $d \leq \frac{r}{\log r}$ and $r \geq 4$.
\end{claim}
\begin{proof}
Since we selected $d$ as a lower bound for $\frac{r}{\log r}$ we have to find $r$ from the inequality $\frac{r}{\log r} \geq d$. Putting $r = 2 d \log d$ satisfies the inequality for every $d \geq 2$ since
\[
\frac{r}{\log r} = \frac{2 d \log d}{1 + \log d + \log \log d} = \frac{2 d}{1 + \frac{1}{\log d} + \frac{\log \log d}{\log d}} \geq \frac{2d}{2} = d \text{.}
\]

The value of $r$ is thus $r = 2d \log d \geq 4$.
\end{proof}

The probability estimate of the event $\lpsl > 2 c_\epsilon \alpha r$ requires that $r \geq 4$. For the choice of the value $r$ from Claim \ref{claim-choose-r} it follows that $r \geq 4$ when $d \geq 2$. Because we choose $d \geq 2$, we no longer pay attention to the assumption of Remark \ref{remark-lpsl-pdf-linear-amount}. The remaining one $\alpha < \frac{\log r}{2}$ may cause a problem. Its validity must be verified at the end, immediately after we state the exact choice of $r$.

To finish the proof set the lower bound $d = j b$ for a positive constant $j$. Instead of finding exact value of $d$, it is sufficient to find a minimal value of $j$. The simplification is motivated by the fact that we already know that the asymptotic growth of $\Expect{\lpsl}$. It is in the order of $b \log b$. We just need find as small multiplicative constant as possible. In Claim \ref{claim-whole-limit} we show that putting $d = jb$ respects this asymptotic growth.

\begin{claim}
\label{claim-whole-limit}
Let $j$ be a positive constant such that $d = jb \geq 2 $, then the chain limit rule we propose has the form \[ 4 c_\epsilon \alpha j b (\log b + \log j) \text{.} \]
\end{claim}
\begin{proof}
We choose the value of $r$ from Claim \ref{claim-choose-r} as
\[
	r = 2 d \log d = 2 j b (\log b + \log j) \text{.}
\]

Hence our chain limit can be finally rewritten as:
\[
	4 c_\epsilon \alpha r = 4 c_\epsilon \alpha j b (\log b + \log j) \text{.}
\]
\end{proof}

\begin{claim}
To find the minimal value of $d$, $d \geq 2$ satisfying $f(d) \leq (1 - \epsilon) p$ use the inequality
\stepcounter{definition}
\begin{equation}
\label{inequality-formula-j}
	\left(j b\right)^{-\log \alpha -\log j - \log \log (j b)} \leq (1 - \epsilon)p \text{.}
\end{equation}
\end{claim}
\begin{proof}
From remark Remark \ref{remark-lpsl-pdf-linear-amount} we obtain the following inequality that allows us to find the minimal suitable value of $d$:
\[
	f(d) = d ^ {\log b - \log \alpha - \log d - \log \log d} \leq (1 - \epsilon) p \text{.}
\]

We substitute $j b$ into $d$ to get the required result:
\[
	d ^ {\log b - \log \alpha - \log d - \log \log d} = \left(j b\right)^{-\log \alpha -\log j - \log \log (j b)} \leq (1 - \epsilon)p \text{.}
\]
\end{proof}

Recall that we use the hash table $B = \vecspace{b}$ and refer to $m = 2 ^ b$ as to its size. Inequality \ref{inequality-formula-j} may have various interpretations.
\begin{itemize}
\item For fixed $0 < p, \epsilon < 1$ and a positive constant $j$ we get a lower bound on $m$ when our estimate becomes valid. Realise, that we can obtain arbitrarily low multiplicative constant. This follows from the fact that we are allowed to choose values for the constant $j$. Such estimates are valid only for large numbers of stored elements, since we need $d = jb \geq 2$ and $n \geq \frac{m}{2} \geq 2 ^ {\frac{2}{j} - 1}$.
\item Or we can find the parameters $\epsilon$ and $j$ such that multiplicative constant $4 c_\epsilon \alpha j$ is the smallest possible for the trimming rate $p$ and the size of the hash table $m$. This statement is used to find the required estimate for $m \geq \text{4 096}$.
\end{itemize}

We use Inequality \ref{inequality-formula-j} to obtain the chain limit. The limit is computed for tables consisting of at least 4 096 buckets and the probability bound is set to $0.5$. These choices were not random. Our first tries with the formula showed that the multiplicative constants gained by its application are in the order of tens. Estimates with the multiplicative constant in the order of tens start beating the most basic linear estimate, $\lpsl \leq \alpha m$, when hashing thousands of elements.

Program optimising the value of the multiplicative constant only minimises the value $4 c_\epsilon \alpha j$ and does not pay any attention to the other constant. After the minimal value is retrieved, together with the values of the parameters, the value of the constant, $4 c_\epsilon j \log j b$, is determined. To find the value of the constant $c_\epsilon$ according to Inequality \ref{inequality-better-c-e} of Statement \ref{statement-better-c-e}, additional parameters $k$ and $l$ are required. We use Algorithm \ref{algorithm-scheme-3} to solve this optimisation problem.

The last statement of the theorem now follows from Algorithm \ref{algorithm-scheme-3}, which is able to compute the minimal constant $a$ and corresponding constant $b$, and Claim \ref{claim-whole-limit}.

\begin{algorithm}[ht!]
\caption{Calculate the multiplicative constant for parameters $p, m, \alpha, \epsilon, k, l$.}
\label{procedure-scheme-3}
\begin{algorithmic}
\STATE $c_\epsilon$ $\leftarrow$ constant $c_\epsilon$ computed with parameters $k$, $\epsilon$ and $l$
\STATE $r \leftarrow (1 - \epsilon)p$ \COMMENT{Right side, inevitably achieved bound.}
\STATE $j \leftarrow 1$
\STATE 
\STATE \COMMENT{Lower the value of $j$ so that the right side is skipped.}
\STATE $l \leftarrow (j \log m) ^ {-\log(\alpha) - \log(j) - \log(\log(j \log m))}$
\WHILE {$l < r$ \textbf{and} $j > 0$}
	\STATE $j \leftarrow j - STEP$;
	\STATE $l \leftarrow (j \log m) ^ {-\log(\alpha) - \log(j) - \log(\log(j \log m))}$
\ENDWHILE
\STATE
\STATE $j \leftarrow j + STEP$
\RETURN $4 c_\epsilon \alpha j$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht!]
\caption{Calculate the smallest limit for $p=0.5$, $m \geq \text{4 096}$ and prescribed $\alpha$.}
\label{algorithm-scheme-3}
\floatname{algorithm}{Procedure}
\begin{algorithmic}
\STATE $c \leftarrow \infty$
\FOR {$k \in \left[2, 4\right]\text{ with }STEP$}
	\FOR {$l \in \left[1.3, 3\right]\text{ with }STEP$}
		\FOR {$\epsilon \in \left[0.85, 0.99\right]\text{ with }STEP$}
			\IF {$c > \text{multiplicative constant for }p = 0.5, m = \text{4 096}, \alpha, \epsilon, k, l$}
				\STATE $c \leftarrow \text{computed multiplicative constant}$
			\ENDIF
		\ENDFOR
	\ENDFOR
\ENDFOR
\STATE
\RETURN $c$
\end{algorithmic}
\end{algorithm}
\end{proof}

The best result achieved for $\alpha = 1.5$, $m = 2 ^ b \geq \text{4 096}$ is for $\epsilon = 0.96$, $j = 0.74$ and equals $4 c_\epsilon \alpha j = 57.29$. The same approach gives multiplicative constant $47.63$ for $\alpha = 1$. The assumption $\alpha < \frac{r}{2}$ holds since $\alpha \leq 1.5 < 6 < jb \leq d \log d = \frac{r}{2}$.

Now compare this limit with the linear estimate, $\lpsl \leq \alpha m$, for the size of the hash table $m \geq \text{4 096}$ and the number of stored elements $n \geq \alpha m = 0.5 \cdot \text{4 096} = \text{2 048}$. Linear estimate on the length of the longest chain equals $n = \text{2 048}$. Our estimate equals approximately $57.29 \log m \log \log m \leq 57.29 \cdot 12 \cdot 2.27 \leq \text{1 555}$ which is already far better. 

These limits may be improved since we neglected the part $4 c_\epsilon \alpha j \log j \log m$ which is negative.

\section{The Proposed Model}
\label{section-proposed-model}
The model we propose is a slight modification of the simplest model of universal hashing scheme with separated chains. We distinguish the two cases -- when the delete operation is not allowed or when stored elements can be deleted.
\begin{itemize}
\item \textbf{Universal class.} We use the system of linear transformations as the universal family of functions. The universe $U = \vecspace{u}$ and the target space, the hash table, is referred to as $B = \vecspace{b}$. We may imagine the situation as hashing $u$-bit binary numbers to the space of $b$-bit binary numbers. We refer to the size of the hash table as to $m = |B| = 2 ^ b$.

\item \textbf{Load factor rule.} The load factor of the table is kept in the predefined interval. If the load factor is outside the interval, whole table is rehashed into a greater or smaller table. New size is chosen so that the load factor is near $0.5$ as possible. Load factor is maintained in the interval $\left[0.5, 1\right]$ for the scheme without the delete operation. We need the interval $\left[0.25, 1\right]$ if the delete operation is allowed.

At the beginning we create a quite large hash table of size 4 096 buckets. This size is determined by the assumptions of Theorem \ref{theorem-model-chain-limit-rule} which states the chain limit rule. When the size of the table is 4 096 buckets, then the lower bound of the load factor rule is not applied and we allow the table's load factor to be in the interval $[0, 1)$.

\item \textbf{Chain limit rule.} When there is a chain longer than the limiting value $l(m)$, then the hash table is rehashed. The value $l(m)$ is chosen so that the trimming rate $p = 0.5$ is achieved. Notice that the limiting value of this model does not depend on the load factor. The load factor for which we compute the limit is chosen so that we are able to perform the analysis of the expected amortised complexity in Theorem \ref{theorem-no-delete-time} and in Theorem \ref{theorem-delete-time}.

If the delete operation is allowed, then the limit value $l(m)$ is computed for $\alpha = 1.5$. When the delete operation is forbidden, the limiting value is computed for $\alpha = 1$. In Theorem \ref{theorem-model-chain-limit-rule} we state that for $p = 0.5$ the limit $l(m) = 47.63 \log m \log \log m$ when $\alpha = 1$ and $l(m) = 57.29 \log m \log \log m$ for $\alpha = 1.5$.

\end{itemize}

Either estimate for the chain limit rule we obtain in the following section is valid for the tables consisting of at least 4 096 buckets. There are two solutions how to address this problem. We may use tables with the initial size of $m = \text{4~096}$ elements. Since not every hash table grows to a size of 4~096 buckets, this overhead becomes unacceptable. The other option lies in turning off the chain limit rule when the table has less than 4~096 buckets. 

\section{Algorithms}
In Section \ref{section-proposed-model} we propose a model of universal hashing without any exact definition. Precise algorithms, showing how the operations work, are required. Despite the fact that the algorithms are very similar to those shown in Chapter \ref{chapter-hashing}, now they are described exactly.

Although Theorem \ref{theorem-model-chain-limit-rule} gives the chain length limit for $\alpha \in \{1, 1.5\}$ and $p = 0.5$, different parametrisation is possible as well. Algorithm \ref{algorithm-scheme-3} from the proof of the theorem is general enough to be able to compute the bound for arbitrary $\alpha > 0$ and prescribed $p \in (0, 1)$. According to Theorem \ref{theorem-model-chain-limit-rule} the limit function is in the form $a \log \log m + b \log m$ for $a, b \in \mathbb{R}$ depending only on $\alpha$ and $p$.

First, let us describe the hash table's variables. It contains variables storing its size $Size$, the number of represented elements $Count$ and array $T$ is the hash table itself. Every bucket $T[i]$ is associated with two variables $T[i].Size$ and $T[i].Chain$ -- its size and the linked list of the elements in the bucket. Function $Limit$ denotes the chain length limiting function and $Hash$ is the current universal function -- a linear transformation represented by a binary matrix.

Initialisation creates a new empty table with the prescribed size. It also chooses a universal function by the random initialisation of bits in the matrix $Hash$. The values of bits are chosen with the same probability $0.5$.

To enumerate the stored elements in the rehash operation, Algorithm \ref{algorithm-rehash}, we iterate through the table and through its chains. A common optimisation is to place all the stored elements into a linked list. This allows faster enumeration but causes a space overhead.

Whenever a load factor rule or the chain limit rule is violated, the table is rehashed using a new function chosen in initialisation. Both initialisation, Algorithm \ref{algorithm-initialisation}, and rehash, Algorithm \ref{algorithm-rehash}, take an argument $m$ specifying the new size, so the table is rehashed into a possibly larger or smaller table.

Find is very straightforward. Let us notice that the linked list manipulation operations return \textbf{true} in the successful case, if an element is found, deleted or inserted, and otherwise return \textbf{false}. Insert and delete  operations are slightly complicated compared to the original ones because of the both rules we require to be valid.

\begin{algorithm}[p]
\caption{Initialisation of the hash table}
\label{algorithm-initialisation}
\floatname{algorithm}{Initialisation of the hash table}
\begin{algorithmic}
\REQUIRE $\alpha \in (0, \infty)$, default according to the status of the delete operation
\REQUIRE $p \in (0, 1)$, default 0.5
\REQUIRE $m \in \mathbb{N}$, default 4 096
\STATE
\ENSURE the random uniform choice of a linear transformation $Hash$
\STATE
\STATE use Algorithm \ref{algorithm-scheme-3} to compute the bound $Limit(m)$ for prescribed $p$ and $\alpha$
\STATE initialise $\alpha_{min}$ and $\alpha_{max}$ according to the status of the delete operation
\STATE create the talbe $T$ of size $m$
\STATE $Size \leftarrow m$
\STATE $Count \leftarrow 0$
\STATE choose the hash function -- random binary matrix $Hash$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[p]
\caption{Rehash operation}
\label{algorithm-rehash}
\floatname{algorithm}{Rehash operation}
\begin{algorithmic}
\REQUIRE $m \in \mathbb{N}$, default $Size$ \COMMENT{New size of the hash table.}
\STATE
\REPEAT
	\STATE $T'.Initialise(m)$
	\STATE turn off the chain limit rule in $T'$
	\STATE turn off the load factor rule in $T'$
	\FORALL{element $x$ stored in the hash table}
		\STATE $T'.Insert(x)$
	\ENDFOR
\UNTIL{chain limit rule is satisfied in $T'$}
\STATE
\STATE swap $T$ and $T'$
\STATE turn on the chain limit rule in $T'$
\STATE turn on the load factor rule in $T'$
\STATE free $T$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[p]
\caption{Find operation}
\label{algorithm-find}
\floatname{algorithm}{Find operation}
\begin{algorithmic}
\REQUIRE $x \in U$
\STATE
$i \leftarrow h(x)$
\IF{T[i].Chain.Contains(x)}
	\RETURN \textbf{true} \COMMENT{Successful find.}
\ELSE
	\RETURN \textbf{false} \COMMENT{Unsuccessful find.}
\ENDIF
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ph!]
\caption{Insert operation}
\label{algorithm-insert}
\floatname{algorithm}{Insert operation}
\begin{algorithmic}
\REQUIRE $x \in U$
\STATE
$i \leftarrow h(x)$
\IF{T[i].Chain.Insert(x)}
%	\STATE \COMMENT{Set the number of stored elements.}
	\STATE $Count \leftarrow Count + 1$
	\STATE $T[i].Size \leftarrow T[i].Size + 1$
	\STATE
	\STATE $Rehash \leftarrow \mathbf{false}$
	\IF{$Count > Size * \alpha_{max}$} % \COMMENT{The load factor rule may be violated.}
		\STATE $NewSize \leftarrow 2 * Size$
		\STATE $Rehash \leftarrow \mathbf{true}$
	\ELSE
		\STATE $NewSize \leftarrow Size$	
	\ENDIF
	\STATE
%	\STATE \COMMENT{The chain limit rule may be violated.}
	\IF{$T[i].Size > Limit(Size)$}
		\STATE $Rehash \leftarrow \mathbf{true}$
	\ENDIF
	\STATE
%	\STATE \COMMENT{Rehash the table if needed, new function is chosen.}
	\IF{$Rehash$}
		\STATE $Rehash(NewSize)$
	\ENDIF
	\STATE
	\RETURN \textbf{true} \COMMENT{Successful insert.}
\ELSE
	\RETURN \textbf{false} \COMMENT{Unsuccessful insert.}
\ENDIF
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ph!]
\caption{Delete operation}
\label{algorithm-delete}
\floatname{algorithm}{Delete operation}
\begin{algorithmic}
\REQUIRE $x \in U$
\STATE
$i \leftarrow h(x)$
\IF{T[i].Chain.Remove(x)}
%	\STATE \COMMENT{Set the number of stored elements.}
	\STATE $Count \leftarrow Count - 1$
	\STATE $T[i].Size \leftarrow T[i].Size - 1$
	\STATE
%	\STATE \COMMENT{The load factor rule may be violated.}
	\IF{$Count < Size * \alpha_{min}$}
		\STATE Rehash($Size / 2$)
	\ENDIF
	\STATE
	\RETURN \textbf{true} \COMMENT{Successful delete.}
\ELSE
	\RETURN \textbf{false} \COMMENT{Unsuccessful delete.}
\ENDIF
\end{algorithmic}
\end{algorithm}

\section{Time Complexity of Computing a Hash Value}
\label{section-time-complexity}
Since our model is based on the system of linear transformations, what is quite unusual, we have to deal with the time complexity of computing an element's hash value. We ask if the time required to compute the hash value is still constant. When we bound the size of the universe $|U|$, certainly it is. But it may be worse, when compared to the time required by linear system \ref{definition-linear-system}. 

Linear system uses a constant number of arithmetic operations. If we do not bound the size of the universe $|U|$, we may compute them in $O(\log^2 |U|)$ time. 

Our system, system of linear transformations, represents every transformation by a binary matrix $M \in \vecspace{b \times u}$. Every element $\vec{x} \in U$ is a vector consisting of $u$ bits. To obtain its hash the matrix-vector multiplication $M\vec{x}$ is performed. We can certainly do it in $O(u b) = O(\log |U| \log |B|)$ time. 

If the size of the universe is bounded, this time is constant. Despite the better asymptotic bound, in practise, the time is worse than that of linear system. Assume, that we represent elements by a word determined by the computer's architecture. In this case linear system needs just three operations, one multiplication, one addition and a modulo. Computing a matrix-vector multiplication can be optimised, but is not so fast. 

In addition, when hashing is applied, elements usually fit in the word of the underlying architecture. Therefore the arithmetic operations become constant and this is our case, too. Sometimes it is possible to cache once computed hash value within the element. This solution is a trade-of that improves the time complexity but consumes additional memory.

\section{Potential Method} 
In Section \ref{section-time-complexity} we estimate the expected amortised time complexity using the potential method. So let us explain it first. Assume that we have a data structure and a sequence of operations $\{o\}_{i = 1}^{n}$ performed on it. We want to estimate the running time of the sequence. Of course, we can use the worst case time for each operation but this may be very misleading. 

Consider a simple array. The elements are placed at the array's end. If there is a free position, we store the element. When the array is full, then we double its size and store the element. Question is, what time is needed to store $n$ elements in such array. The worst case analysis gives the result $O(n ^ 2)$. Amortised analysis gives a far better result, $O(n)$. This result can be explained by the fact that the fast inserts accumulated some potential that is later used by a following slow insert. The amortised time of the operation thus may be seen as the time consumed by all operations uniformly distributed across the operations. To bound the amortised complexity of every operation in a sequence of length $n$, we must take the supremum through all the sequences of the same length $n$.
\begin{definition}[Amortised complexity]
Let $\{o\}_{i = 1}^{n}$ be the sequence of $n \in \mathbb{N}$ operations. Let $T_o$ be the time required to perform the operations $o_i$ for $i = 1, \dots, n$ in this order. Then the \emph{amortised complexity} of an operation is $\lim\limits_{n \rightarrow \infty}\frac{\sup_{o, |o| = n}T_o}{n}$.
\end{definition}

Now we describe the potential method -- a method how to estimate the amortised complexity. First, we define a potential function, $\{p\}_{i = 0}^{n}$. The constant $p_0$ is the starting potential, often chosen as zero. After each operation the data structure is associated with a new potential $p_i$. The operations not only change the data structure, they change its potential, too. Put $a_i = t_i + p_{i} - p_{i - 1}$ as the amortised complexity of the $i$\textsuperscript{th} operation where $t_i$ is its running time. The time taken by the operations in a sequence $o$, $T_o = \sum\displaystyle_{i = 1}^{n} t_i$, may be estimated by the amortised time of the sequence, $A_o = \sum\displaystyle_{i = 1}^{n} a_i$, as
\[
	T_o = \sum\displaystyle_{i = 1}^{n} t_i = \sum\displaystyle_{i = 1}^{n} (a_i - p_i + p_{i - 1}) = A_o + p_0 - p_n \text{.}
\]

Let $p^o$ denote the potential after performing the sequence $\{o\}_{i = 1}^{n}$. Amortised complexity of an operation is hence
\[
	t = \lim\limits_{n \rightarrow \infty} \frac{\sup\displaylimits_{o, |o| = n} T_o}{n} = \lim\limits_{n \rightarrow \infty} \frac{\sup\displaylimits_{o, |o| = n} (A_o + p_0 - p^o)}{n} \text{.}
\]

Let us show some facts regarding the potential functions and amortised complexity. Also let us note that the first approach of the following remark is used later in Theorem \ref{theorem-delete-time}.
\begin{claim}
\label{claim-amortised-complexity}
Assume that we estimate the amortised complexity by a potential function.
\begin{enumerate}
\item [(1)] If $p_0 = 0$, then $T = A - p^o$. An operation's amortised complexity then equals \[ \lim\limits_{n \rightarrow \infty} \frac{\sup\displaylimits_{o, |o| = n} A_o}{n} - \lim\limits_{n \rightarrow \infty} \frac{\sup\displaylimits_{o, |o| = n} p^o}{n} \text{.} \] If for every sequence $\{o_i\}_{i = 1}^{n}$ the value $a_i$ is constant and $\lim\limits_{n \rightarrow \infty} \frac{\sup\displaylimits_{o, |o| = n} p^o}{n} < \infty$, then the amortised complexity of every operation is constant.
\item [(2)] If $p_i \geq 0$, then $T \leq A + p_0$. Moreover, if $p_0 = 0$, then $T \leq A$.
\item [(3)] If $p_i \leq 0$, then $T \leq A - p^o$.
\end{enumerate}
\end{claim}

For randomised algorithms we take the expected time consumed by the sequence. We also could take the expectation before the limit and have the choices for the algorithm given in advance. Since the lengths of the sequences are prolonging to infinity and so the running times and the number of random choices, we would need infinite sequences of random answers. Thus, it seems easier to find the amortised complexity in the expected case.

\begin{definition}[Expected amortised complexity]
Let $\{o\}_{i = 1}^{n}$ be the sequence of $n \in \mathbb{N}$ operations of a randomised algorithm. Let $T_o$ be the time required to perform the operations $o_i$ for $i = 1, \dots, n$ in this order. Then by the \emph{expected amortised complexity} of an operation we understand \[ \lim\limits_{n \rightarrow \infty}\frac{\sup_{o, |o| = n}\Expect{T_o}}{n} \text{.} \]
\end{definition}

This description of the potential method may be found in \cite{VK-skripta}.

\section{Expected Amortised Complexity}
Expected amortised time complexity of the introduced scheme is analysed in the next two theorems in either of the two cases -- if the delete operation is allowed or not.

\begin{theorem}
\label{theorem-no-delete-time}
Let $m$ be the initial size of a hash table confirming to the model with the forbidden delete operation. Assume that the number of successful insert operations performed is $2^k m$ for $k \in \mathbb{N}$. Then the expected amortised complexity of every performed operation, find or insert, is $O(1)$.
\end{theorem}
\begin{proof}
For the amortised complexity analysis we use the potential method. Let the expression $\alpha m - m$ denote the potential of the scheme. This is the negative value of the number of the remaining successful insert operations which would make the table's load factor reach one. Whenever a successful insertion is performed we check if 
\begin{itemize} 
\item the prolonged chain does not violate the chain limit rule or
\item the load factor is not greater than $1$.
\end{itemize} If either of the two conditions is violated the whole table is rehashed. We have four cases to analyse.
\begin{itemize}
\item \emph{Find operation} or an \emph{unsuccessful insertion} is performed. From Theorem \ref{theorem-expected-chain-length-universal} and Corollary \ref{corollary-trimming-linear} we know that it takes $O(1)$ time only. It can not change the table's potential.

\item \emph{Insert operation} is performed and the \emph{rehash} operation is not necessary. Then from the previous Theorem (\ref{theorem-expected-chain-length-universal}) the operation itself takes $O(1)$ time in the expected case. The potential is also increased by one. So the operation consumed $O(1)$ amortised time.

\item The rehash after an \emph{insertion} is required because the \emph{load factor} exceeds one. The rehash itself takes the $O(m)$ time and the size of the table is doubled. Thus the resulting potential is $-m$. The potential before the operation equals $0$. So the operation took only $O(1)$ amortised time.

\item Suppose that a rehash operation after an \emph{insertion} is needed because of the \emph{violation of the chain limit rule}. We seek for a new function satisfying the rule and without resizing the table. A single rehash operation takes the $O(m)$ time. In the following we refer to the \emph{cycle} as to the two immediate rehashes caused by the load factor rule. We find out how many rehash operations caused by the chain limit rule violation can occur in a single cycle. 

Seeking a suitable function for the stored set $S$ during a cycle is remarkably similar to seeking it at the cycle's end. This fact has more causes. First realise that the chain limit depends only on the size of the hash table and is computed for $\alpha = 1$. Moreover we can not delete elements, they can only be added. We can see the process of finding a suitable function in the middle of a cycle in the following manner. When we select a function from the family $H$ we ask if the function is fine for the stored set $S$. What if we asked whether the selected function is suitable for a set $S_e$ that will be represented at the end of the cycle. At the end of the cycle we have $S \subset S_e$, $m = |S_e|$ and $\alpha = 1$. Thus the probability bounds remain the same. Although we do not know the stored set $S_e$ in advance, it is not a problem. We placed no further assumptions on the represented set $S_e$. Since $S_e$ contains all the currently stored elements, $S \subset S_e$, we can afford this simplification. Hence after $\frac{1}{(1-p)^2}$ choices of a hash function we are expected to find a suitable one. The expected number of the rehashes in a single cycle caused by the violation of the chain limit rule is then bounded by the same expression. 

Now we compute the amortised complexity of the insert operations causing the violation of the chain limit rule in a single cycle. There are exactly $0.5 m$ insert operations raising the load factor from $\alpha = 0.5$ to $\alpha = 1$ for the table of size $m$. The expected time spent by fixing the chain limit rule in a cycle is then $\frac{1}{(1 - p)^2}O(m)$. We can distribute this amount of time along the cycle of $0.5 m$ inserts. This does not violate $O(1)$ expected amortised time for every insert operation.
\end{itemize}

Slightly weaker statement holds, if the number of the insert operations is not a power of two. We have one issue to care about. In the last case we distributed the time evenly throughout the cycle. If the number of inserts is not a power of two, we could distribute a long time over a non-complete cycle. Then we can not obtain a constant amortised time for the insert operation. However, for complete cycles the complexity estimates hold.

However, we can distribute time spent on fixing the chain limit rule from an incomplete cycle along all the insert operations performed. Amortised time of the insert operation then converges to $O(1)$ with a growing number of insert operations.
\end{proof}

One can find a better potential function proving the previous result more formally. We use such an explicitly expressed potential function in the next theorem. We assume that the delete operation is allowed.
\begin{theorem}
\label{theorem-delete-time}
If the initial hash table is empty and the delete operation is allowed, then the expected amortised time complexity of every operation is constant.
\end{theorem}
\begin{proof}
In this proof we have two types of the operation cycles. We need to distinguish between the work performed to enforce the load factor rule and the time spent by keeping the chain limit rule. In the previous theorem these cycles were the same, we did not permit the delete operation. And recall the difference, the limiting value is computed for $\alpha = 1.5$.

We deal with the amortised time of the find and unsuccessful insert or delete operation in advance. Their expected running time is proportional to the expected chain length. From Theorem \ref{theorem-expected-chain-length-universal} it follows that this value is constant. The find operation does not change the potential\footnote{The potential is defined later in the proof.}. Our analysis is thus simplified by omitting the find and unsuccessful delete and insert operations.

Let the sequence $O = \{o_i\}_{i=1}^o$ denote the performed operations, the $i$\textsuperscript{th} operation $o_i \in \lbrace Insert, Delete \rbrace$. The following two definitions make the analysis more exact and clear.

\begin{definition}[$\alpha$-cycle]
The $\alpha$-cycle consists of the operations between the two immediate rehash operations causing the violation of the load factor rule. Every $\alpha$-cycle contains the last operation -- the one causing the violation.
\end{definition}
First, notice that for this definition it is not important if the upper or lower bound of the load factor is the cause of the violation. Also note that the load factor $\alpha$ is now in the interval $\left[0.25, 1\right]$ since the delete operation is allowed, too. When a rehash operation is executed, the table size is chosen so that the value $\alpha$ is near $0.5$ as possible. 

The next definition of the $\alpha$-cycle is intended for the analysis of the time spent by fixing the violations of the load factor rule.

\begin{definition}[l-cycle]
The l-cycles are the partitioning of the sequence $O$ such that every l-cycle ends
\begin{itemize}
\item when the rehash operation caused by the load factor rule is needed or
\item we performed $0.5 m$ insert operations from the start of the l-cycle and the load factor did not exceed the value of $1$.
\end{itemize}
\end{definition}
The l-cycle allows us to analyse the work caused by the chain limit rule. Both l-cycles and $\alpha$-cycles divide the sequence $O$. Notice that if an $\alpha$-cycle ends at the position $i$, the corresponding l-cycle also ends at the same position. 

The analysis now takes the $i$\textsuperscript{th} operation for every $i = 1, \dots, o$. We show that the expected amortised time complexity of the opertion $o_i$ is $O(1)$ independently on its type. The potential now consists of the two parts, $p_1$ and $p_2$. Let $e = \frac{1}{(1 - p) ^ 2}$ denote the expected number of rehash operations, the expected number of tries, when finding a suitable function. 

Above all we want every simple insertion and deletion to take $O(1)$ time only. Thus the potential consists of the part $p_1 = 4ei_{\alpha} + 8ed_{\alpha}$ where $i_{\alpha}$ is the number inserts and $d_{\alpha}$ is the number of delete operations performed so far in the current $\alpha$-cycle. 

Next we need to distribute the work caused by the chain limit rule. The second part of the potential $p_2 = 2ei_{l} + (ce - r) m$ where $i_l$ is the number of insertions performed so far in the current l-cycle. The variable $r$ denotes the number of rehash operations caused by the chain rule violation and $c$ denotes the number of l-cycles occurred from the beginning so far. The overall potential is mixed as $p = p_1 + p_2$.

The analysis of the delete operation is simpler and comes first. When a deletion is performed we have to discuss the two cases:
\begin{itemize}
\item Simple deletion that takes expected $O(1)$ time because we search in a chain of expected length $O(1)$. The potential is increased by $8e$ since the number of deletions in $p_1$ gets increased by one.
\item The load factor $\alpha$ violates the lower bound of the load factor rule. Realise that there are $0.25 m$ deletions performed in the current $\alpha$-cycle, let us explain why. At the beginning of the cycle the table has the size of $m$ slots and the load factor equals $0.5$. At the end of the cycle the load factor decreased to $0.25$. Such a descent can be caused by $0.25 m$ successful delete operations at least. Let us discuss the potential change. First, the difference of the $p_1$ part of the potential $p$ is $-2em$ at most, since $i_{\alpha}$ and $d_{\alpha}$ get zeroed. The second part gets increased by at most $em$ since a new l-cycle is started and the $i_l$ is zeroed. Rehashing of the table takes $O(em)$ expected time. Hence the amortised time of the operation equals $O(1)$. 
\end{itemize}

The analysis of the first two cases of the insert operation remains similar to that in the previous proof:
\begin{itemize}
\item Suppose no rehash operation after an insert was performed. The searched chain had the constant expected length and the potential was increased by $4e + 2e$. So the amortised time of the operation is constant.
\item If the load factor exceeds the upper bound there are at least $\frac{m}{2}$ insertions in the current $\alpha$-cycle. It follows that the potential $p_2$ is certainly greater or equals $2em$. After performing the operation a new l-cycle is started. The potential $p_2$ is raised by $em$ because the variable $c$ gets incremented by one. But it is also lowered by at least $2em$ because $i_l$ is set to zero. The rehash operation is expected to take $O(em)$ time. Regarding the fact that the potential $p_1$ may only decrease we have $O(1)$ amortised time for the operation.
\item Insert operation is the last one in the l-cycle and the chain limit rule is not violated. Then there are at least $0.5 m$ insertions and the accumulated potential denoted by the variable $p_2$ is equal to $em + (ce - r)m$. Since a new l-cycle is started the $i_l$ term is set to zero and the value $c$ is incremented by one. Therefore there is no potential change.
\item Only the insert operations violating the chain limit rule remain. Time consumed by rehashing the table is $O(m)$ times the number of fails when finding a suitable function. Regarding that the potential $p_2$ is decreased by the same value, since the variable $r$ gets incremented by the number of fails, the amortised time complexity of the operation is thus constant. 

We clarify why the expected number of fails in one l-cycle is equal to $e$. The reason is similar to the reason why we expect $em$ time to enforce the chain limit rule in one cycle of the previous proof. Because the load factor is maintained lower than 1 we need at least $0.5 m$ successful insert operations to raise it above $1.5$. Therefore inserts in one l-cycle, when omitting deletes, can cause raise of the load factor to $1.5$ at most. Since the limiting value $l(m)$ is computed for the $\alpha = 1.5$ and $S \subset S_e$ we expect only $e$ rehash operations in one l-cycle to enforce the chain limit rule.
\end{itemize}

We already showed that $a_i = t_i + p_i - p_{i - 1}$ is constant for every operation. Next, we need that $\frac{(ce - r)m}{o} = 0 $ in the expected case. This holds and thus the amortised complexity in the expected case is constant, Claim \ref{claim-amortised-complexity}.

To be precise when proving the previous facts, define the random variable $C_i$ as the number of rehashes required to fix the chain limit rule in the $i$\textsuperscript{th} l-cycle for $i = 1, \dots, c$. From Lemma \ref{lemma-linear-transformations-tries} it follows that $\Expect{C_i} = e$. Clearly $r = \sum\displaylimits_{i = 1}^{c} C_i$, $o \geq m > 0$. From Lemma \ref{lemma-expected-value-properties} it follows that
\[
	\Expect{\frac{(ce - r)m}{o}} \leq \frac{m}{m}\left(ce - \Expect{\displaystyle\sum_{i = 1}^{c} C_i}\right) = ce - c\Expect{C_1} = c(e - e) = 0 \text{.}
\]

The remaining parts of both potentials $p_1$ and $p_2$ are non-negative, thus $p \geq 0$. The starting potential is zero. So from Claim \ref{claim-amortised-complexity} we have that the amortised complexity of every operation is constant, in the expected case.
\end{proof}

Fact $\left|\frac{(ce - r)m}{o}\right| \xrightarrow{o \rightarrow \infty} 0$ might indicate that it is possible to show that the amortised complexity is constant, even without any expectation. Since universal hashing is a randomised algorithm, such result would be certainly remarkable. However, if the statement was really true, there still would by many troubles along the way.

So, one may ask if using the Law of Large Numbers, Theorem \ref{theorem-weak-law-of-large-numbers}, can not help. Indeed, from Lemma \ref{lemma-linear-transformations-tries} we have that $\Variance{C_i}$ is finite and it may be applied:
\[
\begin{split}
\left|\frac{(ce - r)m}{o}\right|
	& \leq m \left|\frac{ce - \sum\displaylimits_{i = 1}^{c} C_i}{c}\right| \\
	& \leq m \left|\frac{c\Expect{C_1}}{c} - \frac{\sum\displaylimits_{i = 1}^{c} C_i}{c}\right| \\
	& = m \left|\Expect{C_1} - \frac{\sum\displaylimits_{i = 1}^{c} C_i}{c}\right| \text{.}
\end{split}
\]

Clearly, the convergence in probability stated by Theorem \ref{theorem-weak-law-of-large-numbers} holds:
\[
	\left|\Expect{X_1} - \frac{\sum\displaylimits_{i = 1}^{c} X_i}{c}\right| \xrightarrow{c \rightarrow \infty} 0 \text{.}
\]

But in the case of $m \left|\Expect{C_1} - \frac{\sum\displaylimits_{i = 1}^{c} C_i}{c}\right|$ we have to be careful. There is an infinite class of sequences $\{o\}_{i = 1}^{n}$ for which $c \in \Omega(m)$. Thus for these sequences, if $m \rightarrow \infty$, then $c \rightarrow \infty$ as well.

In order to obtain convergence in probability we have that
\[
\begin{split}
\Prob{m \left|\Expect{C_1} - \frac{\sum\displaylimits_{i = 1}^{c} C_i}{c}\right| \geq \epsilon} 
	& = \Prob{\left|\Expect{C_1} - \frac{\sum\displaylimits_{i = 1}^{c} C_i}{c}\right| \geq \frac{\epsilon}{m}}  \\
	& = \frac{\Variance{C_i} m ^ 2}{m \epsilon ^ 2} = \frac{m\Variance{C_i}}{\epsilon ^ 2} \text{.}
\end{split}
\]
This is a problem, since when $m \rightarrow \infty$, we can not expected that the probability converges to zero for fixed $\epsilon$. 

 For the class of sequences satisfying $c \in \Omega(m)$ the inequalities we used for estimate of $\left|\frac{(ce - r)m}{o}\right|$ mean only a multiplicative factor. Thus the estimate is tight and wee see that the bound obtained from the Chebyshev's inequality is not sufficient. The asymptotic rate is not high enough and the factor $m$ appears. However, we know the probability distribution of $C_i$. Maybe we can exploit it and be more accurate. This problem remains open.



