\chapter{Amortised complexity of a hashing scheme}

% TODO: ktore schemy nevedia delete, alebo nie je znamene, zmienit dynamizaciu
The hashing scheme we propose later in this chapter is a solution to the set representation problem. Every such solution provides us basic with some operations. These allow obtaining an information if an element is stored within the set, accessing the stored elements and inserting the element. Some schemes do not implement the element removal or the efficient implementation is not known.

% TODO: definicia vahovo vyvazeneho stromu.
The must crucial are the operations' running times. Various solutions to different problems prefer different operations. For instance some applications modify stored data rarely but the other storages are used for data that is almost never queried. So the running times of some operations are considered more important. In the case of a simple array we have $O(1)$ time for the find procedure and $O(n)$ insertion or deletion time in the canonical case. Better bounds can be obtained using the amortised complexity. Routines of balanced trees' are typically bounded by the logarithmic time. As already mentioned the right answer what is a good choice for data structure that should be used lies in the estimated structure of operations. This choice can be an asymptotic improvement, too. Anyway this does not change the fact that the short running times are welcomed.

We analyse the running times of the universal hashing first by mentioning the known facts. Then we extend this analysis to the universal hashing using the system of linear transformations over vector spaces. We will guarantee the worst case complexity of the find operation.

\section{Expected time of the find operation in the classical universal hashing}
In this section we assume that the used system of hashing functions is at least $c$-universal. The running time of the find operation is proportional to the number of collisions in the bucket where the searched element should be placed according to its hash value. The obvious worst case is $O(n)$ where $n$ is the number of elements hashed. The universal hashing gives a far better expected estimate, $O(1)$.

\begin{theorem}
\label{theorem-expected-chain-length-universal}
Expected time of the find operation in the universal hashing using a $c$-universal system is $O(1)$ if the load factors are upper bounded.
\end{theorem}
\begin{proof}
The time of the find operation is given by the length of the chain into which the searched element falls. We must also add the time for hashing the element but this can be typically done in one unit of time. The expected length of a chain containing the element $x \in U$ must be computed. The expectation is taken over the family of hash functions and the chain's length is the sum of indicator variables:
\begin{displaymath}
\frac{\sum\displaylimits_{h \in H} \sum\displaylimits_{y \in S - \lbrace x \rbrace} P(h(x) = h(y))}{|H|} = \frac{\sum\displaylimits_{h \in H} \sum\displaylimits_{y \in S - \lbrace x \rbrace} \frac{c}{m}}{|H|} = \frac{cn}{m}
\end{displaymath}

Since the element $x$ may be present in the set $S$ the time is bounded by the expression $1 + c\alpha$ where $\alpha$ is the table's load factor. The typical load factor is lower than 1, we only assumed that it is bounded. It is still enough to see that the expected time of every operation is still in $O(1)$.
\end{proof}

\section{Effects of trimming long chains on a universal system}
The model of hashing that we present guarantees the worst case bound on the length of the longest chain or equivalently running times of all operations. Knowledge of $\Expect{\lpsl}$ provides a limit when to stop inserting into a long chain and whole table needs to be rehashed. The algorithm finds a more suitable function which does create such long chains. This limit is set according to the probability of existence of a long chain.

Some formal computations follow to support these ideas. Fact that \[ \Prob{\lpsl > k \Expect{\lpsl}} \leq \frac{1}{k} \] may be obtained by direct use of Markov inequality. This means that less than half of all functions create longest chains longer than $2 \Expect{\lpsl}$. In case of system of linear transformations it means that the limit is $2\Expect{\lpsl} \leq \text{1 076} \log m \log \log m + 88$ for $\epsilon = 0.8$. The choice of $\epsilon$ corresponds to selection of minimal multiplicative constant.

Expected length of the longest chain gives us the generic hint when we should rehash the table to guarantee the worst case time for the member operation. The lower the value $\Expect{\lpsl}$, or tighter its estimate, is the better worst case limit on a operation's running time is achieved.

Mentioned probability bound computed from the expected value can be improved. Probability density function of $\lpsl$ can be used directly if it is known. For linear systems we have already found it. In addition we have consequently improved it in a few previous theorems. To sum up, the approach with the expected length is more general but achieves worse performance. It is more general because whenever we know the probability density function we are able to compute the expected value.

Probability bound $p$, $0 < p < 1$, on existence of a long chain has important consequences for the supposed model.
\begin{itemize}
\item At most the fraction $p$ of all functions cause long chains; longer than prescribed limit. 
\item The probability of rehashing, selecting inconvenient function, for its uniform choice is lower than $p$.
\item During rehashing caused by appearance of a long chain probability of finding a suitable function is at least $1 - p$. This statement holds under the assumption of the uniform choice of an universal function.
\end{itemize}

\begin{definition}
Let $H$ be the universal system mapping an universe $U$ to a hash table $B$. Let $m$ be size of the hash table and $\alpha$ its load factor. Let $l(m, \alpha)$ be a chain length limiting function and $S \subset U$ be a hashed set. 

We say that function $h \in H$ creates a long chain if there is a chain of length longer than limiting value $l(m, \alpha)$.

Moreover let $0 < p < 1$ be probability such that and $\Prob{\lpsl \geq l(m, \alpha)} \leq p$ then system $H_p = \{ h \in H \setdelim h \text{ does not create a long chain} \}$ is called $p$-trimmed system.
\end{definition}

\begin{lemma}
\label{lemma-size-of-trimmed-system}
If $H_p$ is a $p$-trimmed universal system then the number of functions is $(1 - p)|H|$ at least.
\end{lemma}
\begin{proof}
\[
\begin{split}
|H_p|
	& = |\lbrace h \in H \setdelim \text{ h does not create a long chain} \rbrace| \\
	& =\Prob{\lpsl < l(m, \alpha)} |H| \\
	& = \left(1 - \Prob{\lpsl \geq l(m, \alpha)}\right) |H| \\
	& \geq (1 - p)|H| \text{.}
\end{split}
\]
\end{proof}

Regarding that every function is chosen uniformly and the unsuitable ones are discarded we still perform uniform selection. But now it is performed among functions that does not create long chains only. Note that the restriction on the functions of the universal system is done by using the information about the hashed set.

\begin{theorem}
Let $H$ be an universal system of hash functions mapping an universe $U$ to a hash table $B$. Let $0 < p < 1$ be the trimming rate and set $m = |B|$. Then system of functions $H_p$ is still universal with the constant of universality multiplied by factor $\frac{1}{1 - p}$. Equivalently:
\[
	\Prob{h(x) = h(y) \text{ for } h \in H_S} \leq k \Prob{h(x) = h(y) \text{ for } h \in H} \text{.}
\]
\end{theorem}
In the probability statement we are force to choose function $h \in H_p$.
\begin{proof}
Lemma \ref{lemma-size-of-trimmed-system} combined with $c$-universality of the original system gives $\frac{c}{1 - p}$-universality of the $p$-trimmed system. 

\[
\begin{split}
& \Prob{h(x) = h(y) \text{ for } h \in H_p}  \\
	& \qquad =  \frac{|\{ h \in H \mid h(x) = h(y) \wedge \textit{ h does not create long chains} \}|}{|\lbrace h \in H \mid \textit{ h does not create long chains} \rbrace|} \\
	& \qquad \leq \frac{|\{ h \in H \mid h(x) = h(y) \wedge \textit{ h does not create long chains} \}|}{(1 - p)|H|} \\ 
	& \qquad \leq \frac{|\{ h \in H \mid h(x) = h(y) \}|}{(1 - p)|H|} \\
	& \qquad = \frac{1}{1 - p} \Prob{h(x) = h(y) \text{ for } h \in H} \\
	& \qquad \leq \frac{c}{(1 - p) m} \\
\end{split}
\]

The universality constant is hence $\frac{c}{1 - p}$.
\end{proof}

Similar restrictions for the other universal systems may be found. For such systems probability of collision of two (or more) elements is estimated by their strong $k$-universality. Resulting probability is always $\frac{1}{1 - p}$ times higher than the original one.

Next statement summarises results for the systems of linear transformations that we examine.

\begin{corollary}
\label{corollary-trimming-linear}
For every trimming rate $0 < p < 1$ the system of linear transformations, $LT(U, B)$ is $\frac{1}{1 - p}$-universal.
\end{corollary}
\begin{proof}
System of linear transformations is $1$-universal as seen in remark \ref{remark-system-of-linear-transformations}. Because of the previous statement it is clear that the system $LT(U, B)_S$ is still $\frac{1}{1 - p}$-universal.
\end{proof}

Previous computations show that the expected chain length is still constant but gets larger and depends on our choice of $p$. The looser the value of trimming rate is the better expected results are obtained but the warranty for the worst case is adequately worsened. 

The probability bound provides us with an estimate how many functions have to be tried to find a suitable one creating short chains only.

\begin{lemma}
\label{lemma-linear-transformations-tries}
If the hash function for a table with the size of $m$ elements and with the chain limit setup $\Prob{\lpsl \geq l(m, \alpha)} \leq p$ is sought the expected number of tries is $\frac{1}{(1 - p)^2}$.
\end{lemma}
\begin{proof}
The estimate of failing $k$ times is bounded by the geometric distribution and this probability is certainly less or equal than $p^k$. So the expected time of success is given by:
\begin{displaymath}
\sum_{i = 0}^{\infty} (i + 1)p^i = \sum_{i = 0}^{\infty}p^i + \sum_{i = 0}^{\infty}ip^i = \frac{1}{1 - p} + \frac{p}{(1- p)^2} = \frac{1}{(1 - p)^2}
\end{displaymath}
\end{proof}

The most interesting idea of trimming is that system $H_p$ is an adaptation of the original class to hashed sets. We showed that finding a suitable function takes constant time in the expected case and moreover restricted system is still universal. Hence all operations using system $H_p$ take constant expected time, too.

\section{Bound estimate based on probability density function}
\label{section-linear-systems-linear-amount-constant-estimate}
From now on we concentrate on obtaining a bound on the length of the chain such that probability of rehashing because of the chain length must be lower than $p$. The corresponding bound is computed from the density function shown in the proof of remark \ref{theorem-hashing-linear-amount}. Let us note that the following probability estimate works for $r \geq 4$ only. 

\[
\begin{split}
\Prob{\lpsl \geq 2\alpha c_\epsilon r} & \leq p \\ 
\frac{1}{1 - \epsilon}\left(\frac{r}{\log r}\right)^{\log \log m - \log \alpha - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} & \leq p \\
\end{split}
\]

Define a function $f(d) = d ^ {\log \log m - \log \alpha - \log d - \log \log d}$. At first we show that function $f(d)$ is decreasing in the interval $\left[ d', \infty \right]$. We chose $d'$ so that it is minimal among $d' > 1$ and $\log \log m - \log \alpha - \log d' - \log \log d' < 0$. 

To show monotonicity of f let $d' \leq d_1 < d_2$.
\[
\begin{split}
f(d_2) 
	& = {d_2} ^ {\log \log m - \log \alpha - \log d_2 - \log \log d_2} \\
	& < {d_1} ^ {\log \log m - \log \alpha - \log d_2 - \log \log d_2} \\
	& < {d_1} ^ {\log \log m - \log \alpha - \log d_1 - \log \log d_1} \\
	& = f(d_1) \\
\end{split}
\]

The bound on $\lpsl$ for rehashing the table with probability $p$ equals $2 c_\epsilon \alpha r$ where $r$ must be found. At first we define a tight lower bound $d$ on expression $\frac{r}{\log r}$ and substitute it into function $f$ then \[ f(d) \geq f\left(\frac{r}{\log r}\right) \text{.} \] Whenever minimal $d$ such that $f(d) \leq (1 - \epsilon) p$ is found then wanted probability bound is satisfied, too.
\[ 
\begin{split}
\Prob{\lpsl \geq 2 \alpha c_\epsilon r} 
	& = \frac{1}{1 - \epsilon} f\left(\frac{r}{\log r}\right) \\
	& \leq \frac{1}{1-\epsilon}f(d) \leq p \\
\end{split}
\]
From the lower bound $d$ of the expression $\frac{r}{\log r}$ variable $r$ can be expressed. The estimate on the length of the longest chain such that probability of rehashing is lower than $p$ is then finished.

Let us define the lower bound $d$ as $d = k \log m$ and $d \leq \frac{r}{\log r}$ for a positive constant $k$. Following inequality allows us to find minimal suitable $d$.
\[
d ^ {\log \log m - \log \alpha - \log d - \log \log d} \leq (1 - \epsilon) p \text{.}
\]

Since we selected $d$ as a lower bound for $\frac{r}{\log r}$ we have to express $r$ from $\frac{r}{\log r} \geq d$. Putting $r = 2 d \log d$ satisfies the inequality for every $d \geq 2$.
\begin{displaymath}
\frac{r}{\log r} = \frac{2 d \log d}{1 + \log d + \log \log d} = \frac{2 d}{1 + \frac{1}{\log d} + \frac{\log \log d}{\log d}} \geq d
\end{displaymath}
One of the assumptions for using probability estimate of event $\lpsl > 2 c_\epsilon \alpha r$ is $r \geq 4$. It is certainly satisfied when choosing $d \geq 2$ since $r = 2 d \log d \geq 4$.

We substitute into $d$ and find the explicit dependence of $r$ on the table's size $m$:
\begin{displaymath}
r = 2 d \log d = 2 k \log m (\log k + \log \log m) = 2 k \log m \log \log m + 2 k \log k \log m \text{.}
\end{displaymath}

Our estimate on $\lpsl$ can be finally written as:
\[
4 c_\epsilon \alpha k (\log m \log \log m + \log k \log m) \text{.}
\]

The probability bound with $d$ substituted into it.
\[
\begin{split}
d ^ {\log \log m - \log \alpha - \log d -\log \log d} & \leq (1 - \epsilon)p \\
\left(k \log m\right)^{-\log \alpha -\log k - \log \log (k \log m)} & \leq (1 - \epsilon)p \\
\end{split}
\]

This inequality has various interpretations.
\begin{itemize}
\item For fixed $0 < p, \epsilon < 1$ and positive constant $k$ we get a lower bound on $m$ when our estimate is valid. Note that we can obtain arbitrary low multiplicative constant since we are allowed to choose values for $k$. Such estimates are valid for large quantities of stored elements only.
\item We can search for parameters of $\epsilon$ and $k$ so that multiplicative constant is the smallest possible for given hash table size $m$ and probability $p$. This approach is used to find the following estimate for $m \geq 4096$.
\end{itemize}

% TODO:
For small hash tables the best bound is achieved when the value of $n$ when our estimates are valid is the same as the value obtained by the above estimate. This value is for $k = 0.2766$, $n \geq 572$ and the multiplicative constant is $19.152$. This value has also been computed by a simple computer program.

\section{Hashing using the family of linear transformations}

The model we propose is a slight modification of the simplest universal hashing scheme with separated chains.
\begin{itemize}
\item We use the universal family of linear transformations from the $U = \vecspace{w}$ to the space $\vecspace{t}$. We may imagine this as hashing $w$-bit binary numbers to the space of $t$-bit binary numbers. We will refer to the size of the hashing table as $m = 2 ^ t$.
\item \textbf{Chain limit rule:} when there is a chain longer than a limiting value $l(m)$, specified later, then the whole table is rehashed.
\item \textbf{Load factor rule:} if the only operations performed are insert and find the load factor of the table is maintained in the interval $\left[0.5, 1\right]$. We need the interval $\left[0.25, 1\right]$ if delete is allowed, too. If the load factor rule is violated whole table is rehashed into a greater or smaller table respectively without violating the second property.
\end{itemize}

The limiting value, $l(m)$, for the table of size $m$ is chosen so that probability of rehashing is bounded by the prescribed value $p$, $0 < p < 1$. We would like to guarantee the worst case limit even when the table is fully loaded. The results of the section \ref{section-linear-systems-linear-amount-constant-estimate} are applied for the load factor $\alpha = 1$ when the delete operation is forbidden and for $\alpha = 1.5$ when it is allowed.

% TODO
The value $l(m)$ is chosen so that the probability of failing to find a suitable function is lower than a prescribed value $p$. This probability bound certainly works for the lower values of $\alpha$. In the mentioned section we computed the limit for $p = 0.5$ as $19.152 \log m \log \log m$ when $\alpha = 1$ and $28.728 \log m \log \log m$ for $\alpha = 1.5$.

Because the estimates used work only with the initial size of the table greater than 4 096 we have to options. Either we use tables with initial size of $m = \text{4 096}$ elements. But not every hash table grows to a size of 4 096 elements and this overhead becomes unacceptable. The other option lies in turning off the chain limit rule violation checks when the table has less than 4 096 buckets. Expected amortised time complexity of the introduced scheme is about to be analysed.

\begin{theorem}
Let $m$ be the start size of a hash table confirming to our model where the delete operation is not allowed. Assume that the number of insert operations performed is $2^k m$ for $k \in \mathbb{N}$. Then the expected amortised complexity of every performed find or insert operation is $O(1)$. The expectation is taken over the uniform choice of a hash function. 
\end{theorem}
\begin{proof}
For the amortised complexity analysis we use the potential method. The potential used here is the number $\alpha m - m$. This is the negative value of the number of remaining subsequent insert operations which would make the load factor reach one. Whenever an real insert is done we check if the prolonged chain is not longer than the limiting value or the load factor is not greater than $1$. So there are four cases to be analysed.
\begin{itemize}
\item Find operation is performed. From \ref{theorem-expected-chain-length-universal} and \ref{corollary-trimming-linear} we know it takes $O(1)$ time only.

\item No rehash is needed. Then from the previous theorem (\ref{theorem-expected-chain-length-universal}) the operation itself took only $O(1)$ time in the expected case. The potential is also increased by one. So the operation consumed $O(1)$ amortised time.

\item The rehash is performed because the load factor exceeds the value of $1$. The rehash itself takes the time $O(m)$ and the size of the table is doubled. Thus the resulting potential is $-m$ and the starting potential was $0$. So the operation took only a $O(1)$ time because of the first insertion.

\item Suppose the rehash operation is needed because of violation of chain limit rule. We seek for a new function satisfying the rule without resizing the table. A single rehash operation takes the time of $O(m)$ operations. In the following we refer to two immediate rehashes caused by the load factor rule as cycle. We need to find out how many rehash operations caused by the violation of the chain limit rule can occur in one cycle. 

Finding a suitable function for the hashed elements during the cycle is remarkably similar to finding it at the cycle's end. This is caused by setting the limit dependent on the size of the hash table only. We can see the process in the following manner. When we select functions from the family $H$ we can ask if the function is fine for a set $S_e$ that will be represented at the end of the cycle. The limits and probability bounds remain the same, $|S| < |S_e| = m$. Just remark that we do not know the hashed set $S_e$ in advance. Because universal hashing has no assumptions on a hashed set $S_e$\footnote{It only needs to be a subset of the universe $U$.} and it contains all currently hashed elements $S \subset S_e$ we can afford this simplification. After $\frac{1}{(1-p)^2}$ such choices we are expected to find a right function. The expected number of rehashes in a single cycle is then bounded by the same expression. 

There are exactly $0.5 m$ insert operations raising the load factor $\alpha = 0.5$ to $\alpha = 1$ for the table of size $m$. The expected time for the limit rule caused operations is $\frac{1}{(1 - p)^2}O(m)$ and if this amount of time is distributed uniformly along the cycle of $0.5 m$ inserts we are still at $O(1)$ amortised time per operation.
\end{itemize}

To finish the proof we have one issue to care about. In the last case we distributed the time evenly throughout the cycle. If the number of inserts is not a power of two we would distribute possibly long time over a non-complete cycle and would not obtain $O(1)$ time in it. In the previous cycles our complexity estimates hold. 

Amortised time of insert operation then converges to the $O(1)$ time with a growing number of operations. The expected time of find operation is constant by \ref{theorem-expected-chain-length-universal}. This operation obviously does not change the potential.
\end{proof}

One can find a potential to prove previous result more formally. We use this approach when the delete operation is also allowed.

\begin{theorem}
The expected amortised complexity of the operations find, insert and delete converges to $O(1)$. The expectation is taken over an uniform choice of a hash function.
\end{theorem}
\begin{proof}
In this proof we need to distinguish two types of the operation cycles. First we need to distribute the work of the load factor rule and that of the chain limit rule as well. In the previous theorem these cycles were the same. Also remember the difference that the limiting value is computed for the $\alpha = 1.5$.

In advance we must mention that the time of the find operation is proportional to the chain length. This value is expected to be constant and the operation does not change the potential. The following analysis is simplified by omitting the find operation.

Let the sequence $O = (o_i)_{i=1}^o$ denote the sequence of operations performed starting from the empty table, $o_i \in \lbrace Insert, Delete \rbrace$ and $o$ is the number of operations performed. Two later definitions will make the following analysis more clear.

\begin{definition}[$\alpha$-cycle]
The $\alpha$-cycle contains the operations between two immediate rehash operations caused by violations of the load factor rule independently on its cause - lower or upper bound. 
\end{definition}
Also note that the $\alpha$ is now in the interval $\left[0.25, 1\right]$. Whenever the rehash operation is executed the table size is chosen to fit the $\alpha$ as nearest $0.5$ as possible. The $\alpha$-cycle is defined to amortise the work caused by the load factor rule.

\begin{definition}[l-cycle]
The l-cycles are the partitioning of the sequence $O$ such that every l-cycle ends
\begin{itemize}
\item when the rehash operation caused by the load factor rule is needed or
\item we performed $0.5 m$ insert operations from the start of the l-cycle the load factor did not reach the value $1$.
\end{itemize}
\end{definition}
The l-cycle allows us to analyse the work caused by the chain limit rule.

The potential consists of two parts. Above all we want a simple insertion and deletion to take $O(1)$ time only. So the potential consists of the part $p_1 = 2i_{\alpha} + 4d_{\alpha}$ where $i_{\alpha}$ is the number inserts and $d_{\alpha}$ is the number of delete operations in the current $\alpha$-cycle. 

Next we need to distribute the work caused by the chain limit rule. Let $e$ be the expected number of rehash operations, equally fails, when finding a good function. The second part of the potential is $p_2 = 2ei_{l} + (ce - r) m$ where $i_l$ is the number of insertions in the current l-cycle, $r$ is the number of rehash operations caused by the chain rule violation and $c$ denotes the number of cycles occurred so far. The overall potential is combined as $p = 2p_1 + p_2$.

The analysis of the delete operation is simpler. When a deletion is performed we have just two cases:
\begin{itemize}
\item Simple deletion takes expected $O(1)$ time. We search in a chain of expected length $O(1)$ and the potential is increased by $8$.
\item The $\alpha$ violates the lower value. Now remark the number of deletions performed in the current cycle. The table has the size of $m$ slots in the beginning with load factor $0.5$. In the end the load factor is lowered to $0.25$. This work can be done by at least $0.25 m$ delete operations. The rehashing takes $O(m)$ time and the potential is also lowered by $m$ at least. Let us discuss the potential change. At first the difference of the $2p_1$ part of the potential is $2m$ at least. The second part gets increased by at most $m$ since a new l-cycle is started and the $i_l$ is zeroed.
\end{itemize}

The analysis of the first two cases of the insert operation remains similar as in the previous proof:
\begin{itemize}
\item Suppose no rehash operation after an insert was performed. The searched chain had the constant expected length and the potential was increased by $4 + 2e$. So the amortised time of the operation is constant.
\item If the load factor exceeds the upper bound there were at least $0.5 m$ insertions in the $\alpha$-cycle. The potential is certainly greater or equal $2m$. The rehash operation is expected to take $O(m)$ time. The resulting potential is lowered by at least $m$ since and a new l-cycle started.
\item Insert operation is the last in the l-cycle. Then there are at least $0.5 m$ insertions and the accumulated potential denoted by variable $p_2$ is equal to $em + (ce - r)m$. Since the new l-cycle begins the $i_l$ term is set to zero and the potential is moved to the term $((c + 1)e -r)m$.
\item Only the insert operations violating the chain limit rule remain. The time to rehash the table is $O(m)$ times the number of fails. The part $p_2$ was decreased by the same value since the variable $r$ increased by the number of fails. The expected number of fails in one l-cycle is equal to $e$ because the load factor is maintained lower than 1. The $0.5 m$ insert operations of the l-cycle can cause the increase at most to $1.5$ and the limiting value is chosen for the $\alpha = 1.5$.
\end{itemize}

The fraction $\frac{(ce - r)m}{o}$ converges to $1$ with the growing number of operations, $o$. This can be seen from the weak law of large numbers.
\end{proof}
