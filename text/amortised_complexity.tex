\chapter{Amortised complexity of the hashing scheme}

% TODO: ktore schemy nevedia delete, alebo nie je znamene, zmienit dynamizaciu
The hashing scheme we propose later in this chapter is a solution to the set representation problem. Every such solution provides us basic with some operations. These allow obtaining an information if an element is stored within the set, accessing the stored elements and inserting the element. Some schemes do not implement the element removal or the efficient implementation is not known.

% TODO: definicia vahovo vyvazeneho stromu.
The must crucial are the operations' running times. Various solutions to different problems prefer different operations. For instance some applications modify stored data rarely but the other storages are used for data that is almost never queried. So the running times of some operations are considered more important. In the case of a simple array we have $O(1)$ time for the find procedure and $O(n)$ insertion or deletion time in the canonical case. Better bounds can be obtained using the amortised complexity. Routines of balanced trees' are typically bounded by the logarithmic time. As already mentioned the right answer what is a good choice for data structure that should be used lies in the estimated structure of operations. This choice can be an asymptotic improvement, too. Anyway this does not change the fact that the short running times are welcomed.

We analyse the running times of the universal hashing first by mentioning the known facts. Then we extend this analysis to the universal hashing using the system of linear transformations over vector spaces. We will guarantee the worst case complexity of the find operation.

\section{Expected time of the find operation in the classic universal hashing}
In this section we assume that the used system of hashing functions is at least $c$-universal. The running time of the find operation is proportional to the number of collisions in the bucket where the searched element should be placed according to its hash value. The obvious worst case is $O(n)$ where $n$ is the number of elements hashed. The universal hashing gives a far better expected estimate, $O(1)$.

\begin{theorem}
\label{theorem-expected-chain-length-universal}
Expected time of the find operation in the universal hashing using a $c$-universal system is $O(1)$ if the load factors are upper bounded.
\end{theorem}
\begin{proof}
The time of the find operation is given by the length of the chain into which the searched element falls. We must also add the time for hashing the element but this can be typically done in one unit of time. The expected length of a chain containing the element $x \in U$ must be computed. The expectation is taken over the family of hash functions and the chain's length is the sum of indicator variables:
\begin{displaymath}
\frac{\sum\displaylimits_{h \in H} \sum\displaylimits_{y \in S - \lbrace x \rbrace} P(h(x) = h(y))}{|H|} = \frac{\sum\displaylimits_{h \in H} \sum\displaylimits_{y \in S - \lbrace x \rbrace} \frac{c}{m}}{|H|} = \frac{cn}{m}
\end{displaymath}

Since the element $x$ may be present in the set $S$ the time is bounded by the expression $1 + c\alpha$ where $\alpha$ is the table's load factor. The typical load factor is lower than 1, we only assumed that it is bounded. It is still enough to see that the expected time of every operation is still in $O(1)$.
\end{proof}

\section{Effects of trimming long chains on a universal system}
The model of hashing that we present guarantees the worst case bound on the length of the longest chain or equivalently running times of all operations. Knowledge of $\Expect{\lpsl}$ provides a limit when to stop inserting into a long chain and whole table needs to be rehashed. The algorithm finds a more suitable function which does create such long chains. This limit is set according to the probability of existence of a long chain.

Some formal computations follow to support these ideas. Fact that \[ \Prob{\lpsl > k \Expect{\lpsl}} \leq \frac{1}{k} \] may be obtained by direct use of Markov inequality. This means that less than half of all functions create longest chains longer than $2 \Expect{\lpsl}$. In case of system of linear transformations it means that the limit is $2\Expect{\lpsl} \leq \text{1 076} \log m \log \log m + 88$ for $\epsilon = 0.8$. The choice of $\epsilon$ corresponds to selection of minimal multiplicative constant.

Expected length of the longest chain gives us the generic hint when we should rehash the table to guarantee the worst case time for the member operation. The lower the value $\Expect{\lpsl}$, or tighter its estimate, is the better worst case limit on a operation's running time is achieved.

Mentioned probability bound computed from the expected value can be improved. Probability density function of $\lpsl$ can be used directly if it is known. For linear systems we have already found it. In addition we have consequently improved it in a few previous theorems. To sum up, the approach with the expected length is more general but achieves worse performance. It is more general because whenever we know the probability density function we are able to compute the expected value.

Probability bound $p$, $0 < p < 1$, on existence of a long chain has important consequences for the supposed model.
\begin{itemize}
\item At most $p|H|$ of all the functions cause long chains; longer than the prescribed limit $l(m, \alpha)$. 
\item The probability of rehashing, selecting inconvenient function, for its uniform choice is lower than $p$.
\item During rehashing caused by appearance of a long chain probability of finding a suitable function is at least $1 - p$. This statement holds under the assumption of the uniform choice of an universal function.
\end{itemize}

\begin{definition}
Let $H$ be the universal system mapping an universe $U$ to a hash table $B$. Let $m$ be size of the hash table and $\alpha$ its load factor. Let $l(m, \alpha)$ be a chain length limiting function and $S \subset U$ be the stored set. 

We say that function $h \in H$ creates a long chain if there is a chain of length longer than limiting value $l(m, \alpha)$.

Moreover let $0 < p < 1$ be probability such that and $\Prob{\lpsl \geq l(m, \alpha)} \leq p$ then system $H_p = \{ h \in H \setdelim h \text{ does not create a long chain} \}$ is called $p$-trimmed system.
\end{definition}

\begin{lemma}
\label{lemma-size-of-trimmed-system}
If $H_p$ is a $p$-trimmed system then the number of functions is $(1 - p)|H|$ at least.
\end{lemma}
\begin{proof}
\[
\begin{split}
|H_p|
	& = |\lbrace h \in H \setdelim \text{ h does not create a long chain} \rbrace| \\
	& =\Prob{\lpsl < l(m, \alpha)} |H| \\
	& = \left(1 - \Prob{\lpsl \geq l(m, \alpha)}\right) |H| \\
	& \geq (1 - p)|H| \text{.}
\end{split}
\]
\end{proof}

Regarding that every function is chosen uniformly and the unsuitable ones are discarded we still perform uniform selection. Now the choice is restricted to functions that do not create long chains; to set $H_p$ only. Note that the restriction on the functions of the original universal system $H$ uses some information about the stored set.

\begin{theorem}
Let $H$ be a $c$-universal system of hash functions mapping an universe $U$ to a hash table $B$. Let $0 < p < 1$ be the trimming rate and set $m = |B|$. Then system of functions $H_p$ is still universal with the constant of universality multiplied by factor $\frac{1}{1 - p}$. Equivalently:
\[
	\Prob{h(x) = h(y) \text{ for } h \in H_S} \leq k \Prob{h(x) = h(y) \text{ for } h \in H} \text{.}
\]
\end{theorem}
In the probability statement we are force to choose function $h \in H_p$.
\begin{proof}
Lemma \ref{lemma-size-of-trimmed-system} combined with the assumption of $c$-universality of the original system gives $\frac{c}{1 - p}$-universality of the resulting $p$-trimmed system. 

\[
\begin{split}
& \Prob{h(x) = h(y) \text{ for } h \in H_p}  \\
	& \qquad =  \frac{|\{ h \in H \mid h(x) = h(y) \wedge \textit{ h does not create long chains} \}|}{|\lbrace h \in H \mid \textit{ h does not create long chains} \rbrace|} \\
	& \qquad \leq \frac{|\{ h \in H \mid h(x) = h(y) \wedge \textit{ h does not create long chains} \}|}{(1 - p)|H|} \\ 
	& \qquad \leq \frac{|\{ h \in H \mid h(x) = h(y) \}|}{(1 - p)|H|} \\
	& \qquad = \frac{1}{1 - p} \Prob{h(x) = h(y) \text{ for } h \in H} \\
	& \qquad \leq \frac{c}{(1 - p) m} \\
\end{split}
\]

The universality constant is hence $\frac{c}{1 - p}$.
\end{proof}

Similar restrictions for the other universal systems may be found. For such systems probability of collision of two (or more) elements is estimated by their strong $k$-universality. Resulting probability is always $\frac{1}{1 - p}$ times higher than the original one.

Next statement summarises results for the systems of linear transformations that we examine.

\begin{corollary}
\label{corollary-trimming-linear}
For every trimming rate $0 < p < 1$ the $p$-trimmed system of linear transformations, $LT(U, B)_p$, is $\frac{1}{1 - p}$-universal.
\end{corollary}
\begin{proof}
System of linear transformations is $1$-universal as seen in Remark \ref{remark-system-of-linear-transformations}. By using the previous theorem it is clear that system $LT(U, B)_p$ is $\frac{1}{1 - p}$-universal.
\end{proof}

Computations proving Theorem \ref{theorem-expected-chain-length-universal} show that the expected chain length is still constant. It naturally gets larger and certainly depends on our choice of $p$. The looser the value of trimming rate is the better expected results are obtained. For benevolent choices of $p$ the worst case warranty is adequately worsened.

Every limit $l(m, \alpha)$ has an associated probability of event that $\lpsl > l(m, \alpha)$. This probability determines how many functions have to be tried in the expected case to find a suitable one. Suitable function is one that creates only short chains.

\begin{lemma}
\label{lemma-linear-transformations-tries}
If the hash function for a table with the size of $m$ elements and with the chain limit setup $\Prob{\lpsl \geq l(m, \alpha)} \leq p$ is sought the expected number of tries is $\frac{1}{(1 - p)^2}$.
\end{lemma}
\begin{proof}
The estimate of failing $k$ times in a row is clearly bounded by the geometric distribution with parameter $p$. The probability of failure is hence less or equals $p^k$. The expected time of success is then given by:
\[
\sum_{i = 0}^{\infty} (i + 1)p^i = \sum_{i = 0}^{\infty}p^i + \sum_{i = 0}^{\infty}ip^i = \frac{1}{1 - p} + \frac{p}{(1- p)^2} = \frac{1}{(1 - p)^2}
\]
\end{proof}

The most interesting and the most important idea of trimming is that system $H_p$ is an adaptation of the original class to the stored sets. We showed that finding a suitable function needs constant number of tries in the expected case. Moreover restricted systems are still $c$-universal for some constant $c \geq 1$. Hence all operations using system $H_p$ take constant expected time, too.

\section{Bound estimate based on probability density function}
\label{section-linear-systems-linear-amount-constant-estimate}
From now on we concentrate on obtaining a chain length limit such that probability of rehashing caused by the limit must be lower than prescribed parameter $p$, $0 < p < 1$. The corresponding bound is computed from the density function shown in the proof of Theorem \ref{theorem-hashing-linear-amount}. 

\[
\begin{split}
\Prob{\lpsl \geq 2\alpha c_\epsilon r} & \leq p \\ 
\frac{1}{1 - \epsilon}\left(\frac{r}{\log r}\right)^{\log \log m - \log \alpha - \log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} & \leq p \\
\end{split}
\]
Notice that the following probability estimate works only for $r \geq 4$. 

For simplicity define a function $f(d) = d ^ {\log \log m - \log \alpha - \log d - \log \log d}$. At first we show that function $f(d)$ is decreasing in the interval $\left[ d', \infty \right]$. Point $d'$ is chosen so that it is minimal among $d' \geq 2$ and $\log \log m - \log \alpha - \log d' - \log \log d' < 0$. 

To show monotonicity of $f$ let $d' \leq d_1 < d_2$.
\[
\begin{split}
f(d_2) 
	& = {d_2} ^ {\log \log m - \log \alpha - \log d_2 - \log \log d_2} \\
	& < {d_1} ^ {\log \log m - \log \alpha - \log d_2 - \log \log d_2} \\
	& < {d_1} ^ {\log \log m - \log \alpha - \log d_1 - \log \log d_1} \\
	& = f(d_1) \\
\end{split}
\]

The sought bound on $\lpsl$ for rehashing the table with probability $p$ equals $2 c_\epsilon \alpha r$ where parameter $r$ is not known. First step is to define a tight lower bound $d$ on expression $\frac{r}{\log r}$ and substitute it into function $f$ then \[ f(d) \geq f\left(\frac{r}{\log r}\right) \text{.} \] Whenever minimal $d$ such that $f(d) \leq (1 - \epsilon) p$ is found then wanted probability bound is satisfied, too.
\[ 
\begin{split}
\Prob{\lpsl \geq 2 \alpha c_\epsilon r} 
	& = \frac{1}{1 - \epsilon} f\left(\frac{r}{\log r}\right) \\
	& \leq \frac{1}{1-\epsilon}f(d) \leq p \\
\end{split}
\]
Variable $r$ has to be expressed from our lower bound $d$ on the expression $\frac{r}{\log r}$. The estimate on the length of the longest chain such that probability of rehashing is lower than $p$ is then finished.

Let us define the lower bound $d$ as $d = j \log m$ and $d \leq \frac{r}{\log r}$ for a positive constant $j$. Following inequality allows us to find minimal suitable $d$.
\[
d ^ {\log \log m - \log \alpha - \log d - \log \log d} \leq (1 - \epsilon) p \text{.}
\]

Since we selected $d$ as a lower bound for $\frac{r}{\log r}$ we have to express $r$ from $\frac{r}{\log r} \geq d$. Putting $r = 2 d \log d$ satisfies the inequality for every $d \geq 2$.
\begin{displaymath}
\frac{r}{\log r} = \frac{2 d \log d}{1 + \log d + \log \log d} = \frac{2 d}{1 + \frac{1}{\log d} + \frac{\log \log d}{\log d}} \geq d
\end{displaymath}
One of the assumptions for using probability estimate of event $\lpsl > 2 c_\epsilon \alpha r$ is $r \geq 4$. It is certainly satisfied when choosing $d \geq 2$ since $r = 2 d \log d \geq 4$.

We substitute into $d$ and find the explicit dependence of $r$ on the table's size $m$:
\begin{displaymath}
r = 2 d \log d = 2 j \log m (\log j + \log \log m) = 2 j (\log m \log \log m + \log j \log m) \text{.}
\end{displaymath}

Our estimate on $\lpsl$ can be finally written as:
\[
4 c_\epsilon \alpha j (\log m \log \log m + \log j \log m) \text{.}
\]

The probability bound with $d$ substituted into it.
\[
\begin{split}
d ^ {\log \log m - \log \alpha - \log d -\log \log d} & \leq (1 - \epsilon)p \\
\left(j \log m\right)^{-\log \alpha -\log j - \log \log (j \log m)} & \leq (1 - \epsilon)p \\
\end{split}
\]

This inequality has various interpretations.
\begin{itemize}
\item For fixed $0 < p, \epsilon < 1$ and positive constant $j$ we get a lower bound on $m$ when our estimate is valid. Note that we can obtain arbitrary low multiplicative constant since we are allowed to choose values for $j$. Such estimates are valid for large quantities of stored elements only.
\item We can search for parameters of $\epsilon$ and $j$ so that multiplicative constant is the smallest possible for given hash table size $m$ and probability $p$. This approach is used to find the following estimate for $m \geq \text{4 096}$.
\end{itemize}

\section{Bound for 4 096 elements and probability 0.5}
As mentioned before we used the formula obtain in the previous section. The limit is computed for tables consisting of 4 096 buckets at least and probability bound set to 0.5. These choices were not random. Our first tries with the formula shown that the multiplicative constants gained by its use are in order of tens. This order of multiplicative constant corresponds to thousands of stored elements when using the most basic linear estimate.

Program we used to compute the multiplicative constant used parameters $j$ and $\epsilon$ to minimise the expression $4 c_\epsilon \alpha j$. Since to compute the best multiplicative according to Remark \ref{remark-better-c-e} additional parameters $k$ and $l$ are present we needed those, too. We used Algorithm \ref{algorithm-scheme-3} we to solve this problem. It can be further improved but provided us valuable results.

The best result achieved for $\alpha = 1.5$, $m \geq \text{4 096}$ is for $\epsilon = 0.96$, $j = 0.74$ and equals $4 c_\epsilon \alpha j = 57.29$. This result is used later in the following section.

So compare this value with the linear estimate for size of the hash table $m \geq \text{4 096}$ and number of stored elements $n \geq \alpha m = 0.5 \cdot \text{4 096} = \text{2 0248}$. Linear estimate on the length of the longest chain equals $n = \text{2 0248}$. Our estimate is less than $57.29 \log m \log \log m \leq 57.29 \cdot 12 \cdot 2.27 \leq \text{1 555}$ which is already far better.  The same approach gives multiplicative constant $47.63$ for $\alpha = 1$.

\begin{algorithm}
\caption{Calculate the multiplicative constant for parameters $p, m, \alpha, \epsilon, k, l$.}
\label{procedure-scheme-3}
\begin{algorithmic}
\STATE $c_\epsilon$ $\leftarrow$ constant $c_\epsilon$ computed with parameters $k$, $\epsilon$ and $l$
\STATE $r \leftarrow (1 - \epsilon)p$ \COMMENT{Right side, inevitably achieved bound.}
\STATE $j \leftarrow 1$
\STATE 
\COMMENT{Lower the value of $j$ so that the right side is skipped.}
\STATE $l \leftarrow (j \log m) ^ {-\log(\alpha) - \log(j) - \log(\log(j \log m))}$
\WHILE {$l < r$ \textbf{and} $j > 0$}
	\STATE $j \leftarrow j - STEP$;
	\STATE $l \leftarrow (j \log m) ^ {-\log(\alpha) - \log(j) - \log(\log(j \log m))}$
\ENDWHILE
\STATE
\STATE $j \leftarrow j + STEP$
\RETURN $4 c_\epsilon \alpha j$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Calculate the smallest limit for $p=0.5$, $m \geq \text{4 096}$ and prescribed $\alpha$.}
\label{algorithm-scheme-3}
\floatname{algorithm}{Procedure}
\begin{algorithmic}
\STATE $c \leftarrow \infty$
\FOR {$k \in \left[2, 4\right]\text{ with }STEP$}
	\FOR {$l \in \left[1.3, 3\right]\text{ with }STEP$}
		\FOR {$\epsilon \in \left[0.85, 0.9\right]\text{ with }STEP$}
			\IF {$c > \text{multiplicative constant for }p = 0.5, m = \text{4 096}, \alpha, \epsilon, k, l$}
				\STATE $c \leftarrow \text{computed multiplicative constant}$
			\ENDIF
		\ENDFOR
	\ENDFOR
\ENDFOR
\STATE
\RETURN $c$
\end{algorithmic}
\end{algorithm}

\section{Hashing using the family of linear transformations}
\label{section-hashing-linear-transformations}
The model we propose is a slight modification of the simplest universal hashing scheme with separated chains. It can be described by the following statements. We must distinguish two cases; when delete operation is not allowed or the case when elements can be deleted as well.
\begin{itemize}
\item We use the universal family of linear transformations from the $U = \vecspace{w}$ to the space $\vecspace{t}$. We may imagine this as hashing $w$-bit binary numbers to the space of $t$-bit binary numbers. We will refer to the size of the hashing table as $m = 2 ^ t$.
\item \textbf{Chain limit rule:} when there is a chain longer than limiting value $l(m)$ then the whole table is rehashed. The value $l(m)$ is chosen according to Section \ref{section-hashing-linear-transformations} so that the probability of failing to find a suitable function is lower than a prescribed value $p$, $0 < p < 1$. In the following we put $p = 0.5$. 

If the delete operation is allowed then the limit value $l(m)$ is computed for $\alpha = 1.5$. When delete is forbidden values for $\alpha = 1$ are already valid. In Section \ref{section-hashing-linear-transformations} we computed the limit for $p = 0.5$ as $47.63 \log m \log \log m$ when $\alpha = 1$ and $57.29 \log m \log \log m$ for $\alpha = 1.5$.
\item \textbf{Load factor rule:} table's load factor is kept in the predefined interval. If the load factor is outside the interval whole table is rehashed into a greater or smaller table respectively. The table's size is chosen so that it does not violate this rule. Load factor is maintained in the interval $\left[0.5, 1\right]$ for scheme without the delete operation. We need the interval $\left[0.25, 1\right]$ if the delete operation is allowed as well.
\end{itemize}

Both estimates for chain limit rule we obtained are valid for tables of size greater than 4~096 buckets. There are two solutions how to address this problem. Either we use tables with initial size of $m = \text{4~096}$ elements. Since not every hash table grows to a size of 4~096 elements this overhead becomes unacceptable. The other option lies in turning off the chain limit rule checks when the table has less than 4~096 buckets. 

Expected amortised time complexity of the introduced scheme is analysed in the next two theorems for both cases.

\begin{theorem}
Let $m$ be the start size of a hash table confirming to our model where the delete operation is not allowed. Assume that the number of insert operations performed is $2^k m$ for $k \in \mathbb{N}$. Then the expected amortised complexity of every performed operation is $O(1)$.
\end{theorem}
\begin{proof}
Note that the expectation is taken over the uniform choice of a hash function. For the amortised complexity analysis we use the potential method. The potential used is defined as $\alpha m - m$. This is the negative value of the number of remaining insert operations that would make the table's load factor reach one. Whenever an insertion is done successfully we check if the prolonged chain is not longer than the limiting value or the load factor is not greater than $1$. If either of the conditions is violated the whole table is rehashed. So there are four cases to be analysed.
\begin{itemize}
\item \emph{Find operation} is performed. From Theorem \ref{theorem-expected-chain-length-universal} and Corollary \ref{corollary-trimming-linear} we know it takes $O(1)$ time only. It can not changed the table's potential.

\item \emph{Insert operation} performed with the result that \emph{no rehash} is needed. Then from the previous Theorem (\ref{theorem-expected-chain-length-universal}) the operation itself took only $O(1)$ time in the expected case. The potential is also increased by one. So the operation consumed $O(1)$ amortised time.

\item The rehash after an \emph{insertion} is performed because the \emph{load factor} exceeds the value of $1$. The rehash itself takes the time $O(m)$ and the size of the table is doubled. Thus the resulting potential is $-m$ and the starting potential was $0$. So the operation took only a $O(1)$ time because of the first insertion.

\item Suppose a rehash operation after an \emph{insertion} is needed because of \emph{violation of the chain limit rule}. We seek for a new function satisfying the rule without resizing the table. A single rehash operation takes the time of $O(m)$ operations. In the following we refer to two immediate rehashes caused by the load factor rule as cycle. We need to find out how many rehash operations caused by the violation of the chain limit rule can occur in one cycle. 

Finding a suitable function for the stored set $S$ during the cycle is remarkably similar to finding it at the cycle's end. This fact has more causes. At first we have the limit dependent on the size of the hash table only and computed it for $\alpha = 1$. Moreover we can not delete elements, they can only be added. We can see the process of finding a suitable function in the middle of a cycle in the following manner. When we select a function from the family $H$ we ask if the function is fine for the stored set $S$. We can ask if it is suitable for set $S_e$ that will be represented at the end of the cycle instead. The limits and probability bounds may remain the same. At the end of the cycle we have $S \subset S_e$, $m = |S_e|$ and $\alpha = 1$. Although we do not know the stored set $S_e$ in advance it is not a problem. Universal hashing places no assumptions on the represented set $S_e$\footnote{It only needs to be a subset of the universe $U$.} and since $S_e$ contains all the currently stored elements, $S \subset S_e$, we can afford this simplification. Hence after $\frac{1}{(1-p)^2}$ such choices we are expected to find a right function. The expected number of rehashes in a single cycle caused by the violation of the chain limit rule is then bounded by the same expression. 

Now we compute the amortised complexity of insert operations causing violation of the chain limit rule in one cycle. There are exactly $0.5 m$ insert operations raising the load factor from $\alpha = 0.5$ to $\alpha = 1$ for the table of size $m$. The expected time spent on fixing the chain limit rule in a cycle is then $\frac{1}{(1 - p)^2}O(m)$. We can distribute this amount of time along the cycle of $0.5 m$ inserts. This still means $O(1)$ expected amortised time for every insert operation.
\end{itemize}

To finish the proof we have one issue to care about. In the last case we distributed the time evenly throughout the cycle. If the number of inserts is not a power of two we would distribute possibly long time over a non-complete cycle and would not obtain $O(1)$ time in it. For complete cycles our complexity estimates hold. Since we assumed such number of insert operations that complete the cycle this problem disappears.

We could distribute time spent on fixing the chain limit rule from an incomplete cycle along all the insert operations performed. Amortised time of insert operation would then converge to $O(1)$ with a growing number of insert operations.
\end{proof}

One can find a potential function to prove previous the result more formally. We use an explicitly expressed potential function when the delete operation is also allowed.

\begin{theorem}
When starting from the empty table the expected amortised time complexity converges to $O(1)$ for every of the find, insert and delete operations.
\end{theorem}
\begin{proof}
In this proof we need to distinguish two types of the operation cycles. First we need to distinguish between the work done to enforce the load factor rule and that needed for keeping the chain limit rule. In the previous theorem these cycles were the same. Also remember the difference that the limiting value is computed for $\alpha = 1.5$.

In advance we mention the time of the find operation. It is proportional to the chain length and this value is expected to be constant. The find operation does not change the potential\footnote{The potential is defined later in the proof.}. Our analysis can be thus simplified by omitting the find operations.

Let sequence $O = (o_i)_{i=1}^o$ denote the performed operations, $o_i \in \lbrace Insert, Delete \rbrace$ for the $i$\textsuperscript{th} operation. The two following definitions make the following analysis more exact and clear.

\begin{definition}[$\alpha$-cycle]
The $\alpha$-cycle consists of the operations between two immediate rehash operations causing the violation of the load factor rule. It also contains the last operation, the one causing the violation.
\end{definition}
At first for this definition it is not important if the upper or lower bound is the cause. Also note that the load factor $\alpha$ is now in the interval $\left[0.25, 1\right]$ since the delete operation is allowed, too. Whenever a rehash operation is executed the table size is chosen to put value $\alpha$ to $0.5$ as near as possible. The definition $\alpha$-cycle is intended for analysis of the work caused by the load factor rule.

\begin{definition}[l-cycle]
The l-cycles are the partitioning of the sequence $O$ such that every l-cycle ends
\begin{itemize}
\item when the rehash operation caused by the load factor rule is needed or
\item we performed $0.5 m$ insert operations from the start of the l-cycle the load factor did not reach the value $1$.
\end{itemize}
\end{definition}
The l-cycle allows us to analyse the work caused by the chain limit rule. Both l-cycles and $\alpha$-cycles divide the sequence $O$. Moreover if $\alpha$-cycle ends at the position $i$, corresponding l-cycle also ends at the same position. 

The analysis takes the $i$\textsuperscript{th} operation and shows that the expected amortised time complexity is $O(1)$ independently on the few operation's types. The potential consists of the two parts, $p_1$ and $p_2$. For both cases let $e$ denote the expected number of rehash operations, number of tries, when finding a suitable function. 

Above all we want every simple insertion and deletion to take $O(1)$ time only. Thus the potential consists of the part $p_1 = 2ei_{\alpha} + 4ed_{\alpha}$ where $i_{\alpha}$ is the number inserts and $d_{\alpha}$ is the number of delete operations in the current $\alpha$-cycle. 

Next we need to distribute the work caused by the chain limit rule. Suitable function means every function that does not violate the chain limit rule. The second part of the potential $p_2 = 2ei_{l} + (ce - r) m$ where $i_l$ is the number of insertions in the current l-cycle, $r$ is the number of rehash operations caused by the chain rule violation and $c$ denotes the number of l-cycles occurred so far. The overall potential is combined as $p = 2p_1 + p_2$.

The analysis of the delete operation is simpler and comes first. When a deletion is performed we have just two cases:
\begin{itemize}
\item Simple deletion that takes expected $O(1)$ time because we search in a chain of expected length $O(1)$. The potential is increased by $8e$ since the number of deletions in $p_1$ gets increased by one.
\item The $\alpha$ violates the lower value of the load factor rule. Now remark that the number of deletions performed in the current $\alpha$-cycle must be $0.25$ at least. The table has the size of $m$ slots with load factor of $0.5$ at its beginning and at the end the load factor is lowered to $0.25$. Such descent may be done by  $0.25 m$ delete operations at least. Let us discuss the potential change. At first the difference of the $2p_1$ part of the potential is $2em$ at least since $i_{\alpha}$ and $d_{\alpha}$ get zeroed. The second part gets increased by at most $em$ since a new l-cycle is started and the $i_l$ is zeroed. Rehashing of the table takes $O(em)$ expected time and the potential is also lowered by $m$ at least so the amortised time is $O(1)$. 
\end{itemize}

The analysis of the first two cases of the insert operation remains similar as in the previous proof:
\begin{itemize}
\item Suppose no rehash operation after an insert was performed. The searched chain had the constant expected length and the potential was increased by $4e + 2e$. So the amortised time of the operation is constant.
\item If the load factor exceeds the upper bound there are at least $\frac{m}{2}$ insertions in the current $\alpha$-cycle. It means potential $p_2$ is certainly greater or equals $2em$. After performing the analysed operation a new l-cycle gets started. The potential is raised by $em$ because of increase of variable $c$. It is also lowered by $2em$ because $i_l$ is put to zero. The rehash operation is expected to take $O(em)$ time. Regarding the fact that the resulting potential is lowered by value $em$ at least we get $O(1)$ amortised time.
\item Insert operation is the last one in the l-cycle and the chain limit rule is not violated. Then there are at least $0.5 m$ insertions and the accumulated potential denoted by variable $p_2$ is equal to $em + (ce - r)m$. Since a new l-cycle is started the $i_l$ term is set to zero and value of $c$ is increased by one. Therefore there is no potential change.
\item Only the insert operations violating the chain limit rule remain. Time consumed by rehashing the table is $O(m)$ times the number of fails. Part $p_2$ was decreased by the same value since the variable $r$ gets increased by the number of fails. 

We clarify why the expected number of fails in one l-cycle is equal to $e$. The reason is similar to the reason why needed $em$ time to ensure the chain limit rule in one cycle in the previous case. Because the load factor is maintained lower than 1 we would need at least $0.5 m$ insert operations to raise it above $1.5$. Therefore inserts in one l-cycle, when omitting deletes, can cause raise of the load factor to $1.5$ at most. Since the limiting value $l(m)$ is computed for the $\alpha = 1.5$ and $S \subset S_e$ we expect $e$ rehash operations in one l-cycle to enforce the chain limit rule.
\end{itemize}

The fraction $\frac{(ce - r)m}{o}$ converges to $1$ with the growing number of operations, $o$. This is true because of the weak law of large numbers.
\end{proof}
