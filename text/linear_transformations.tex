\chapter{Systems of linear maps between vector spaces}

Main result regarding the~systems of linear maps is the~upper bound on the~length of the~longest probe sequence. The~proof of this property does not come from the~ideas already mentioned in this text. The~approach used here does not try to compute the~probability of collision of $k$-elements directly. Thus we will not exploit the~attributes of a~$2$-strongly universal system. Instead we concentrate and work with special properties of this unique system.

In the~following section we consider the~vector spaces over the~$Z_2$ field only. Also the scheme supposes hashing $n \log n$ elements into a~hash table of size $n$ slots. Under these assumptions we can prove one very important property of this model; the~length of the~longest chain is bounded by function in $O(\log n \log \log n)$. If vector spaces are over other finite fields we can not expect such good properties \cite{linear-hash-functions}.

\begin{lemma}
\label{lemma-linear-transformation-domain-distribution}
Let $T: Z_2^u \rightarrow Z_2^t$, $u \leq t$ be an~onto linear map and vector $x \in Z_2^t$. Then $|T^{-1}(x)| = 2 ^ {u-t}$.
\end{lemma}
\begin{proof}
First we remark that $T^{-1}(0)$ is a~vector subspace of the~vector space $Z_2^u$:
\begin{align*}
&\textit{}& T(0) & = 0 \\
T(u) = 0, T(v) = 0: &\textit{}& T(u + v) & = T(u) + T(v) = 0 \\
\delta \in \lbrace 0, 1 \rbrace, T(u) = 0: &\textit{}& T(\delta u) & = \delta T(u) = \delta 0 = 0 \\
\end{align*}

We show that $T^{-1}(x)$ is an~affine subspace since $T^{-1}(x) = u + T^{-1}(0)$. The~consequence is that all sets $T^{-1}(x)$ are of the~same size, exactly $|T^{-1}(x)| = 2^{u-t}$. For every vector $u \in T^{-1}(x)$ the~set $T^{-1}(x)$ is the~same as $u + T^{-1}(0)$.
\begin{displaymath}
\begin{split}
v \in T^{-1}(0) 
	& \Rightarrow T(u+v) = T(u) + T(v) = x + 0 = x  \\
	& \Rightarrow u + T^{-1}(0) \subseteq T^{-1}(x)
\end{split}
\end{displaymath}
\begin{displaymath}
\begin{split}
v \in T^{-1}(x) 
	& \Rightarrow T(v-u) = T(v) - T(u) = x - x = 0 \\
	& \Rightarrow v - u \in T^{-1}(0) \\
	& \Rightarrow T^{-1}(x) \subseteq u + T^{-1}(0)
\end{split}
\end{displaymath}
\end{proof}

All of the~following claims are taken from \cite{linear-hash-functions}. It is convenient to show the~original proofs and then modify them according to our future needs.

\begin{lemma}
\label{lemma-choose-random-vector}
If $V$ is a~finite vector space, $A$ is a~subset of $V$, $\alpha = 1 - \frac{|A|}{|V|}$. Then for a~random uniformly chosen vector $v \in V$ holds:
\begin{displaymath}
E (1 - \frac{|A \cup (v + A)|}{|V|}) = \alpha^2 \textit{.}
\end{displaymath}
The~expectation is taken through all the~possible choices of $v$.
\end{lemma}
\begin{proof}
We choose two vectors $u$ and $v$ uniformly and independently from the~space $V$. They can also be equal. Probability of event $u \notin A$ and $u \notin v + A$ is equal to $\alpha$. These two events are independent and by stating the~following we will finish the~proof this lemma.
\begin{displaymath}
\begin{split}
|A \cup (v + A)| 
	& = \sum_{u \in V} 1 . P(u \in a~\vee u \in v + A) \\ 
	& = \sum_{u \in V} 1 - P(u \notin a~\wedge u \notin v + A) \\ 
	& = |V| (1 - \alpha ^ 2)
\end{split}
\end{displaymath}
\begin{displaymath}
\begin{split}
E (1 - \frac{|A \cup (v + A)|}{|V|}) 
	& = \frac{\sum\displaylimits_{v \in V}(1 - \frac{|A \cup (v + A)|}{|V|})}{|V|} \\
	& = \frac{\sum\displaylimits_{v \in V}\alpha ^ 2}{|V|}
\end{split}
\end{displaymath}
\end{proof}

The following lemma is a~very technical one ans is used to estimate the~probabilities of some events related to linear maps.
\begin{lemma}
\label{lemma-random-variable}
For $1 \leq i \leq k$ the~$\alpha_i$ are random variables and $0 < \alpha_0 < 1$ is a~constant. For the~random variables, $1 \leq i \leq k$, we assume the~following:
\begin{gather*}
0 \leq \alpha_i \leq \alpha_{i - 1} \\
E[ \alpha_i | \alpha_{i-1} \dots \alpha_1 ] = \alpha_{i-1}^{2} \\
\end{gather*}
Then for every constant $0 < t < 1$ we can estimate the~probability:
\begin{displaymath}
P(\alpha_k \geq t) \leq \alpha_0^{k - \log \log (\frac{1}{t}) + \log \log \left(\frac{1}{\alpha_0}\right)}
\end{displaymath}
\end{lemma}
\begin{proof}
For $k = 0$ the~claim is certainly true; we have just two cases to take into account. First if $t \leq \alpha_0$, the~exponent is negative:
\begin{displaymath}
-\log \log (1/t) + \log \log \left(\frac{1}{\alpha_0}\right) < 0
\end{displaymath}
The base, $\alpha_0$, is less then $1$ so our estimate of the~probability is above $1$. 

In the~remaining possibility the~real probability is $0$ and our estimate is certainly positive.

By using induction we suppose that the~lemma holds for $k$ and we will show its validity for $k + 1$. For convenience we set the~constant $c$ equal to $k - \log \log \left(\frac{1}{t}\right)$. The~expected result on the~right side is then:
\begin{displaymath}
\alpha_0^{c + 1 + \log \log \left(\frac{1}{\alpha_0}\right)}
\end{displaymath}

As in the~induction start we will rule out the~case if exponent is negative:
\begin{displaymath}
c + 1 + \log \log \left(\frac{1}{\alpha_0}\right) < 0\textit{.}
\end{displaymath}
Our estimate will be greater than $1$. 

Now we will assume the~opposite inequality. At~this place the~proof needs to be split into a~few cases. At first we need to point out some useful remarks. The~estimated probability can certainly be computed by taking the~expectation when we fix the~random variable $\alpha_1$ in an~interval $[0, \alpha_0]$.
\begin{displaymath}
P(\alpha_{k+1} \geq t) = E_{\alpha_{k+1}} P(\alpha_{k+1} \geq t | \alpha_1) \leq E_{\alpha_{k+1}} f(\alpha_1)
\end{displaymath}

Where $f$ is defined for $0 < x < 1$ as:
\begin{displaymath}
f_0(x) = x ^ {c + \log \log \left(\frac{1}{x}\right)}
\end{displaymath}
\begin{equation}\label{f-definition}
 f(x) = 
  \begin{cases} 
   \min(1, f_0(x)) & \text{if } 0 < x < 1 \\
   1 & \text{if } x = 0
  \end{cases}
\end{equation}
In the~function $f$ the~argument $x$ plays the~role of the~constant $\alpha_1$ since we fixed its value. There are only $k$ variables among variables $\alpha_{k+1}$ and $\alpha_1$. From the~induction hypothesis it follows that $f$ is the~upper bound for $P(\alpha_{k+1} \geq t | \alpha_1)$.

For all $0 \leq x \leq \alpha_0$ we would like to have the~value of $f(x)$ upper bounded by the~expression $\frac{f_0(\alpha_0)x}{\alpha_0}$. In order to prove this property we define two values $x'$ and $x''$ both less than $1$:
\begin{equation}
x' = 2 ^ {-2 ^ {-c - 1}}
\end{equation}
\begin{equation}
x'' = {x'}^2 = 2 ^ {-2 ^ {-c}}
\end{equation}

Now we should point out how the~mentioned bound is obtained.
\begin{equation}
f(x) = \frac{x f(x)}{x} \leq \frac{x f_0(\alpha_0)}{\alpha_0}
\end{equation}
The last inequality remains to be proved. We will bound the~function $\frac{f(x)}{x}$. 

From the~derivation of the~function $\frac{f(x)}{x}$ in the~interval $(0, 1)$ we know that it is initially in the~increasing phase and then decreases.
\begin{displaymath}
\frac{f(x)}{x}' = \left(c + \log e - 1 + \log \log \left(\frac{1}{x}\right)\right)\frac{f_0(x)}{x}
\end{displaymath}

The point $x''$ lies in the~increasing phase:
\begin{displaymath}
c + \log e - 1 + \log \log \left(\frac{1}{x''}\right) = c + \log e - 1 - c > 0
\end{displaymath}

For every $x$, $x'' \leq x$ the~value of $f(x)$ is 1:
\begin{equation}\label{f-x-double-prime}
f_0(x'') = {x''}^{c + \log \log \left(\frac{1}{2^{-2^{-c}}}\right)} = {x''}^{c - c} = 1
\end{equation}
\begin{equation}\label{f-x-after-double-prime}
f_0(x) = x^{c - \log \log \left(\frac{1}{x}\right)} \geq {x''}^{c - \log \log \left(\frac{1}{x}\right)} \geq {x''}^{c - \log \log \left(\frac{1}{x''}\right)} = 1
\end{equation}

Finally from the~above claims we can conclude.
\paragraph{The constant $\alpha_0$ is in the~increasing phase.}
Since $\alpha_1 \leq \alpha_0$ the~values $x$ are certainly less then $\alpha_0$ and also are in the~increasing phase. Under these assumptions we immediately have:
\begin{equation}
\frac{f(x)}{x} \leq \frac{f(\alpha_0)}{\alpha_0}
\end{equation}

\paragraph*{The constant $\alpha_0$ is in the~decreasing phase.}
\subparagraph*{Value $x$ is before $x''$.}
\begin{equation}
\frac{f(x)}{x} \leq \frac{f(x'')}{x''} \leq \frac{f_0(x'')}{x''} = \frac{1}{x''}
\end{equation}
The first inequality holds because $x$ is also in the~increasing phase. The~second comes from the~definition \eqref{f-definition} and the~equality is from the~\eqref{f-x-double-prime}.

\subparagraph*{Value $x$ is greater than $x''$.}
\begin{equation}
\frac{f(x)}{x} \leq \frac{1}{x} \leq \frac{1}{x''}
\end{equation}
The first inequality follows from \eqref{f-x-after-double-prime}. And the~second is clear because $x \geq x''$.

In this case we have bounded the~value $\frac{f(x)}{x}$ by $\frac{1}{x''}$. Next we will discuss the~upper bound on that value.
\begin{equation}
\frac{f_0(x')}{x'} = \frac{\left({2 ^ {-2 ^ {-c - 1}}}\right)^{-1}}{2 ^ {-2 ^ {-c - 1}}} = \frac{1}{\left({2 ^ {-2 ^ {-c - 1}}}\right)^2} = \frac{1}{x''}
\end{equation}

We have already ruled out the~case $x' < \alpha_0$ since the~exponent in the~proved claim would be negative:
\begin{displaymath}
c + 1 + \log \log \left(\frac{1}{\alpha_0}\right) < c + 1 + \log \log \left(\frac{1}{x'}\right) = c + 1 - c - 1 = 0
\end{displaymath}

So if the~$\alpha_0$ is in the~decreasing phase we must assume $x' \geq \alpha_0$. Because $\alpha_0$ is before $x'$ the~value $\frac{f(\alpha_0)}{\alpha_0}$ is still greater than $\frac{f(x')}{x'}$ and the~following holds:
\begin{displaymath}
\frac{1}{x''} = \frac{f_0(x')}{x'} \leq \frac{{f_0}(\alpha_0)}{\alpha_0}\textit{.}
\end{displaymath}

So in both cases, $\alpha_0$ is in increasing or decreasing phase, for every $0 \leq x \leq \alpha_0$ the~value $f(x) = \frac{f(x)x}{x} \leq \frac{f_0(\alpha_0)x}{\alpha_0}$. Finally we can estimate the~above mentioned expected value and probability.
\begin{displaymath}
P(\alpha_{k+1} \geq t) \leq E f(\alpha_1) \leq E \frac{f(\alpha_0)\alpha_1}{\alpha_0} = \frac{f_0(\alpha_0)}{\alpha_0}E \alpha_1 = \alpha_0 f(\alpha_0) = \alpha_0^{c + 1 + \log \log \left(\frac{1}{\alpha_0}\right)}
\end{displaymath}

\end{proof}

\begin{theorem}
\label{theorem-linear-function-set-onto}
Let $A$ be a~subset of the~vector space $Z_2^u$ and $T: Z_2^u \rightarrow Z_2^t$ is a random uniformly chosen surjective linear map, $0 \leq t < u$. Denote $\alpha = 1 - \frac{|A|}{2^u} < 1$. Then:
\begin{displaymath}
P(T(A) \neq Z_2^t) \geq \alpha^{u - t - \log t + \log \log \frac{1}{\alpha}} \textit{.}
\end{displaymath}
\end{theorem}
\begin{proof}
Consider uniformly and independently selected $s$ vectors, $v_1, \dots, v_s$ from the~vector space $Z_2^u$. If $T$ is a~linear map that maps every $v_i$ to the $0$ vector; selected vectors are members of $Ker(T)$. However, they do not necesarilly span the~whole kernel, they do not need to be linearly independent, even they may be equal. Because they are selected uniformly and independently we still can have $T$ selected uniformly. We can achieve this by extending its above definition. 

Random and independent selection of linear transormations may be done like this. One basis, canonical for example, of the~space $Z_2^t$ is fixed. The~number of all bases of the~space $Z_2^u$ is finite. One is uniformly chosen and is mapped by the~function $T$ on the~fixed basis of $Z_2^t$. This mapping has to be implemented carefully. First we find the~kernel, $u-t$ vector of the~chosen basis are sent to $0$. The~remaining $t$ vectors are permuted and then mapped.

The selection of $s$ vectors gives us the~part of the~kernel in the~following manner. The~$i$-th vector is added to the~kernel if and only if it does not belong to the~$span(v_1, \dots, v_{i-1})$. This is certainly a~linearly independent set of dimension lower or equal to $s$. From Steinitz's theorem we know that it can be extented to a~basis. By mapping an~uniformly selected extension basis to the~target space one can obtain the~wanted function $T$.

Define a~sequence $A_i = a~+ span(v_1, \dots, v_i)$ for $0 \leq i \leq s$, the~matching $\alpha_i = 1 - \frac{|A_i|}{2^u}$, $0 \leq \alpha_i \leq \alpha_{i-1}$. Since $A_{i} = a~+ span(v_1, \dots, v_i) = A_{i-1} \cup (A_{i-1} + v_i)$, by using lemma \ref{lemma-choose-random-vector} we already know that $E[\alpha_i | \alpha_{i-1}, \dots, \alpha_1] = \alpha_{i-1}^2$. And the lemma \ref{lemma-random-variable} implies:
\begin{displaymath}
\begin{split}
P(\alpha_s \geq 2^{-t}) 
	& \leq \alpha^{s - \log \log (\frac{1}{2^{-t}}) + \log \log (\frac{1}{\alpha})} \\
	& = \alpha^{s - \log t + \log \log (\frac{1}{\alpha})} \\
	& = \alpha^{u - t - \log t + \log \log (\frac{1}{\alpha})}
\end{split}
\end{displaymath}

The event $\alpha_s \geq 2^{-t}$ is more probable than $T(A) \neq Z_2^t$. Whenever $\alpha_s$ is lower than $2^{-t}$ $T(A)$ must be the whole space $Z_2^t$. If there is a $x \in Z_2^t - T(A)$ the sets $T^{-1}(x)$ and $A_s$ are of sizes $2^{u-t}$ (lemma \ref{lemma-linear-transformation-domain-distribution}) and $(1 - \alpha_s)2^u > 2^u - 2^{u-t}$ respectively. We also know that they must be disjoint since $A_s = a + span(v_1, \dots, v_s)$ and $T(v_i) = 0$ implies $T(A) = T(A_s)$. Considering the sizes of the sets $T^{-1}(x)$, $A_s$ and the fact that they are disjoint we immediately obtain a contradiction.
\end{proof}

\begin{theorem}
\label{theorem-set-onto-by-linear-transform}
If $T: V -> Z_2^t$ is a random uniformly chosen linear map where $V$ is a finite vector space over $Z_2$. Then for every $0 < \epsilon < 1$ there is a costant $c_\epsilon > 0$ such that for every subset $A$ of the domain $V$, $|A| \geq c_\epsilon t 2^t$, the probability of mapping $A$ onto the whole space is high enough.
\begin{displaymath}
P(T(A) = Z_2^t) \geq 1 - \epsilon \textit{.}
\end{displaymath}
\end{theorem}
\begin{proof}
First we will define a factor vector space $W = Z_2^u$ where $u = \left\lceil \log (\frac{2|A|}{\epsilon}) \right\rceil$. Any linear map $T$ may be constructed by putting $T = T_0 \circ T_1$ where $T_0: V \rightarrow W$ and $T_1: W \rightarrow Z_2^t$ last is also surjective. For the uniform selection of function $T$ the mapping $T_1$ may be fixed and it is sufficient to uniformly selected a map $T_0$. However we also need a random mapping $T_1$ so it will be selected uniformly, too.

Consider two vectors $v \neq w \in A$ which collide because of the function $T_0$, $T_0(v) = T_0(w)$. Because a system of linear function is $1$-universal the probability of the collision is $P(T_0(v) = T_0(w)) = \frac{1}{|W|}$. By using the law of total probability we get:
\begin{displaymath}
P(T(A) \neq A) = P(T(A) \neq A \wedge |T_0(A)| \leq \frac{|A|}{2}) + P(T(A) \neq A \wedge |T_0(A)| > \frac{|A|}{2})
\end{displaymath}

If the size $|T_0(A)| \leq \frac{|A|}{2}$ there must be at least $\frac{|A|}{2}$ collision. The expected number of collision caused by the transformation $T_0$ is:
\begin{displaymath}
\frac{\dbinom{|A|}{2}}{|W|}\textit{.}
\end{displaymath}

In the first case we will vanish the first part of conjunction. The probability estimate is obtained by applying the Markov inequality.
\begin{displaymath}
P(T(A) \neq Z_2^t \wedge |T_0(A)| \leq \frac{|A|}{2}) \leq P(|T_0(A)| \leq \frac{|A|}{2}) \leq \frac{2 \dbinom{|A|}{2}}{|A||W|} < \frac{|A|}{|W|} \leq \frac{\epsilon}{2}
\end{displaymath}

There is one case remaining - $|T(A)| > \frac{|A|}{2}$. By using the theorem \ref{theorem-linear-function-set-onto} for vector space $W$, set $|T_0(A)|$ and mapping $T_1$ we get:
\begin{displaymath}
\alpha = 1 - \frac{|T_0(A)|}{|W|} < 1 - \frac{|A|}{2|W|} = 1 - \frac{\epsilon |A|}{8|A|} \leq e^{-\frac{\epsilon}{8}}
\end{displaymath}
In the following the constant $c_\epsilon$ is chosen as $4\left(\frac{2}{\epsilon}\right)^{\frac{8}{\epsilon}}$:
\begin{align*}
P(T(A) & \neq Z_2^t \wedge |T(A)| > \frac{|A|}{2}) && \leq \alpha ^ {u - t - \log t + \log\log\left(\frac{1}{\alpha}\right)} \\
    & \leq e^{-\frac{\epsilon\left(u - t - \log t + \log\log\left(\frac{1}{\alpha}\right)\right)}{8}} && \leq e^{-\frac{\epsilon\left(t + \log t + 3 - \log \epsilon + \frac{8}{\epsilon}\log\left(\frac{2}{\epsilon}\right) - t - \log t + \log\log\left(\frac{1}{\alpha}\right)\right)}{8}} \\
    & \leq e^{-\frac{\epsilon \left(3 - \log \epsilon + \frac{8}{\epsilon}\log\left(\frac{2}{\epsilon}\right) + \log\log\left(\frac{1}{\alpha}\right) \right)}{8}} && \leq e^{-\frac{\epsilon \left(3 - \log \epsilon + \frac{8}{\epsilon}\log\left(\frac{2}{\epsilon}\right) + \log\left(\frac{\epsilon}{8} \log e \right) \right)}{8}} \\
    & = e^{-\frac{\epsilon \left(3 - \log \epsilon + \frac{8}{\epsilon}\log\left(\frac{2}{\epsilon}\right) + \log\log e - 3 + \log \epsilon \right)}{8}} && = e^{-\frac{\epsilon \left( \frac{8}{\epsilon}\log\left(\frac{2}{\epsilon}\right) + \log\log e \right)}{8}} \\
    & \leq e^{-\frac{\epsilon \left( \frac{8}{\epsilon}\log\left(\frac{2}{\epsilon}\right) \right)}{8}} = e^{{\log\left(\frac{\epsilon}{2}\right)}} \leq e^{\ln\left(\frac{\epsilon}{2}\right)} && = \frac{\epsilon}{2}
\end{align*}

In both alternatives the probability limit is $\frac{\epsilon}{2}$. It gives us that $P(T(A) = Z_2^t) \geq 1 - \epsilon$.

The biggest disadvantage of the that the current estimate of $c_\epsilon$ is very inaccurate. For the practical use of this scheme we need the smallest value possible. In the later section we will try to lower the value of $c_\epsilon$.
\end{proof}

The two previous theorems give us enough strength to achieve our goal - restriction of the worst case chain length. Once again we achieve this limit when hashing even super-linear amount of $n \log n$elements into the $n$ slots. By hashing a linear amounts the expected length can not grow, instead we can extend the hashed set to $n \log n$ values and the estimate remains valid. The most common application is to have load factors lower than 1. For this model we need will show an important relation of the chain length on the load factor. This is achieved by a slight proof modification and the multiplicative constant contains the table's load factor itself.

\begin{theorem}
\label{theorem-n-logn-to-n}
Let $H$ be the system of all linear mappings between two vector spaces over the field $Z_2$. The expected length of the longest chain when hashing $n \log n$ elements into the table of size $n$ is $O(\log n \log \log n)$.
\end{theorem}
\begin{proof}
As in the previous proof we will factor a linear transformation $h \in H$, $h: D \rightarrow B$. We use the vector space $A = Z_2^l$ pre $l \geq \log n$ and two functions $h_1: D \rightarrow A$ and $h_2: a~\rightarrow B$ which is surjective. Both linear functions $h_1$ and $h_2$ are selected uniformly. This implies the uniform choice of the transformation $h = h_1 \circ h_2$.

The idea presented here is to estimate the probability $P(lpsl > t)$ by using the other somehow unnatural event $E_2$. Its probability is then easy to find. The probability of the event $E_1 \equiv lpsl > t$ can be simply determined by discovering the relation between them. We will remark the following fact. When the $E_1$ is true with a high probability the event $E_2$ also appears.

\begin{itemize}
\item $E_1$ denotes the existence of a chain of length at least $t$ elements. Formally written $\exists \alpha \in B: | h^{-1}(\alpha) \cap S | > t$.
\item $E_2$ is equivalent to existence of a vector $\alpha \in B$ such that $h_2^{-1}(\alpha) \subseteq h_1(S)$.
\end{itemize}

\begin{remark}
\label{remark-e2-probability}
If $d = \frac{2^l}{n \log n} > 1$ then we have:
\begin{displaymath}
P(E_2) \leq d^{-\log d - \log \log d}\textit{.}
\end{displaymath}
\end{remark}
\begin{proof}
First we should point out one equivalent definition of $E_2$.
\begin{displaymath}
h_2^{-1}(\alpha) \subseteq h_1(S) \Leftrightarrow h_2(A - h_1(S)) \neq B \textit{.}
\end{displaymath}

Event $E_2$ holds if and only if there is an element $\alpha$ which inverse image $h_2^{-1}(\alpha)$ is a subset of image $S$ by $h_1$. Then equivalently the function $h_2$ can not display $A - h_1(S)$ onto the whole set $B$.

Now we use the theorem \ref{theorem-linear-function-set-onto} for the function $h_2$, set $h_1(A)$ and target space $B$. Because $\frac{2^l}{n \log n} > 1$ holds the assumptions of the \ref{theorem-linear-function-set-onto} are satisfied. Corresponding inverse density $\alpha = 1 - \frac{|A| - |h_1(S)|}{|A|} = \frac{|h_1(S)|}{|A|}$.
\begin{displaymath}
\alpha = \frac{|h_1(S)|}{|S|} \leq \frac{|S|}{|A|} = \frac{1}{d}
\end{displaymath}
Because $d = \frac{2^l}{n \log n} > 1$:
\begin{displaymath}
\log d = l - \log n - \log \log n
\end{displaymath}
\begin{displaymath}
P(E_2) \leq \alpha^{l - \log n - \log \log n + \log \log \left(\frac{1}{\alpha}\right)} = \alpha ^ {\log d + \log \log \left(\frac{1}{\alpha}\right)} \leq d^{-\log d - \log \log d}
\end{displaymath}
\end{proof}

Follows the similar lemma for the conditional probability.
\begin{remark}
\label{remark-prob-t-length-chain}
If $t > c_{\frac{1}{2}}{\frac{2^l}{n}}\log\left(\frac{2^l}{n}\right)$ then for the conditional probability of events holds:
\begin{displaymath}
P(E_2 | E_1) \geq \frac{1}{2} \textit{.}
\end{displaymath}
\end{remark}
\begin{proof}
Suppose that we have a given mapping $h$ and the event $E_1$ holds. There must be a subset $S' \subseteq S$ consisting of at least $t$ elements mapped by $h$ to a single element $\alpha \in Z_2^{\log n}$. We fix this element and define $D' = h^{-1}(\alpha)$ and $A' = h_2^{-1}(\alpha)$. 

Consider the distribution of all transformations $h_1$ which satisfy the equation $h = h_1 \circ h_2$. By restricting $h_1$ to the set $D'$ we obtain a affine linear transformation onto the set $A'$. The uniform selection of $h_1$ corresponds to the uniform selection of all transformations from $D'$ onto $A'$.

$E_2$ is certainly present whenever $A' \subseteq h_1(S)$ occurs. The size of the set $A'$ is exactly $\frac{2^l}{n}$, lemma \ref{lemma-linear-transformation-domain-distribution}. The cardinality of  $S' = D' \cap S$ is at least $t = \left\lceil c_{\frac{1}{2}}\left(\frac{2^l}{n}\right)\log\left(\frac{2^l}{n}\right)\right\rceil$ since we supposed the presence of $E_2$. The theorem \ref{theorem-set-onto-by-linear-transform} gives us the lower bound $\frac{1}{2}$ on the probability of covering the whole set $A'$ by a set of size at least $t$. 
\end{proof}

Now we must bound the probability of existence of a long chain.
\begin{remark}
There is a constant $C$ such that for all $r > 4$ in the scheme of hashing $S \subset D$, $|S| = n \log n$ using the hashing table $B = Z_2^{\log n}$ the following holds:
\begin{displaymath}
P(lpsl > rC \log n \log \log n) \leq 2 \left(\frac{r}{\log r}\right)^{-\log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)}\textit{.}
\end{displaymath}
\end{remark}
\begin{proof}
The proof is a straightforward use of previous remarks.
\begin{displaymath}
\begin{split}
l & = \left\lfloor \log n + \log \log n + \log r - \log \log r + 1 \right\rfloor \\
t & = 4c_{\frac{1}{2}}r\log n \log \log n \textit{.}
\end{split}
\end{displaymath}

The condition of the remark \ref{remark-e2-probability} is certainly fulfilled:
\begin{displaymath}
d = \frac{2^l}{n \log n} \geq \frac{2^{\log n + \log \log n + \log r - \log \log r}}{n \log n} = \frac{r}{\log r} > 1\textit{.}
\end{displaymath}

The following inequality helps us to prove the assumption of the remark \ref{remark-prob-t-length-chain}:
\begin{displaymath}
\frac{2^l}{n} \leq \frac{2 ^{\log n + \log \log n + \log r - \log \log r + 1}}{n} = \frac{2 r\log n}{\log r}
\end{displaymath}
\begin{displaymath}
\begin{split}
c_{\frac{1}{2}}\frac{2^l}{n}\log\left(\frac{2^l}{n}\right)
	& < c_{\frac{1}{2}} 2 \left(\log n\right) \left(\frac{r}{\log r}\right)\left(2\log\log n \log r\right) \\
	& = 4 c_{\frac{1}{2}} r \log n \log \log n \\
	& = t
\end{split}
\end{displaymath}

The probability $P(E_2 | E_1) \geq \frac{1}{2}$ is obtained for the given values of $l$ and $t$. This implies $P(E_1) \leq 2 P(E_2)$ which gives us the $E_1$ event's probability as:
\begin{displaymath}
\begin{split}
P(E_1) 
	& \leq 2d^{-\log d - \log \log d} \\
	& \leq 2\left(\frac{r}{\log r}\right)^{-\log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} \\
\end{split}
\end{displaymath}

The event $E_1$ denotes the existence of a chain of size at least $t$ elements. There is a chain longer than $t$ in a hash table if and only if the longest chain is longer than $t$. This proof completed is by putting $C = 4c_{\frac{1}{2}}$.
\end{proof}

Because of the previous claim we simply find the expected longest chain length, denote $K = C\log n \log \log n$.
\begin{displaymath}
\begin{split}
E lpsl 
	& = \int\limits_0^{\infty} P(lpsl > t) dt \\
	& \leq 4K + \int\limits_{4K}^\infty P(lpsl > t) dt \\
	& = 4K + K \int\limits_4^\infty P(lpsl > tK) dt \\
	& \leq 4K + K \int\limits_4^\infty 2 \left(\frac{r}{\log r}\right)^{-\log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)} dr \\
	& = K(4 + I) = O(K) = O(\log n \log \log n) \\
I 	& = \int\limits_4^\infty 2 \left(\frac{r}{\log r}\right)^{-\log \left(\frac{r}{\log r}\right) - \log \log \left(\frac{r}{\log r}\right)}
\end{split}
\end{displaymath}
\end{proof}

For the practical use the multiplicative constant $4C(4 + I)$ is also important. In our proofs we neglected its estimation and we only obtained a good asymptotic growth which is negated by its great value. For example when choosing $\epsilon$ equal to $\frac{1}{2}$ the constant $c_\epsilon$ equals $4 ^ {17}$ when using the original estimate. This value is too big for a practical use. Our next goal is to show a better constant's estimate and explore the dependency of the longest chain on load factor of the hash table.

\section{Expected case when rehashing}
We already showed that $P(lpsl > 2C \log n \log \log n)$ is less than $\frac{1}{2}$. This fact is obtained by a direct use of the Markov inequality. This means that less than half of functions create longest chains longer than $2C \log n \log \log n$. This ends by rehashing of the whole table.
\begin{displaymath}
\begin{split}
|\lbrace h \in H \mid \textit{ h does not create a long chains} \rbrace| 
	& = \left(1 - P(lpsl > 2Z \log n \log \log n)\right) |H|  \\
	& \geq \frac{|H|}{2} \\
\end{split}
\end{displaymath}

Regarding the fact that the function is chosen uniformly and not suitable are discarded we still have an uniform selection. We used a smaller function system; note that the restriction here is done by using the information about the hashed set.
\begin{displaymath}
\begin{split}
P(h(x) = h(y)) 
	& =  \frac{|\lbrace h \in H \mid h(x) = h(y) \wedge \textit{ h does not create long chains} \rbrace |}{|\lbrace h \in H \mid \textit{ h does not create long chains} \rbrace|} \\
	& \leq \frac{2 |\lbrace h \in H \mid h(x) = h(y) \rbrace}{|H|} \\
	& \leq 2 \frac{|H|}{m |H|} = \frac{2}{m}
\end{split}
\end{displaymath}

The last equality is implied by $1$-universality of systems of linear transformations. Similar restrictions of the other universal systems may be used. For such systems probability of collision of two (or more) elements is estimated by their strong $k$-universality or $c$-universality.

The previous computation shows us that the expected chain length is still constant, at most it can be doubled. Of course by selecting the greater length of longest chain we will omit less mappings. Thus we obtain better expected results for the find operation. But the warranty for the worst case is adequately worsened.
