\begin{section}{Probabilistic properties of the system of linear maps}

Most of the~following claims in this section are taken from \cite{DBLP:journals/jacm/AlonDMPT99}. It is convenient to show the~original proofs and then modify them according to our future needs. Some technical definitions and statements follow that are useful in order to show our goal.

\begin{definition}
Let $V$ be a vector space and $A$ be a subset of $V$. For a vector $v \in V$ define set $v + A$ as
\[ v + A = \{ v + a \setdelim a \in A \} \text{.} \] 
\end{definition}

\begin{definition}
Let $V$ be a vector space and $A, B$ be a subsets of $V$. Define set $A + B$ as
\[ A + B = \{ a + b \setdelim a \in A, b \in B \} \text{.} \] 
\end{definition}

\begin{lemma}
\label{lemma-choose-random-vector}
Let $V$ be a~finite vector space and $A$ be its subset. Define $\mu = 1 - \frac{|A|}{|V|}$ as inverse density of set $A$ in vector space $V$. Let $v \in V$ be a random uniformly chosen vector independent of $A$. Then
\begin{displaymath}
\Expect{1 - \frac{|A \cup (v + A)|}{|V|}} = \mu^2
\end{displaymath}
where the~expectation is taken throughout all possible choices of $v \in V$.

\begin{proof}
To simplify further computations define $X_v = |A \cup (v + A)|$ as a random variable taken throughout random uniform choice of vector $v \in V$. The most difficult part of the proof is to compute $\Expect{X_v}$.
\[
\Expect{X_v} = \displaystyle\sum_{v \in V} |A \cup (v + A)| . \Prob{v \text{ is chosen}} = \displaystyle\sum_{v \in V} \frac{|A \cup (v + A)|}{|V|}
\]

Size of set $|A \cup (v + A)|$ can be expressed using indicator function.
\[
|A \cup (v + A)| = \displaystyle\sum_{u \in V} I(u \in A \vee u \in (v + A)) \\
\]

Now notice if $u \in A$ there are exactly $|V|$ vectors that satisfy the above condition. For $u \notin A$ and $a \in A$ there is exactly one vector $v \in V$ such that $a + v = u$ and there are $|A|(|V| - |A|)$ such possibilities and the remaining choices are refused. Thus
\[ 
\begin{split}
|\{(u, v) \setdelim u \in A \vee u \in (v + A), u, v \in V \}| 
	& = |A|(|V| - |A|) + |V||A| \\
	& = 2|V||A| - |A|^2 \text{.} \\
\end{split}
\]

Substituting into the definition of $\Expect{X_v}$ and rewriting sums into the just computed size of a suitable set.
\[
\begin{split}
\Expect{X_v} 
	& = \frac{\sum_{v \in V} \sum_{u \in V} I(u \in A \vee u \in (v + A))}{|V|}  \\
	& = \frac{|\{(u, v) \setdelim u \in A \vee u \in (v + A), u, v \in V \}|}{|V|} \\ 
	& = \frac{2|V||A| - |A|^2}{|V|} \\
\end{split}
\]

Now finally compute the wanted expected value.
\[
\begin{split}
\Expect{1 - \frac{|A \cup (v + A)|}{|V|}} 
	& = 1 - \frac{\Expect{|A \cup (v + A)|}}{|V|}  \\
	& = 1 - \frac{\Expect{X_v}}{|V|} \\
	& = 1 - \frac{2|V||A| - |A|^2}{|V|^2} \\
	& = \frac{|V|^2 + 2|V||A| - |A|^2}{|V|^2} \\
	& = \left(1 - \frac{|A|}{|V|}\right)^2 = \mu^2 \\
\end{split}
\]
\end{proof}
\end{lemma}

The following lemma is a~very technical one and is used to estimate the~probabilities of some events related to linear maps.
\begin{lemma}
\label{lemma-random-variable}
For $1 \leq i \leq k$ the~$\mu_i$ are random variables and $0 < \mu_0 < 1$ is a~constant. For the~random variables, $1 \leq i \leq k$, we assume the~following:
\begin{gather*}
0 \leq \mu_i \leq \mu_{i - 1} \\
E[ \mu_i | \mu_{i-1} \dots \mu_1 ] = \mu_{i-1}^{2} \\
\end{gather*}
Then for every constant $0 < t < 1$ we can estimate the~probability:
\begin{displaymath}
P(\mu_k \geq t) \leq \mu_0^{k - \log \log (\frac{1}{t}) + \log \log \left(\frac{1}{\mu_0}\right)}
\end{displaymath}
\end{lemma}
\begin{proof}
We prove the statement by induction over $k$. 

\paragraph*{The initial step, $k = 0$.}
Since $\mu_0$ is constant we have
\[
	\Prob{\mu_0 \geq t} = \begin{cases}
		0 & \text{ if } \mu_0 < t \\
		1 & \text{ if } \mu_0 \geq t \text{.} \\
	\end{cases}
\]

From $\mu_0 > 0$ it follows
\[
	\mu_0^{0 - \log \log \left(\frac{1}{t}\right) + \log \log \left(\frac{1}{\mu_0}\right)} \geq 0
\]
and thus the estimate holds for $\mu_0 < t$.

If $1 > \mu_0 \geq t$ then $-\log \log \left(\frac{1}{t}\right) + \log \log \left(\frac{1}{\mu_0}\right) \leq 0$ and hence
\[
	\mu_0^{0 - \log \log \left(\frac{1}{t}\right) + \log \log \left(\frac{1}{\mu_0}\right)} \geq 1 \text{.}
\]

Thus the statement holds for $k = 0$.

\paragraph*{The induction step.} We prove the statement holds for $k \geq 0$ then it holds for $k + 1$. Let $t \in (0, 1)$ be fixed. For simplicity, let us denote $c = k - \log \log \left(\frac{1}{t}\right)$. Then we have to prove
\[
	\Prob{\mu_{k + 1} \geq t} \leq \mu_0 ^ {c + 1 + \log \log \left(\frac{1}{\mu_0}\right)} \text{.}
\]
Whenever exponent $c + 1 + \log \log \left(\frac{1}{\mu_0}\right) \leq 0$ the estimate holds, because $\mu_0 < 1$. We can restrict ourselves to case when $c + 1 + \log \log \left(\frac{1}{\mu_0}\right) > 0$. To prove our statement we fix $\mu_1$ and we the induction hypothesis for $k$. For value $a \in \left[0, \mu_0\right]$ define $g(a) = \Prob{\mu_{k + 1} \geq t | \mu_1 = a}$. Then
\[
	\Prob{\mu_{k + 1} \geq t} = \int\limits_{0}^{\mu_0}\Prob{\mu_{k + 1} \geq t | \mu_1 = a}\Prob{\mu_1 = a} \, da = \Expect{g(\mu_1)} \text{.}
\]

Functions $f$ and $f_0$ are defined as
\[ 
	f_0(x) = \begin{cases}
		x ^ {c + \log \log \left(\frac{1}{x}\right)} & \text{ if } 0 < x < 1 \\ 
		0 & \text{ if } x = 0 \\
	\end{cases}
\]
and $f(x) = \min \{1, f_0(x) \}$ for $0 \leq x < 1$.

If $\beta_0 = a \in \left[0, \mu_0 \right]$ is constant and $\beta_i$ for $i = 1, \dots, k$ are random variables satisfying conditions of this lemma then $\Prob{\beta_k \geq t} = g(a)$. From the induction hypothesis for $k$ we have $g(a) \leq f(a) = a^{k - \log \log \left(\frac{1}{t}\right) + \log \log \left(\frac{1}{a}\right)}$ for every $a \in \left(0, \mu_0 \right]$.

Next we investigate behaviour of $\frac{f_0}{x}$ on the interval $(0, 1)$. We start to compute the first derivation for every $x \in (0, 1)$.
\[
\left(\frac{f_0(x)}{x}\right)' = (c - 1 + \log \log \left( \frac{1}{x} \right) + \log e)\frac{f_0(x)}{x^2}
\]

Define the stationary point $x_s = 2 ^ {-2 ^ {-c + 1 - \log e}}$. Function $\frac{f_0(x)}{x}$ is then increasing in the interval $(0, x_s)$ and decreasing in the interval $(x_s, 1)$. 

Let us also define $x_1 = 2 ^ {-2 ^ {-c}}$, the point where $f_0(x)$ first reaches 1.
\[
f_0(x_1) = {x_1} ^ {c + \log \log \left(\frac{1}{x_1}\right)} = {x_1} ^ {c - c} = 1
\]
Since $-c > -c + 1 -\log e$ inequality $x_1 < x_s$ holds and the investigated function $\frac{f_0(x)}{x}$ is still increasing in the point $x_1$.
Function $f_0(x)$ is increasing for every $x \in \left[x_1, 1\right)$. Let $x' < x$.
\[
\begin{split}
f_0(x') 
	& = {x'} ^ {c + \log \log \left(\frac{1}{x'}\right)} \\
	& < {x} ^ {c + \log \log \left(\frac{1}{x'}\right)} \\
	& < {x} ^ {c + \log \log \left(\frac{1}{x}\right)} \\
\end{split}
\]

Thus we have $f_0(x) \geq f_0(x_1) = 1$ and $f(x) = \min \{1, f_0(x)\} = 1$.

At last define the third point $x_2 = x_1 ^ 2 = 2 ^ {-2 ^ {-c - 1}}$. This point lies in the decreasing phase because $-c - 1 < -c + 1 - \log e$ and then $x_2 > x_s$. In the just defined point $x_2$ the following statement holds.
\[
\begin{split}
\frac{f_0(x_2)}{x_2} 
	& = \frac{\left(2 ^ {-2 ^ {-c - 1}}\right) ^ {c + \log \left(- \log \left(2 ^ {-2 ^ {-c - 1}}\right)\right)}}{2 ^ {-2 ^ {-c - 1}}} \\
	& = \frac{\left(2 ^ {-2 ^ {-c - 1}}\right) ^ {c + \log \left(2 ^ {-c - 1}\right)}}{2 ^ {-2 ^ {-c - 1}}} \\
	& = \frac{\left(2 ^ {-2 ^ {-c - 1}}\right) ^ {-1}}{2 ^ {-2 ^ {-c - 1}}} \\
	& = \frac{1}{x_2^2} = \frac{1}{x_1}
\end{split}
\]

The proof is divided into three cases. The first and the second case take care about the situation when the exponent is non-negative, $c + 1 + \log \log \left(\frac{1}{\mu_0}\right) \geq 0$. In both cases it is proved $f(x) \leq \frac{f_0(\mu_0)x}{\mu_0}$ for every $x \in (0, \mu_0]$.
\paragraph{Constant $\mu_0$ is in the increasing phase, $\mu_0 \leq x_s$.}
Function $\frac{f(x)}{x}$ is increasing in this phase hence
\[
f(x) = \frac{f(x)x}{x} \leq \frac{f_0(x)x}{x} \leq \frac{f_0(\mu_0)x}{\mu_0} \text{.}
\]

\paragraph{Constant $\mu_0$ is in the decreasing phase, $x_s \leq \mu_0 \leq x_2$.}
For $x \in (0, x_1]$ following holds because we are still in the increasing phase.
\[
\frac{f(x)}{x} \leq \frac{f_0(x)}{x} \leq \frac{f_0(x_1)}{x_1} = \frac{1}{x_1}
\]

Function $f(x)$ is then in the interval $(0, x_1]$ bounded by \[f(x) = \frac{f(x)x}{x} \leq \frac{x}{x_1} \text{.} \]

For every $x \in [x_1, \mu_0]$ 
\[ 
	f(x) = 1 = \frac{x}{x} \leq \frac{x}{x_1}
\]

Using the fact $\frac{1}{x_1} = \frac{f_0(x_2)}{x_2}$ it is clear that 
\[
	f(x) \leq \frac{x}{x_1} = \frac{f_0(x_2)x}{x_2} \leq \frac{f_0(\mu_0)x}{\mu_0}
\]
because both $x_2 > \mu_0$ are already in the decreasing phase.

In both first two cases we showed $f(x) \leq \frac{f_0(\mu_0)x}{\mu_0}$ for every $x \in (0, \mu_0]$. Now using this statement we prove the lemma.
\[
\begin{split}
\Prob{\mu_{k + 1} \geq t}
	& = \Expect{g(\mu_1)} \leq \Expect{f(\mu_1)} \leq \Expect{\frac{f_0(\mu_0)\mu_1}{\mu_0}} = \frac{f_0(\mu_0)}{\mu_0}\Expect{\mu_1|\mu_0} \\
	& = \frac{f_0(\mu_0)}{\mu_0}\mu_0 ^ 2 = \mu_0 f_0(\mu_0) = {\mu_0}^{c + 1 + \log \log \left(\frac{1}{\mu_0}\right)}
\end{split}
\]

\paragraph{Constant $\mu_0$ is set so that exponent was negative, $\mu_0 \geq x_2$.}
To prove the lemma in this case as observed before it is sufficient to show that the exponent is not positive and the estimate is then at least 1.
\[
\begin{split}
	c + 1 + \log \log \left( \frac{1}{\mu_0} \right) 
		& \leq c + 1 + \log \log \left( \frac{1}{x_2} \right) \\ 
		& = c + 1 + \log \left(- \log \left(2 ^ {-2 ^ {-c - 1}}\right)\right) \\ 
		& = c + 1 + \log \left(2 ^ {-c - 1}\right) \\ 
		& = c + 1 - c - 1 = 0
\end{split}
\]

The inequality holds for every of the three cases and the induction step is complete.
\end{proof}

\begin{theorem}
\label{theorem-linear-function-set-onto}
Let $u$ and $t$ be natural numbers such that $0 < t \leq u$. Let $S$ be a~proper and non-empty subset of the~vector space $\vecspace{u}$. Set $\mu = 1 - \frac{|S|}{2^u}$ as inverse density of $S$ in $\vecspace{u}$. Then for a random uniformly chosen surjective linear map $T: \vecspace{u} \rightarrow \vecspace{t}$ we have
\[
	\Prob{T(S) \neq \vecspace{t}} \leq \mu^{u - t - \log t + \log \log \frac{1}{\mu}} \textit{.}
\]
\end{theorem}
\begin{proof}
Set $s = u - t$. Choose vectors $v_1, \dots, v_s \in \vecspace{u}$ independently and randomly using the uniform distribution. Note that vectors $v_1, \dots, v_s \in \vecspace{u}$ need not to be linearly independent.

We perform random and uniform choice of linear transformation $T$ according to model \ref{remark-model-surjective-linear-map-selection}. Maximal linearly independent subset of vectors $v_1, \dots, v_s$ may be extended to a random basis $b$ of vector space $\vecspace{u}$. Now choose a random permutation $\pi \in \Pi_t$ as stated in the just mentioned model. Since selection of vectors $v_1, \dots, v_s$ is random and uniform, moreover vectors are independent of each other the basis $b$ is chosen uniformly indeed. Moreover the vectors $v_1, \dots, v_s$ define a part $s'$ of the needed set $s \in S$ since we need to place them in the kernel of the constructed mapping $T$. Set $s'$ must be uniformly extended to any set $s \in S$ such that $s' \subseteq s$. The uniform choice of function $T$ is done by this. Just note that $T(v_i) = 0$ for all $i = 1, \dots, s$.	

Let us define $S_0 = S$ and $S_i = S_{i - 1} \cup (S_{i - 1} + v_i)$ and set $\mu_i = 1 - \frac{|S_i|}{2 ^ u}$. When considering $\mu_i$ as random variables by using Lemma \ref{lemma-choose-random-vector} we can derive the fact that $\Expect{\mu_i} = \mu_{i - 1} ^ 2$ for every $i \in \{1, \dots, s \}$. Because every set $S_i$ is an extension of the previous set $S_{i - 1}$ it is clear that $0 < \mu = \mu_0 < 1$ and $\mu_i \leq \mu_{i - 1}$ for all $i = 1, \dots, s$. The assumptions of lemma \ref{lemma-random-variable} are satisfied and we obtain
\[
\begin{split}
\Prob{\mu_s \geq 2 ^ {-t}} 
	& \leq \mu ^ {s - \log \log \left(\frac{1}{2 ^ {-t}}\right) + \log \log \left( \frac{1}{\mu} \right)} \\
	& = \mu ^ {s - \log t + \log \log \left(\frac{1}{\mu}\right)} \\
	& = \mu ^ {u - t - \log t + \log \log \left(\frac{1}{\mu}\right)}
\end{split}
\]

We want to show that whenever $\mu_s < 2^{-t}$ the event $T(S_s) = \vecspace{t}$ occurs. Since $\mu_s = 1 - \frac{|S_s|}{2 ^ u}$ the size of set $S_s$ equals ${2 ^ u}(1 - \mu_s)$. Using the assumption $\mu_s < 2 ^ {-t}$ it follows that $|S_s| > 2^u - 2^{u - t}$. To get the contradiction we assume that there is a vector $x \in \vecspace{t} - T(S_s)$ or equivalently $T(S_s) \neq \vecspace{t}$. Under these conditions it is clear that $T ^ {-1}(x)$ and $S_s$ are disjoint. From lemma \ref{lemma-linear-transformation-domain-distribution} we have $|T ^ {-1}(x)| = 2 ^ {u - t}$. This would mean that
\[
2 ^ u = |\vecspace{u}| \geq |S_s \cup T^{-1}(x)| > 2 ^ u - 2 ^ {u - t} + 2 ^ {u - t} = 2 ^u
\] which is impossible. The fact that if $\mu_s < 2^{-t}$ then $T(S_s) = \vecspace{t}$ can be rewritten in terms of probability as
\[
	\Prob{\mu_s < 2^{-t}} \leq \Prob{T(S_s) = \vecspace{t}}
\]

Because $\vecspace{u}$ is a vector space over field $\mathbb{Z}_2$, by induction over $i$, we obtain that $S_i = S_{i - 1} \cup (v_i + S_{i - 1}) = S_0 + \vecspan{v_1, \dots, v_i}$. Note that $T(v_i) = 0$ because mapping $T$ was chosen so that every vector $v_i$ was placed in the kernel of $T$. This simply implies $T(S_s) = T(S)$.

The proof of the theorem is finished by combining the previous notes.

\[
\begin{split}
\Prob{T(S) \neq \vecspace{t}} 
	& = \Prob{T(S_s) \neq \vecspace{t}}  \\
	& = 1 - \Prob{T(S_s) = \vecspace{t}} \\
	& \leq 1 - \Prob{\mu_s < 2 ^ {-t}} \\
	& = \Prob{\mu_s \geq 2 ^ {-t}} \\
	& \leq \mu ^ {u - t - \log t + \log \log \left(\frac{1}{\mu}\right)} \\
\end{split}
\]
\end{proof}

The next theorem shows the probability of the complementary event, $T(S) \neq \vecspace{t}$, if the set $S$ is large enough.
\begin{theorem}
\label{theorem-set-onto-by-linear-transform}
If $T: \vecspace{w} \rightarrow \vecspace{t}$ is a random uniformly chosen linear map. Then for every $0 < \epsilon < 1$ there is a constant $c_\epsilon > 0$ such that for every subset $S$ of the domain $\vecspace{w}$, $|S| \geq c_\epsilon t 2^t$, the probability of mapping $S$ onto the whole space is
\[
	\Prob{T(S) = \vecspace{t}} \geq 1 - \epsilon \text{.}
\]
\end{theorem}
\begin{proof}
\input{linear_transformations_vk_proof}
\end{proof}

The biggest disadvantage of the current estimate of $c_\epsilon$ is high inaccuracy. For a practical use of the asymptotic growth proved in the following theorem we would need the smallest value possible. In the following sections we will try to lower the value of $c_\epsilon$ so that we are able to create a rule that allows a reasonable time warranty for the find operation.
\end{section}
