\section{Model of Universal Hashing}
\label{section-model}
In this section we provided the analysis in case of universal hashing with separate chaining. 
The way we manage to achieve worst-case lookup time is to keep all chains shorter than the prescribed limit. 
First we introduce the concept of limit function which bounds length of the longest chain with a reasonable probability. 
It turns out that time needed to keep chains short can be amortized to a constant.

\subsection{Limit function}
Limit functions play a crucial role in our model. 
The lower the values of a~limit function are the better warranty is obtained. Apparently each limit function depends on the size of the table, the load factor. 
When thinking about the probability with which the limit holds there is a tradeoff between the obtained worst case guarantee and the time spent per update.

\begin{definition}[Limit function, trimming rate, suitable function]
\label{definition-limit-function}
The function $l\colon \bbbn \times \mathbb{R}_0^+ \times (0, 1) \rightarrow \bbbn$ is called a \emph{limit function} with \emph{trimming rate} $p$, for $p \in (0, 1]$, if $\Prob{\lpsl > l(m, \alpha, p)} \leq p$.

Let $l$ be a limit function with a~trimming rate $p$.
We say that a function $f \in H$ is \emph{suitable}, or \emph{does not create a long chain}, for $S$ if $\lpsl(S) \le l(m, \alpha, p)$. 
We consider the value of the random variable $\lpsl$ for the the function $f$.
\end{definition}

\subsection{Algorithms}
The comprehensive list of all parameters of the hash table:
\begin{itemize}
	\item {$m_0$}, initial size of the hash table;
	\item {$\alpha_l$, $\alpha_u$, $\alpha_l < \alpha_u$}, lower and upper bound on the load factor, lower bound may be violated when $m = m_0$;
	\item {$\alpha_m$, $\alpha_m \in (\alpha_l, \alpha_u)$}, target load factor, when rehashing because of having more than $\alpha_u m$ or less than $\alpha_l m$ elements, we choose new $m$ so that $\alpha = \alpha_m$;
	\item \textbf{$\alpha'$, $\alpha' < \alpha_u$}, the load factor for which we bound $\lpsl$, this parameter sets the tradeoff between the worst case guarantee and the trimming rate.
\end{itemize}
Following three invariants briefly describe the model. 
\begin{itemize}
\item[(1)] \textbf{Universal class and limit function.} The chosen class $H$ must be at least a $c$-universal system which has a limit function $l$ with a trimming rate $p$.
\item[(2)] \textbf{Load Factor Rule.} The load factor of the table is kept in a predefined interval $[\alpha_l, \alpha_u]$ except when $m = m_0$.
\item[(3)] \textbf{Chain Length Limit Rule.} If there is a chain longer than $l(m, \alpha', p)$, then the table is rehashed using a new chosen uniformly at random from $H$.
\end{itemize}

\input{algorithms}

\subsection{Consequences of Trimming Long Chains}
Now we deal with with the effects of Chain Length Limit Rule on the expected length of a chain.
We begin by definition of the system of all suitable functions for arbitrary stored set $S$.
\begin{definition}[$(l, p)$ - trimmed system]
\label{definition-trimmed-system}
Let $l$ be a limit function with a~trimming rate $p$.
The \emph{$(p, l)$-trimmed system} is the multiset of functions \[ H(p, l, S) = \{ f \in H \setdelim f \textit{ does not create a long chain for the limit function $l$} \}. \]
\end{definition}

The restriction of $H$ to $H(p, l, S)$ is informed about the stored set since it has a feedback if a function is suitable or not. 
Furthermore, the uniform choice of function from the original system $H$ with refusal of the unsuitable functions is equivalent to the uniform choice of a function from the restricted system.

We show that $(l, p)$-trimmed systems are still universal with a higher constant of universality. 
\begin{lemma}
\label{lemma-trimmed-system}
If $H$ is a universal system with a constant $c$ and $l$ is a limit function with a trimming rate $p$, then $|H(p, l, S)| \geq (1 - p)|H|$ and $H(p, l, S)$ is $c/(1 - p)$ universal.
\end{lemma}
\begin{proof}
From Definitions \ref{definition-trimmed-system} and \ref{definition-limit-function} it is clear that $\Prob{f \in H(p, l, S)} \geq 1 - p$ and, as a result, $|H(p, l, S)| \geq (1 - p)|H|$.
Moreover for different $x, y \in U$ it holds that
\[
\begin{split}
& \Prob{f(x) = f(y) \mid f \in H(p, l, S)} 
	= \frac{\Prob{f(x) = f(y),\, f \in H(p, l, S)}}{\Prob{f \in H(p, l, S)}} \\
	& \qquad \leq \frac{\Prob{f(x) = f(y),\, f \in H}}{1 - p} = \frac{\Prob{f(x) = f(y) \mbox{ for } f \in H}}{1 - p}. \\
\end{split}
\]
\qed
\end{proof}

Because the expected length of a chain is upper bounded by $c\alpha$ for $c$-universal systems, for $(l, p)$-trimmed systems we obtain that $\Expect{\psl} \leq c\alpha/(1-p)$.

Lemma \ref{lemma-no-trials} gives an estimate on the expected number of trials needed to find a function $f \in H(p, l, S)$ if $f$ is chosen uniformly from $H$.
\begin{lemma}
\label{lemma-no-trials}
If $l$ is a limit function with a trimming rate $p$, then the expected number of trials needed to find a function from $H(p, l, S)$ is at most ${1}/{(1 - p)}$.
\end{lemma}
\begin{proof}
Because the subsequent choices are independent, we know that the probability of having at least $k$ trials is at most $p^{k - 1}$. Hence the expected value is upper bounded by $\sum_{k = 1}^{\infty} p^{k - 1} = \sum_{k = 0}^{\infty} p^k = {1}/{(1 - p)}.$
\qed
\end{proof}

\subsection{Amortized Analysis}
We analyze the amortized complexity of the model with the potential method. Let $p_i$ be the potential of the hash table after performing $i$\textsuperscript{th} operation and $t_i$ be the time consumed by the operation. We define the amortized complexity of the operation as $a_i = t_i + p_i - p_{i - 1}$. The expected amortized complexity of the sequence $A$ equals
\[
\Expect{A} = \sum_{i=1}^{k} \Expect{a_i} = \sum_{i = 1}^{k} \left(\Expect{t_i} + \Expect{p_i} - \Expect{p_{i - 1}}\right) = \Expect T + \Expect{p_k} - \Expect{p_0}.
\]

%TODO: Staticke odhady.
%Although deletion of an element can not prolong chains, such static bound may be invalid since the assumed static model is no longer preserved. Let us note that dynamic extensions of Theorem \ref{theorem-universal-hashing-two-choices} are available and discussed in \cite{DBLP:journals/jacm/Vocking03}. However, it is easy to see that allowing deletions with the static estimates does not cause any problem in our model .
%TODO: Kolaps lemy 3?
%Lemma \ref{lemma-sets} states an upper bound on the expected number of trials needed to enforce Chain Length Limit Rule for a sequence of sets $S_1, \dots, S_k$.
%\begin{lemma}
%\label{lemma-sets}
%Let $S_1 \subset \dots \subset S_k$ be a sequence of sets such that $|S_k| \leq \alpha' m$ and $h_0 \in H$ be an initial function. Let $h_1, \dots, h_l \in H$ be a sequence of independently and uniformly chosen functions tried to enforce Chain Length Limit Rule. Assume that $0 = i_0 < \dots < i_k = l$ is a sequence of integers such that for each $j \in \{0, \dots, k - 1\}$
%\begin{itemize}
%\item the functions $h_{i_{j}}, h_{i_{j} + 1}, \dots, h_{i_{j + 1} - 1}$ are not suitable for the set $S_{j + 1}$ and 
%\item the function $h_{i_{j}}$ is suitable for the set $S_j$.
%\end{itemize}
%Then $\Expect{l} \leq {1}/{(1 - p)}$.
%\end{lemma}
%\begin{proof}
%A function $h$ is suitable for the set $S_k$ only if it is suitable for all the sets $S_1, \dots, S_k$. Using Lemma \ref{lemma-no-trials} for the set $S_k$ completes the proof.
%\qed
%\end{proof}

\begin{theorem}
\label{theorem-amortised-expected-time}
Assume the hash table described in Algorithm \ref{algorithm-hash-table} obeying the invariants (1) -- (3). Then the expected amortized time complexity of each operation is constant and the worst case running time of Find operation is $\bigo(1 + l(m, \alpha', p))$.
\end{theorem}
\begin{proof}
To prove the theorem assume that we are given a sequence of operations Insert, Delete and Find. We consider Insert and Delete to be successful or unsuccessful according to the comments in Algorithm \ref{algorithm-hash-table}. Since Find, unsuccessful Delete and unsuccessful Insert do not change the stored set, they do not have any effect on the dictionary and its potential and thus we may omit them from the sequence. The worst case complexity of Find follows from Chain Length Limit Rule. The expected running time of Find and unsuccessful updates follows from Lemma \ref{lemma-trimmed-system}. 

Now we have to prove the statement for successful update operations. To do so we partition the sequence into two types of cycles and for both of them we define a potential. The first cycle type is used to amortize violations of Load Factor Rule. With the second one we are able to distribute the time required to repair violations of Chain Length Limit Rule.

\begin{definition}[$\alpha$-cycle, $l$-cycle]
Cycles partition operations of the analyzed sequence.
Each \emph{$\alpha$-cycle} ends just after the operation causing violation of Load Factor Rule.
Each \emph{$l$-cycle} ends after $(\alpha'-\alpha_u)m$-th successful insertion in the $l$-cycle or after an operation which causes violation of Load Factor Rule (both cycles end).
\end{definition}

Let us define $e = 1/(1-p)$. Let ($i_{\alpha}$, $d_\alpha$, $i_l$) be the number of successful (insertions, deletions, insertions) performed in the current ($\alpha$, $\alpha$, l)-cycle. Let $r$ be the number of performed trials of a hash function caused by violation of Chain Limit Rule and $c$ be the number of started $l$-cycles. Both $r$ and $c$ are counted from the initial state. We define the potential $p_1 = {2ei_{\alpha}}/{(\alpha_u - \alpha_m)} + {2ed_{\alpha}}/{(\alpha_m - \alpha_l)}$ and the potential $p_2 = {ei_{l}}/{(\alpha' - \alpha_u)} + (ce - r) m$.  The overall potential is $p = p_1 + p_2$.

We decompose each update operation into an actual update and a possible rehashing part. Since the expected length of a chain is constant the expected running time of an update is constant as well. Observe that the overall potential is increased by a constant if the analyzed operation is not the last one in a cycle. Hence the amortized complexity of an update is constant. 

Each trial of a function from $H$ caused by Chain Limit Rule violation is amortized by incrementing $r$. When we break Load Factor Rule, the $\alpha$-cycle ends so $p_1$ decreases by at least $2em$ and $p_2$ is increased by at most $em$, hence $\Delta p \leq -em$ which can pay for the rehashing work. Similarly when an $l$-cycle is ended because $i_l = (\alpha' - \alpha_u)m$, then $\Delta p_2 = 0$. 

Now we argue that $\Expect{ce - r} \geq 0$. If we omitted deletes from each $l$-cycle, we would get a sequence of sets created only be insertions. Deletes may not prolong chains so from Lemma \ref{lemma-sets} it follows that the expected number of hash function trials in an $l$-cycle is at most $e$. Since $c$ is the number of started $l$-cycles and $r$ is the total number of such trials, we have $\Expect{ce} \geq \Expect{r}$ and thus $\Expect{p_k} \geq 0$. The theorem now follows because our potential is expected to be non-negative, $p_0 = \bigo(1)$ and as we have already seen $\Expect{T} \leq \Expect{A} - \Expect{p_k} + \Expect{p_0}$.
\qed
\end{proof}

In case of "insertion only" limit functions Delete operation may be allowed as well without any further change. If each $l$-cycle started with a rehash, then we would obtain an insertion only hash table. Observe that one additional rehash in each $l$-cycle can be amortized to constant time by introducing another potential $p_3 = {ei_{l}}/{(\alpha' - \alpha_u)}$. Moreover, performing the mentioned rehash is not necessary and, as a result, the first rehash in each $l$-cycle may be caused by invalidity of the limit function.

With trimming rates which tend to zero with growing $n$, e.g. in case of the two choices paradigm, the amortization overhead caused by Chain Length Limit Rule gradually disappears. It is caused by the fact that for a sufficiently large $n$ almost each function is suitable for every $S$.
