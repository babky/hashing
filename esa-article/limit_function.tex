\section{Obtaining the limit function}
\label{section-limit}

So far we have studied the possibility of providing a worst case guarantee for Find if we are given a limit function. 
In this section we are interested in a way how to find such a limit function.

\subsection{Linear hash functions}
Alon, Dietzfelbinger, Bro Miltersen, Petrank and Tardos \cite{DBLP:journals/jacm/AlonDMPT99} found an interesting upper bound on $\Expect \lpsl$ with the system of all linear functions between two vector spaces over the field $\bbbz_2$. 
Representing the universe $U$ and the addresses of the hash table $V$ as vector spaces is natural and causes no problem. 
If $m_0$ is a power of two at the end of $\alpha$-cycle we just double or halve the address space and thus $V$ is kept to be a vector space over $\bbbz_2$.

\begin{theorem}[\cite{DBLP:journals/jacm/AlonDMPT99}]
\label{theorem-linear-hash-functions-dietzefelbinger}
Suppose universal hashing with the system of linear functions from $U$ to $V$. 
When storing $m \log m$ elements, $\Expect{\lpsl} = \bigo(\log m \log \log m)$. 
\end{theorem}

The major problem of Theorem \ref{theorem-linear-hash-functions-dietzefelbinger} is the high multiplicative constant but it can be significantly reduced by a refinement of the original proof. 
First, in order to obtain a valid limit function, a dependence of $\lpsl$ on $\alpha$ needs to be discovered.
Further improvement is obtained by storing $n$ elements for $n = \bige(m)$ instead of $n = m \log m$.
In our computations we also introduced some variables and optimized their values in order to reduce the multiplicative constant.
The improved result is stated in Theorem \ref{theorem-linear-refined}.

\begin{theorem}
\label{theorem-linear-refined}
Assume universal hashing with the system of all linear transformations between vector spaces over $\bbbz_2$. 
Let $p \in (0, 1)$ be the trimming rate and $\alpha > 0$. 
If $n = \alpha m$ elements are stored inside the hash table of size $m$, then $$\Expect{\lpsl} \leq 538 \alpha \log n \log \log n + 44\mbox{, and}$$ $$\Prob{\lpsl \geq a_{\alpha, p} \log m \log \log m + b_{\alpha, p}\log m} < p.$$ where constants $a$ and $b$ depend on the choice of $\alpha$ and $p$.
\end{theorem}

Let us note that the exact constants for different trimming rates and load factors can be found by a simple computer program.
For example the choice of $\alpha = 1.5$ and $p = 0.5$ yields $a = 57.29$ and $b = 0$.
In addition, constant $a$ can get arbitrarily close to $1$ but such estimates hold only for large $n$.

\subsection{Two choices paradigm}
The two choices paradigm comes from the area of balls and bins systems. For separate chaining with a single perfectly random function it is known that $\Expect{\lpsl} = \bigo\left({\log n}/{\log \log n}\right)$. A proof of this bound may be found in \cite{DBLP:books/sp/Mehlhorn84}. The same bound also holds with a single function chosen from an almost uniform system. On the other hand for $k$-wise independence this problem becomes harder and general bounds for universal hashing are not available.

A further study \cite{DBLP:conf/stoc/AzarBKU94} of balls and bins systems discovered that hashing with at least two independent fully random hash functions brings far better results. If each stored element is put inside a least loaded bin of $d$ ones, then with a high probability the most loaded bin contains less than ${\ln \ln n}/{\ln d} + \bigo(1)$ elements. It is not so hard to realize that these results hold in case of universal hashing with almost uniform systems. Bounds that are possible to achieve with a limited independence for a general $S \subset U$, are not known so far.

Because of the following definition we have to extend the notation. For $k \in \bbbn$, $x_1, \dots, x_k \in U$ and $f \in H$ the function $f$ applied to a vector $(x_1, \dots, x_k) \in U^k$ is defined as $f((x_1, \dots, x_k)) = (f(x_1), \dots, f(x_k))$.
\begin{definition}[Independent hash functions]
\label{definition-independent-hash-functions}
Let $H$ be a uniform system of hash functions and $f, g \in H$ be two functions chosen uniformly at random from the system $H$. We say that the functions $f, g$ are \emph{independent} if for each $\vec{x} \in U^n$ with different elements and arbitrary $y, z \in V^n$ $$\Prob{f(\vec{x}) = \vec{y}, g(\vec{x}) = \vec{z}} = \Prob{f(\vec{x}) = \vec{y}} \Prob{g(\vec{x}) = \vec{z}}.$$
\end{definition}

Notice that the independent choice of $f_1, \dots, f_d$ from $H$ is sufficient for the functions to be independent in terms of Definition \ref{definition-independent-hash-functions}. Theorem \ref{theorem-universal-hashing-two-choices} is a straightforward restatement of results pioneered in \cite{DBLP:conf/stoc/AzarBKU94} which originally hold only with fully random functions.

\begin{theorem}
\label{theorem-universal-hashing-two-choices}
Let $H$ be an almost uniform universal system with a constant $c$ such that $c > 0$ and $d \in \bbbn$. Assume that each stored element $x$ is placed into a least loaded bucket from $f_1(x), \dots, f_d(x)$. The addresses are determined by $d$ independent hash functions $f_1, \dots, f_d \in H$. If $n \leq m$ and $c \alpha \leq 1$, then $$\Prob{\lpsl > \frac{\ln \ln n}{\ln d} + 7} \in \littleo\left(\frac{1}{n}\right).$$
\end{theorem}
\begin{proof}
We only discuss differences from the proof for perfectly random functions found in \cite{Mitzenmacher:2005:PCR:1076315} or \cite{DBLP:conf/stoc/AzarBKU94}. Observe that the probability of collisions of $k$, $k \leq n$, different elements is at most ${c}/{m^k}$. The probability of collisions in the same sequence of elements with a truly random function equals ${1}/{m^k}$. Thus we have to calculate with the multiplicative constant $c$.
From the independence of functions $f_1, \dots, f_d$ it is clear that $\Prob{f_i(x) \in R,\, \forall i \in \{0, \dots, d\}} = \prod_{i = 1}^{d}\Prob{f_i(x) \in R}$ for each $x \in U$ and $R \subseteq V$. Hence for $c = 1$ no change has to be done. However, for $c > 1$ our assumption $\alpha c \leq 1$ is sufficient to prove the result.
\qed
\end{proof}

To be precise we have to discuss existence and efficiency of uniform systems. It is obvious that the system of all functions is strongly $\omega$-universal and hence uniform. The main problem here are space requirements -- $\bigo(|U| \log |V|)$ bits. There are other $\omega$-universal classes known but they are quite complex. Although their functions may be computed in a constant time, they are not efficient enough. 

A convenient example of a uniform system appeared in \cite{DBLP:journals/siamcomp/PaghP08}. Its construction is randomized and the system is uniform with a high probability.
\begin{theorem}[\cite{DBLP:journals/siamcomp/PaghP08}]
\label{theorem-uniform-system}
For any constant $c > 0$ there is a RAM algorithm constructing a random family of functions from $U$ to $V$ in expected $\littleo(n) + (\log \log|U|)^{\bigo(1)}$ time and $\littleo(n)$ words of space, such that:
\begin{itemize}
\item With probability $1 - \bigo(1 / n^c)$ the family is uniform on $S$.
\item There is a RAM data structure of $\bigo(n \log|V| + \log \log|U|)$ bits, which is optimal, representing its functions such that function values can be computed in constant time. Choosing a function from the system takes $\bigo(n)$ time.
\end{itemize}
\end{theorem}

The underlying RAM has words of size $\bige(\log |U| + \log |V|)$ bits. Since the construction of the system is probabilistic and it does not have to be uniform, the limit function is valid only with a certain probability. Because the limit function is valid when $H$ is uniform, the overall trimming rate may be computed as
\[
\begin{split} 
& \Prob{\lpsl \leq \frac{\ln \ln n}{\ln d} + 7,\, H \mbox{ is uniform}} \\
	& \qquad = \Prob{\lpsl \leq \frac{\ln \ln n}{\ln d} + 7 \mid H \mbox{ is uniform}} \Prob{H \mbox{ is uniform}} \\ 
	& \qquad = \left(1 - \littleo\left(\frac{1}{n}\right)\right)\left(1 - \bigo\left(\frac{1}{n^c}\right)\right). \\
\end{split}
\]
The positive fact is that this probability tends to one as $n$ increases. The unpleasant thing is that choosing a new function means regenerating the system and takes $\bigo(n)$ time.

\subsection{Truly random functions}
\label{subsection-truly-random-functions}
In plain hashing we get full randomness because of the probabilistic properties of input. Although these assumptions are seldom correct, we often have in some way randomly distributed data. As observed in \cite{DBLP:conf/soda/MitzenmacherV08}, strong $2$-universality may behave like a truly random function when input contains certain amount of entropy. If the requirements are satisfied, then we can use the same limit functions as for truly random functions.

In case of the two choices paradigm the limit function is $\bigo(\log \log n)$ and with a~single hash function $\bigo(\log n / \log \log n)$. These bounds are tight.

Notice that bounds discussed in previous sections hold for any stored set without any other assumption. They rely only on the randomness inherited by using a high-quality universal system. In a situation when the input data are random enough it is correct and much more efficient to use a simpler universal system with the assumption of full randomness.

% With trimming rates which tend to zero with growing $n$, e.g. in case of the two choices paradigm, the amortization overhead caused by Chain Length Limit Rule gradually disappears. It is caused by the fact that for a sufficiently large $n$ almost each function is suitable for every $S$.

