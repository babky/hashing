
\chapter{The Model of Universal Hashing}
\label{chapter-proposed-model}

The hashing scheme we propose later in this chapter is a solution to the set representation problem. Solution of this problems usually provide some basic operations such as \emph{Insert}, \emph{Delete} and \emph{Find}. These operations allow querying if an element is stored within the represented set, accessing the stored elements and inserting an element. Some schemes, e.g. double hashing, do not implement the element removal at all or the efficient implementation is not known.

The must important are the operations' running times. Various solutions to many algorithmic problems prefer different operations when representing sets. For instance some applications query the stored data rarely, e.g. log files. On the other hand, other applications of the set representation problem may store the data that is almost never changed -- find operation is preferred. Therefore the running times of only selected operations are considered crucial. 

In the case of a simple array we have $O(1)$ time for the find procedure provided that we know an element's index. But insertion or deletion may take $O(n)$ time. Better bounds for dynamic arrays can be obtained using the amortised complexity. Another example are balanced trees, they have running times typically bounded by the logarithmic time. As already mentioned, the right answer which data structure should be used lies in the estimated structure of the operations. The right choice may be an asymptotic improvement. Anyway, this does not change the fact that short running times are appreciated.

In this chapter we analyse the running times of the universal hashing. We start by mentioning the known facts. Then, we analyse the universal hashing using the system of linear transformations over vector spaces. Finally, we propose a model that guarantees the worst case complexity of the find operation.

\section{Time Complexity of Universal Hashing}
In this section we assume that the system of hash functions, which we use, is at least $c$-universal. The running time of the find operation is certainly proportional to the length of the chain of an element's bucket. The obvious worst case time is $O(n)$ where $n$ is the number of elements hashed. The universal hashing gives a far better expected estimate, $O(1)$.

Recall the definitions and notation from Chapter \ref{chapter-hashing}. The value $n$ denotes the size of the represented set and $m$ is the size of the table. The load factor of a hash table is denoted by $\alpha = \frac{n}{m}$. 

\begin{theorem}
\label{theorem-expected-chain-length-universal}
Assume that we store a set $S$ in a hash table using a $c$-universal class of functions $H$. Let the table's load factor $\alpha$ be upper bounded. Then the expected length of a chain is lower or equals $c \alpha$.
\end{theorem}
\begin{proof}
We find the expected length of a chain containing arbitrary element $x \in U$. The expectation is taken over the uniform choice of a function from the universal family of functions. From the definition of the expected value we have that
\begin{displaymath}
\begin{split}
\Expect{\psl} 
	& = \frac{\sum\displaylimits_{h \in H} psl(h(x), h) }{|H|} \\
	& = \frac{\sum\displaylimits_{h \in H} \sum\displaylimits_{y \in S} I(h(x) = h(y))}{|H|} \\
	& = \frac{\sum\displaylimits_{y \in S} \sum\displaylimits_{h \in H} I(h(x) = h(y))}{|H|}  \\
	& = \sum\displaylimits_{y \in S} \Prob{h(x) = h(y)} \\
	& \leq \frac{cn}{m} = c \alpha \text{.}
\end{split}
\end{displaymath}
\end{proof}

\begin{corollary}
\label{corollary-c-universal-find}
If we hash using a $c$-universal class and $\alpha$ denotes the table's load factor, then the expected time of Find operation is $1 + c\alpha$.
\end{corollary}
\begin{proof}
The running time of the find operation is proportional to the length of the chain in which the searched element belongs. In the worst case Find operation iterates the whole chain. We also add time for retrieving the element's hash value and for checking if the chain is not empty. These operations are usually performed in a constant time. From Theorem \ref{theorem-expected-chain-length-universal} it follows that the expected length of a chain is $c\alpha$. Since we have no assumptions on the distribution of the input, the expected running time is then bounded by $1 + c\alpha$. 
\end{proof}

\begin{corollary}
\label{corollary-find-time}
The expected time of Find operation in every model of universal hashing is $O(1)$ when the load factor is bounded.
\end{corollary}
\begin{proof}
Follows from Corollary \ref{corollary-c-universal-find}.
\end{proof}

\section{Consequences of Trimming Long Chains}
The model of hashing we propose guarantees the worst case bound on the length of the longest chain. Hence it bounds the running times of the operations. Knowledge $\Expect{\lpsl}$ enables us to state a bound on the length of a chain. If a chain, whose length is greater than our bound, is found, then we choose another function in our $c$-universal system and we rehash the represented set using the new function. This \emph{bound} corresponds to the probability of the existence of a long chain. In fact, we set the \emph{bound} according to this probability. Now let us examine consequences of such \emph{limits} on models of universal hashing.

Following computations support and motivate our later ideas. By the Markov's inequality, we have that for every $k > 1$ \[ \Prob{\lpsl > k \Expect{\lpsl}} \leq \frac{1}{k} \text{.} \] This fact means that less than half of all the universal functions create longest chains longer than $2 \Expect{\lpsl}$ for arbitrary stored set. For instance, in case of the system of linear transformations we may choose the \emph{limit} as $2\Expect{\lpsl} \leq \text{1 076} \log m \log \log m + 88$. This \emph{bound} follows from Corollary \ref{corollary-best-elpsl}.

The expected length of the longest chain gives us a generic hint when the table should be rehashed if the worst case time for the find operation has to be guaranteed. The lower the value $\Expect{\lpsl}$, or the tighter its estimate is, the better worst case \emph{limit} is achieved.

Mentioned \emph{bound} on the length of a chain, which is computed using the Markov's inequality and the expected value, can be further improved. If the probability density function of the random variable $\lpsl$ is known, then the density function may be used directly. Such \emph{limit} $l$ is associated with a probability $p \in (0, 1)$ that can be computed from the density function as $\Prob{\lpsl > l} \leq p$. And we can also go the other way, for the probability $p$ we may find a minimal limit $l$ with $\Prob{\lpsl > l} \leq p$.

In addition, for the system of linear transformations we already have the probability density function and  we use it in Section \ref{section-linear-systems-linear-amount-constant-estimate}, indeed. To sum up, the approach with the expected length and the Markov's inequality is more general but achieves greater limits. The approach with the probability density function gives better results. It is less general because if we know the probability density function of $\lpsl$, then we are able to find $\Expect{\lpsl}$.

\begin{definition}[Chain length limit function, long chain, $p$-trimmed-system, trimming rate]
Let $H$ be a universal system of functions that map a universe $U$ to a hash table $B$. Let $m$ be the size of the hash table, $S \subset U$ be the stored set with $n = |S|$ and $\alpha = \frac{n}{m}$ be the table's load factor. 

Then function $l: \mathbb{N} \times \mathbb{R}_{0}^{+} \rightarrow \mathbb{N}$ of variables $m$ and $\alpha$, $l(m, \alpha)$, is a \emph{chain length limit function}. 

We say that function $h \in H$ creates a \emph{long chain when hashing the set $S$} if there is a chain of length strictly greater than the limit value $l(m, \alpha)$.

Moreover let $p \in (0, 1)$ be probability such that $\Prob{\lpsl > l(m, \alpha)} \leq p$, then the system \[ H_p^S = \{ h \in H \setdelim h \text{ does not create a long chain when hashing the set $S$} \} \] is called a \emph{$p$-trimmed system}. The probability $p$ is called the \emph{trimming rate}.
\end{definition}

The probability bound $p \in (0, 1)$ of the existence of a long chain has important consequences for the model that limits its chains.
\begin{itemize}
\item At most $p|H|$ of all the functions in the original universal system $H$ create long chains -- longer than the prescribed limit $l(m, \alpha)$. 
\item The probability that the table needs to be rehashed, equivalently probability of selecting an inconvenient function, is lower than $p$ provided the uniform choice of a hash function.
\item During rehashing caused by an occurrence of a long chain, the probability of finding a suitable function is at least $1 - p$ when assuming the uniform choice of a hash function.
\end{itemize}

\begin{lemma}
\label{lemma-size-of-trimmed-system}
If $H_p^S$ is a $p$-trimmed system, then $|H_p^S| \geq (1 - p)|H|$.
\end{lemma}
\begin{proof}
Simply use the definition of $H_p^S$ and that of the trimming rate $p$:
\[
\begin{split}
|H_p^S|
	& = |\lbrace h \in H \setdelim \text{ $h$ does not create a long chain} \rbrace| \\
	& =\Prob{\lpsl \leq l(m, \alpha)} |H| \\
	& = \left(1 - \Prob{\lpsl > l(m, \alpha)}\right) |H| \\
	& \geq (1 - p)|H| \text{.}
\end{split}
\]
\end{proof}

Regarding that every function is chosen uniformly and the unsuitable ones are discarded, we still perform the uniform selection of a hash function. The choice is restricted to the functions that do not create long chains; to the class $H_p^S$. Note that the restriction to the functions of the original universal system $H$ comes from an information about the stored set.

Previous remarks are quite interesting. So now, we ask, if it is possible to use the family $H_p^S$ as a universal one.
\begin{theorem}
\label{theorem-p-trimmed-is-universal}
Let $H$ be a $c$-universal system of hash functions, $U$ be a universe and $B$ be a hash table. Let $p \in (0, 1)$ be the trimming rate and set $m = |B|$. Then for every $S \subset U$ the system of functions $H_p^S$ is $\frac{c}{1 - p}$-universal. Equivalently:
\[
	\Prob{h(x) = h(y) \text{ for } h \in H_p^S} \leq \frac{1}{1 - p} \Prob{h(x) = h(y) \text{ for } h \in H} \text{.}
\]
\end{theorem}
\begin{proof}
From Lemma \ref{lemma-size-of-trimmed-system} and from the assumption of $c$-universality of the original system it follows that 
\[
\begin{split}
& \Prob{h(x) = h(y) \text{ for } h \in H_p^S}  \\
	& \qquad =  \frac{|\{ h \in H \mid h(x) = h(y) \wedge \textit{ h does not create long chains} \}|}{|\lbrace h \in H \mid \textit{ h does not create long chains} \rbrace|} \\
	& \qquad \leq \frac{|\{ h \in H \mid h(x) = h(y) \wedge \textit{ h does not create long chains} \}|}{(1 - p)|H|} \\ 
	& \qquad \leq \frac{|\{ h \in H \mid h(x) = h(y) \}|}{(1 - p)|H|} \\
	& \qquad = \frac{1}{1 - p} \Prob{h(x) = h(y) \text{ for } h \in H} \\
	& \qquad \leq \frac{c}{(1 - p) m} \text{.}
\end{split}
\]

Hence the system $H_p^S$ is $\frac{c}{1 - p}$-universal.
\end{proof}

Similar statements hold for the strongly universal systems. The probability of a collision in the system $H_p^S$ is always $\frac{1}{1 - p}$ times higher than the probability of a collision in the original system $H$.

Next statement summarises results for the systems of linear transformations.

\begin{corollary}
\label{corollary-trimming-linear}
For every trimming rate $0 < p < 1$ the $p$-trimmed system of linear transformations, $LT(U, B)_p^S$, is $\frac{1}{1 - p}$-universal.
\end{corollary}
\begin{proof}
System of linear transformations is $1$-universal as seen in Remark \ref{remark-system-of-linear-transformations}. The fact then follows from  Theorem \ref{theorem-p-trimmed-is-universal}.
\end{proof}

Every chain length limit function $l(m, \alpha)$ comes with an associated trimming rate, probability of the event $\lpsl > l(m, \alpha)$.  This probability not only determines the probability of failure for a single function but it also determines the expected number of trials to find a suitable function, as stated in Lemma \ref{lemma-linear-transformations-trials}. 

\begin{lemma}
\label{lemma-linear-transformations-trials}
Let $l$ be a chain length limit function and $p \in (0, 1)$ be the trimming rate such that $\Prob{\lpsl > l(m, \alpha)} \leq p $. Then the expected number of trials to find a function, which does not create a long chain, is at most $\frac{1}{1 - p}$ and the variance of this number is bounded by $\frac p{(1-p) ^ 2}$.
\end{lemma}
\begin{proof}
The probability of $k$ independent unsuccessful searches of a function with bounded chains is at most $p ^ k$ and thus the distribution of the first successful attempt is bounded by the geometric distribution. For an estimate of the expected value and the variance of the first successful attempt we need the following facts, that may be found in \cite{210884},
\begin{gather*}
	\displaystyle\sum_{i = 0}^{\infty} p ^ i = \frac{1}{(1 - p)} \text{,} \\
	\displaystyle\sum_{i = 0}^{\infty} i p ^ i = \frac{p}{(1 - p) ^ 2} \text{,} \\
	\displaystyle\sum_{i = 0}^{\infty} i^2 p ^ i = \frac{p(1 + p)}{(1 - p) ^ 3} \text{.} \\
\end{gather*}

The expected time of success is then given by:
\[
\sum_{i = 0}^{\infty} (i + 1)p^i(1 - p)= (1 - p)\sum_{i = 0}^{\infty}p^i + (1 - p)\sum_{i = 0}^{\infty}ip^i = \frac{1 - p}{1 - p} + \frac{(1 - p)p}{(1- p)^2} = \frac{1}{(1 - p)} \text{.}
\]

Now we estimate the variance:
\[
\begin{split}
& \sum_{i = 0}^{\infty} \left(i + 1 - \frac{1}{(1 - p)}\right) ^ 2  p ^ i (1 - p) \\
	& \qquad = \sum_{i = 0}^{\infty} \left(i + 1\right) ^ 2 p ^ i (1 - p) - \sum_{i = 0}^{\infty} 2(i + 1)p ^ i + \sum_{i = 0}^{\infty} \frac{p ^ i}{(1 - p)} \\
	& \qquad = \frac{1 - p}{p}\left(\sum_{i = 0}^{\infty} i ^ 2 p ^ i \right) - \frac{2}{(1 - p) ^ 2} + \frac{1}{(1 - p) ^ 2} \\
	& \qquad = \frac{1 - p}{p}\frac{p(1 + p)}{(1 - p) ^ 3} - \frac{1}{(1 - p) ^ 2} \\
	& \qquad = \frac{1 + p }{(1 - p) ^ 2} - \frac{1}{(1 - p) ^ 2} \\
	& \qquad = \frac{p}{(1 - p) ^ 2} \text{.}
\end{split}
\]
\end{proof}

So the schema to obtain a chain length limit function is as follows. For a prescribed probability of failure -- trimming rate $p$ we find a minimal chain length limit function $l(m, \alpha)$ such that $\Prob{\lpsl > l(m, \alpha)} \leq p$. The probability $p$ is chosen according to the expected number of trials required to find a function which does not create a long chain. For example in our model, we choose two trails and thus $p = 0.5$.

The lower the trimming rate $p$ is the greater values of $l(m, \alpha)$ are obtained in order to meet $\Prob{\lpsl > l(m, \alpha)} \leq p$. In addition, from Corollary \ref{corollary-trimming-linear} it follows that the smaller the trimming rate $p$ is, the better expected results are obtained. And Theorem \ref{theorem-expected-chain-length-universal} implies that the expected chain length still remains constant provided that the load factor $\alpha$ is bounded. So the small values of $p$ give good expected results and low number of trails required to obtain a function. On the other hand choosing a low trimming rate $p$ gives only a poor worst case warranty.

The most interesting and the most important idea of trimming is that every $p$-trimmed system is an adaptation of the original class $H$ to the stored set $S$.

\section{Chain Length Limit}
\label{section-linear-systems-linear-amount-constant-estimate}
From now we concentrate our effort to obtain the tightest bound on the chain length for a given trimming rate $p \in (0, 1)$. The corresponding bound is determined from the density function shown in Remark \ref{remark-lpsl-pdf-linear-amount}. This limit is used later in our model in Section \ref{section-proposed-model}.

In Theorem \ref{theorem-model-chain-limit-rule} the set $S_e$ is not directly the stored set. Instead, we use the set $S_e$ that comes from the analysis of our model shown in Theorems \ref{theorem-no-delete-time} and \ref{theorem-delete-time}. On the other hand every set $S_e$ is a subset of the stored set $S$. The size of the set $S_e$ is bounded according to the requirements of our analysis.

\begin{theorem}
\label{theorem-model-chain-limit-rule}
Let $T: \vecspace{u} \rightarrow \vecspace{b}$ be a random uniformly chosen linear transformation, $m = 2 ^ b$, $\alpha' \in \{1, 1.5\}$, $S_e \subset \vecspace{u}$ such that $|S_e| \leq \alpha' m$ and $p \in (0, 1)$ be the trimming rate.

Then there is a chain length limit function $l(m) = a \log m \log \log m + b \log m$ for some $a, b \in \mathbb{R}$ depending only on $\alpha'$ and $p$. For the chain length limit function $l(m)$ we have $\Prob{\lpsl > l(m)} \leq p$.

\begin{itemize}
\item When $\alpha' = 1.5$ and $p = 0.5$, set $l(m) = 57.29 \log m \log \log m$.
\item When $\alpha' = 1$ and $p = 0.5$, set $l(m) = 47.63 \log m \log \log m$.
\end{itemize}
\end{theorem}
\begin{proof}
It is enough to prove the statement for $|S_e| = \alpha' m$. For a smaller set $S_e$, with $|S_e| < \alpha' m$, it follows that $\Prob{\lpsl > l(m)} \leq p$, since it may be extended to a set that has exactly $\alpha'm$ elements and confirms to the bound. Hence the statement holds for every $S_e$ such that $|S_e| \leq \alpha'm$ provided that it holds for every $S_e$ with $|S_e| = \alpha' m$.

Now assume that $|S_e| = \alpha' m$ and set $f(x) = x ^ {\log b - \log \alpha' - \log x - \log \log x}$. From Corollary \ref{corollary-f1} it follows that the function $f(x)$ is decreasing in the interval $\left[2, \infty \right)$.

First, we introduce a parameter $\epsilon \in (0, 1)$ which is used later in the proof to minimise the value of the limit function. If $r \geq 4$, $\alpha' \in \left(0.5, \frac{r}{\log f}\right)$ and $\epsilon \in (0, 1)$, then by Remark \ref{remark-lpsl-pdf-linear-amount} we have
\[
	\Prob{\lpsl > 2 \alpha' c_\epsilon r} \leq \frac{1}{1 - \epsilon} f\left(\frac{r}{\log r}\right) \text{.}
\]

We find the minimal value of $r$ such that $\frac{1}{1 - \epsilon} f\left(\frac{r}{\log r}\right) \leq p$. By setting the value of the variable $r$ we also obtain the chain limit confirming to the prescribed trimming rate $p$. 

Our next step is to define a lower bound $d$, $d \geq 2$, of the expression $\frac{r}{\log r}$. Since $d \leq \frac{r}{\log r}$ we have that $f(d) \geq f\left(\frac{r}{\log r}\right)$ because the function $f$ is decreasing in the interval $[2, \infty)$. Whenever we have a value of the variable $d$ such that $f(d) \leq (1 - \epsilon) p$, then $f\left(\frac{r}{\log r}\right) \leq (1 - \epsilon) p$, too. If we manage to find the minimal value of $r$ from $d \leq \frac{r}{\log r}$, then we set the chain limit to $2 \alpha' c_\epsilon r$ and the trimming rate $p$ is achieved as well. 

First, we show a way how to estimate the value of the variable $r$ for a given $d \geq 2$. 
\begin{claim}
\label{claim-choose-r}
If $d \geq 2$ and $r = 2d \log d$, then $d \leq \frac{r}{\log r}$ and $r \geq 4$.
\end{claim}
\begin{proof}
Since we selected $d$ as a lower bound for $\frac{r}{\log r}$ we have to find minimal $r$ from the inequality $\frac{r}{\log r} \geq d$. Observe that for every $d \geq 2$ we have that $\log d \geq 1 + \log \log d$ and hence \[\frac{\log d + \log d}{1 + \log d + \log \log d} \geq 1 \text{.}\] Putting $r = 2 d \log d$ satisfies the inequality since
\[
\frac{r}{\log r} = \frac{2 d \log d}{1 + \log d + \log \log d} = \frac{d(\log d + \log d)}{1 + \log d + \log \log d} \geq d \text{.}
\]

The value of $r$ is thus $r = 2d \log d \geq 4$.
\end{proof}

The probability estimate of the event $\lpsl > 2 c_\epsilon \alpha' r$ requires that $r \geq 4$. For the choice of the value $r$ from Claim \ref{claim-choose-r} it follows that $r \geq 4$ when $d \geq 2$. Because we choose $d \geq 2$, we no longer pay attention to the assumption of Remark \ref{remark-lpsl-pdf-linear-amount}. The remaining one $\alpha' < \frac{\log r}{2}$ may cause a problem. Its validity must be verified at the end, immediately after we state the exact choice of $r$.

To finish the proof set the lower bound $d = j b$ for a positive constant $j$. Instead of finding exact value of $d$, it is sufficient to find a minimal value of $j$. The simplification is motivated by the fact that the order of the asymptotic growth of $\Expect{\lpsl}$ is $b \log b$. In Claim \ref{claim-whole-limit} we show that putting $d = jb$ respects this asymptotic growth. So now we just need to find the multiplicative constant as small as possible. 

\begin{claim}
\label{claim-whole-limit}
Let $j$ be a positive constant such that $d = jb \geq 2 $, then the chain limit rule we propose has the form \[ 4 c_\epsilon \alpha' j b (\log b + \log j) \text{.} \]
\end{claim}
\begin{proof}
We choose the value of $r$ from Claim \ref{claim-choose-r} as
\[
	r = 2 d \log d = 2 j b (\log b + \log j) \text{.}
\]

Hence our chain limit can be finally rewritten as:
\[
	4 c_\epsilon \alpha' r = 4 c_\epsilon \alpha' j b (\log b + \log j) \text{.}
\]
\end{proof}

\begin{claim}
To find the minimal value of $d$, $d \geq 2$ satisfying $f(d) \leq (1 - \epsilon) p$ use the inequality
\stepcounter{definition}
\begin{equation}
\label{inequality-formula-j}
	\left(j b\right)^{-\log \alpha' -\log j - \log \log (j b)} \leq (1 - \epsilon)p \text{.}
\end{equation}
\end{claim}
\begin{proof}
From remark Remark \ref{remark-lpsl-pdf-linear-amount} we obtain the following inequality that allows us to find the minimal suitable value of $d$:
\[
	f(d) = d ^ {\log b - \log \alpha' - \log d - \log \log d} \leq (1 - \epsilon) p \text{.}
\]

We substitute $j b$ into $d$ to get the required result:
\[
	d ^ {\log b - \log \alpha' - \log d - \log \log d} = \left(j b\right)^{-\log \alpha' -\log j - \log \log (j b)} \leq (1 - \epsilon)p \text{.}
\]
\end{proof}

Recall that we use the hash table $B = \vecspace{b}$ and refer to $m = 2 ^ b$ as to its size. Inequality \ref{inequality-formula-j} may have various interpretations.
\begin{itemize}
\item For fixed $0 < p, \epsilon < 1$ and a positive constant $j$ we get a lower bound on $m$ when our estimate becomes valid. Realise, that we can obtain arbitrarily low multiplicative constant. This follows from the fact that we are allowed to choose values for the constant $j$. Such estimates are valid only for large numbers of stored elements, since we need $d = jb \geq 2$ and $n \geq \frac{m}{2} \geq 2 ^ {\frac{2}{j} - 1}$.
\item Or we can find the parameters $\epsilon$ and $j$ such that multiplicative constant $4 c_\epsilon \alpha' j$ is the smallest possible for the trimming rate $p$ and the size of the hash table $m$. This statement is used to find the required estimate for $m \geq \text{4 096}$.
\end{itemize}

We use Inequality \ref{inequality-formula-j} to obtain the chain limit. The limit is computed for tables consisting of at least 4 096 buckets and the probability bound is set to $0.5$. These choices were not random. When we used the formula for the first time, we gained the multiplicative constants in the order of tens. Estimates with the multiplicative constant in the order of tens start beating the most basic linear estimate, $\lpsl \leq \alpha' m$, when hashing thousands of elements.

Program optimising the value of the multiplicative constant only minimises the value $4 c_\epsilon \alpha' j$ and does not pay any attention to the other constant. After the minimal value is retrieved, together with the values of the parameters, the value of the constant, $4 c_\epsilon j \log j b$, is determined. To find the value of the constant $c_\epsilon$ according to Inequality \ref{inequality-better-c-e} of Statement \ref{statement-better-c-e}, additional parameters $k$ and $l$ are required. We use Algorithm \ref{algorithm-scheme-3} to solve this optimisation problem.

\begin{algorithm}[H]
\caption{Calculate the multiplicative constant for parameters $p, m, \alpha', \epsilon, k, l$.}
\label{procedure-scheme-3}
\begin{algorithmic}
\STATE $c_\epsilon$ $\leftarrow$ constant $c_\epsilon$ computed with parameters $k$, $\epsilon$ and $l$
\STATE $r \leftarrow (1 - \epsilon)p$ \COMMENT{Right side, inevitably achieved bound.}
\STATE $j \leftarrow 1$
\STATE 
\STATE \COMMENT{Lower the value of $j$ so that the right side is skipped.}
\STATE $l \leftarrow (j \log m) ^ {-\log(\alpha') - \log(j) - \log(\log(j \log m))}$
\WHILE {$l < r$ \textbf{and} $j > 0$}
	\STATE $j \leftarrow j - STEP$;
	\STATE $l \leftarrow (j \log m) ^ {-\log(\alpha') - \log(j) - \log(\log(j \log m))}$
\ENDWHILE
\STATE
\STATE $j \leftarrow j + STEP$
\RETURN $4 c_\epsilon \alpha' j$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Calculate the smallest limit for $p=0.5$, $m \geq \text{4 096}$ and prescribed $\alpha'$.}
\label{algorithm-scheme-3}
\floatname{algorithm}{Procedure}
\begin{algorithmic}
\STATE $c \leftarrow \infty$
\FOR {$k \in \left[2, 4\right]\text{ with }STEP$}
	\FOR {$l \in \left[1.3, 3\right]\text{ with }STEP$}
		\FOR {$\epsilon \in \left[0.85, 0.99\right]\text{ with }STEP$}
			\IF {$c > \text{multiplicative constant for }p = 0.5, m = \text{4 096}, \alpha', \epsilon, k, l$}
				\STATE $c \leftarrow \text{computed multiplicative constant}$
			\ENDIF
		\ENDFOR
	\ENDFOR
\ENDFOR
\STATE
\RETURN $c$
\end{algorithmic}
\end{algorithm}

The form of the chain length limit function follows from Claim \ref{claim-whole-limit} and Algorithm \ref{algorithm-scheme-3}, which is able to compute the minimal constant $a$ and the corresponding constant $b$.
\end{proof}

The best result achieved for $\alpha' = 1.5$, $m = 2 ^ b \geq \text{4 096}$ is for $\epsilon = 0.96$, $j = 0.74$ and equals $4 c_\epsilon \alpha' j = 57.29$. The same approach gives multiplicative constant $47.63$ for $\alpha' = 1$. The assumption $\alpha' < \frac{r}{2}$ holds since $\alpha' \leq 1.5 < 6 < jb \leq d \log d = \frac{r}{2}$.

Now compare this limit with the linear estimate, $\lpsl \leq \alpha' m$, for the size of the hash table $m \geq \text{4 096}$ and the number of stored elements $n \geq \alpha' m = 0.5 \cdot \text{4 096} = \text{2 048}$. Linear estimate on the length of the longest chain equals $n = \text{2 048}$. Our estimate equals approximately $57.29 \log m \log \log m \leq 57.29 \cdot 12 \cdot 2.27 \leq \text{1 555}$ which is already far better. 

These limits may be improved since we neglected the part $4 c_\epsilon \alpha' j \log j \log m$ which is negative.

\section{The Proposed Model}
\label{section-proposed-model}
The model we propose is a slight modification of the simplest model of universal hashing scheme with separated chains. We distinguish the two cases -- when Delete operation is not allowed or when the stored elements can be removed.
\begin{itemize}
\item \textbf{Universal class.} We use the system of linear transformations as the universal family of functions. The universe $U = \vecspace{u}$ and the target space, the hash table, is referred to as $B = \vecspace{b}$. We may imagine the situation as hashing $u$-bit binary numbers to the space of $b$-bit binary numbers. We refer to the size of the hash table as to $m = |B| = 2 ^ b$.

\item \textbf{Load factor rule.} The load factor of the table is kept in the predefined interval. If the load factor is outside the interval, whole table is rehashed into a greater or smaller table. New size is chosen so that the load factor was as near $0.5$ as possible. Load factor is maintained in the interval $\left[0.5, 1\right]$ for the scheme without the delete operation. We need the interval $\left[0.25, 1\right]$ if the delete operation is allowed.

\item \textbf{Chain limit rule.} When there is a chain longer than the limit value $l(m)$, then the hash table is rehashed. The value $l(m)$ is chosen according to Theorem \ref{theorem-model-chain-limit-rule} with the trimming rate $p = 0.5$. The chain length limit function of our model does not depend on the table's load factor so we omit the parameter $\alpha$ and use just $l(m)$.

The exact chain length limit function comes from Theorem \ref{theorem-model-chain-limit-rule}. If Delete operation is forbidden, then set the limit value $l(m) = 47.63 \log m \log \log m$ and use $\alpha' = 1$. When Delete operation is allowed, the chain length limit function $l(m) = 57.29 \log m \log \log m$ and $\alpha' = 1.5$.
\end{itemize}

Estimates for the chain limit rule are valid for the tables consisting of at least 4~096 slots. There are two ways how to deal with this problem. First, we may set the initial size of the table to 4~096 buckets. If the size of the table is 4~096 buckets, then the lower bound of the load factor rule is not applied and we allow the table's load factor to be in the interval $[0, 1)$.

Since not every hash table grows to a size of 4~096 buckets, this overhead becomes unacceptable. The other option lies in turning off the chain limit rule when the table has less than 4~096 buckets.

\section{Time Complexity of Computing a Hash Value}
\label{section-time-complexity}
Since our model is based on the system of linear transformations, what is quite unusual, we have to deal with the time complexity of computing an element's hash value. We ask if the time required to compute the hash value is still constant. When we bound the size of the universe $|U|$, certainly it is. But this time may be worse, when compared to the time required by linear system \ref{definition-linear-system}. 

Linear system uses a constant number of arithmetic operations. If we do not bound the size of the universe $|U|$, we may compute them in $O(\log^2 |U|)$ time. 

Our system, system of linear transformations, represents every transformation by a binary matrix $M \in \vecspace{b \times u}$. Every element $\vec{x} \in U$ is a vector consisting of $u$ bits. To obtain its hash the matrix-vector multiplication $M\vec{x}$ is performed. We can certainly do it in $O(u b) = O(\log |U| \log |B|)$ time. 

If the size of the universe is bounded, this time is constant. Despite the better asymptotic bound, in practise, the time is worse than that of linear system. Assume, that we represent elements by a word determined by the computer's architecture. In this case linear system needs just three operations, one multiplication, one addition and a modulo. Computing a matrix-vector multiplication can be optimised, but is not so fast. 

In addition, when hashing is applied, elements usually fit in the word of the underlying architecture. Therefore the arithmetic operations become constant and this is our case, too. To deal with the longer running times of computing a hash value, sometimes, it is possible to cache once computed hash value within the element. This solution is a trade-off that improves the time complexity but consumes additional memory.

\section{Algorithms}
In Section \ref{section-proposed-model} we propose a model of universal hashing without any exact definition. Precise algorithms, showing how the operations work, are required. Despite the fact that the algorithms are very similar to those shown in Chapter \ref{chapter-hashing}, now they are described exactly.

First, let us describe the hash table's variables and notation. Hash table contains variables storing its size $Size$, the number of represented elements $Count$ and array $T$ is the hash table itself. Function $Limit$ denotes the chain length limit function and $Hash$ is the current universal function -- a linear transformation represented by a binary matrix. Every bucket $T[i]$ contains two variables $T[i].Size$, the number of elements inside the slot, and $T[i].Chain$, the linked list of the elements in the bucket.

\paragraph*{Initialisation.} We use Algorithm \ref{algorithm-scheme-3} from the proof of Theorem \ref{theorem-model-chain-limit-rule} to compute the bound for $\alpha' \in \{1, 1.5\}$, according to the status of Delete operation, and for $p = 0.5$. Theorem \ref{theorem-model-chain-limit-rule} states that the limit function is in the form $a \log m \log \log m + b \log m$ for $a, b \in \mathbb{R}$ depending only on $\alpha'$ and $p$. Thus we need to store just two real numbers $a$ and $b$ to represent the chain length limit function. Let us note that Theorem \ref{theorem-model-chain-limit-rule} gives the chain length limit function for a different parametrisation, too. However, in such case the amortised analysis must be changed. 

Initialisation creates a new empty table with the prescribed size. It also chooses a universal function by the random initialisation of the bits in the matrix $Hash$. The values of bits are chosen randomly each bit having the same probability $0.5$. If the delete operation is allowed $\alpha_{min} = 0.25$, $\alpha_{max} = 1$ and $\alpha' = 1.5$, if it is not, then $\alpha_{min} = 0.5$, $\alpha_{max} = {1}$ and $\alpha' = 1$.

\input{algorithms}

\paragraph*{Rehash operation}
To enumerate the stored elements in Rehash operation, Algorithm \ref{algorithm-rehash}, we iterate every chain of the table. A common optimisation is to place all the stored elements into a linked list. This allows faster enumeration but causes a space overhead.

Whenever a load factor rule or the chain limit rule is violated, the table is rehashed using a new function chosen in initialisation. Both Initialisation and Rehash operation, Algorithms \ref{algorithm-initialisation} and \ref{algorithm-rehash}, take an argument $m$ specifying the new size, so that it is possible to rehash the table into a larger or smaller one.

\paragraph*{Find, Insert and Delete operations}
Find is very straightforward. Let us notice that the linked list manipulation operations return \textbf{true} in the successful case, if an element is found, deleted or inserted, and otherwise return \textbf{false}. Insert and Delete operations are slightly complicated compared to the original ones because of the both rules that are required to hold.

\section{Potential Method} 
In Section \ref{section-expected-amortised-complexity} we estimate the expected amortised time complexity using the potential method. So let us explain it first. Assume that we have a data structure and a sequence of operations $\{o_i\}_{i = 1}^{n}$ performed on it. We want to estimate the running time of the sequence. Of course, we can use the worst case time for each operation but this may be very misleading. 

Consider a simple array. The elements are placed at the array's end. If there is a free position, we store the element. When the array is full, then we double its size and store the element. Clearly, if we store $n$ elements inside the array, then the worst case time required to insert another one is $O(n)$. We ask, what time is required to store $n$ elements in such an array. The estimate using the worst case time for each operation gives the result $O(n ^ 2) =O(\sum\displaylimits_{i = 1}^{n} i)$. Amortised analysis gives a far better result, $O(n)$. This result can be explained by the fact that the fast inserts accumulate a potential that is used later by a following slow insert.

Now we describe the potential method -- a method how to estimate the amortised complexity. The potential of the data structure and an operation's amortised cost are tools which distribute the running time of a sequence among the operations more evenly. Assume that we have a data structure at the initial state $s_0$ and we perform a sequence of $n \in \mathbb{N}$ operations $\{o_i\}_{i = 1}^{n}$. Every operation $o_i$ changes the state of the data structure from $s_{i - 1}$ to $s_i$, $i \in \{1, \dots, n\}$. 

Every state has a real-valued potential given by the potential function $p$. Hence we obtain a sequence of potentials $p_i$ for every $i \in \{0, \dots, n\}$. The potential $p_0$ is the initial potential and is often chosen as zero. So the operations not only change the data structure, they change its potential, too. We define $a_i = t_i + p_{i} - p_{i - 1}$ as the amortised cost of the $i$\textsuperscript{th} operation where $t_i$ is its running time. 

\begin{definition}[Amortised complexity]
Assume that we perform an operation of a data structure. Let $t$ be the time required to perform the operation. Let $p_b$ be the potential of the data structure before performing the operation and $p_a$ be the potential after. Then the \emph{amortised complexity} of the operation \[ a = t + p_a - p_b \text{.} \]
\end{definition}

The time taken by the operations in a sequence $o$, $T_o = \sum\displaystyle_{i = 1}^{n} t_i$, may be estimated by the amortised time of the sequence, $A_o = \sum\displaystyle_{i = 1}^{n} a_i$, as
\[
	T_o = \displaystyle\sum_{i = 1}^{n} t_i = \displaystyle\sum_{i = 1}^{n} (a_i - p_i + p_{i - 1}) = A_o + p_0 - p_n \text{.}
\]

Let us show some facts regarding the potential functions and the amortised complexity of a sequence of operations.
\begin{claim}
\label{claim-amortised-complexity}
Assume that we estimate the amortised complexity by a potential function $p$. Let $o = \{o_i\}_{i = 1}^{n}$ be the performed sequence of operations, $p_0$ be the starting potential and $p_n$ be the potential after performing the last operation.
\begin{enumerate}
\item [(1)] If $p \geq 0$, then $T_o \leq A_o + p_0$.
\item [(2)] If $p \leq 0$, then $T_o \leq A_o - p_n$.
\item [(3)] If $p_0 = 0$, then $T_o = A_o - p_n$.
\end{enumerate}
\end{claim}

For randomised algorithms we may take the expected time consumed by an operation.

\begin{definition}[Expected amortised complexity]
Assume that an operation of a randomised data structure is performed and the operation runs in time $t$. Let $p_b$ be the potential before performing the operation and $p_a$ be the potential after. Then the \emph{expected amortised complexity} of the operation is defined as \[ a = \Expect{t + p_a - p_{b}} \text{.} \]
\end{definition}

A description of the potential method may be found in \cite{VK-skripta} and \cite{DBLP:books/sp/MehlhornS2008}.

\section{Expected Amortised Complexity}
\label{section-expected-amortised-complexity}
Expected amortised time complexity of the introduced scheme is analysed in the next two theorems in either of the two cases -- when Delete operation is allowed or not.

Let us discuss the situation of Lemma \ref{lemma-sets}. We are given a sequence of sets $S_1 \subseteq \dots \subseteq S_k \subseteq S_e$ that should be represented in a hash table. We start with the initial hash function $h_0$. Set $S_1$ causes violation of the chain limit rule for function $h_0$. In order to enforce the rule, we select random functions $h_1, h_2, \dots$ until we find a suitable function for the set $S_1$, denote it $h_{i_1}$. Later, after some inserts, we obtain a set $S_2$ and the function $h_{i_1}$ is no longer suitable. We continue by selecting functions $h_{i_1 + 1}, \dots, h_{i_2}$ with $h_{i_2}$ being suitable for the set $S_2$. The chain length limit function is chosen for the set $S_e$ and for the trimming rate $p$ from Theorem \ref{theorem-model-chain-limit-rule}. We need to find the expected number of trials, the number of selected functions, needed to enforce the chain limit rule for the sets $S_1, \dots, S_k$. The set $S_e$ is a set that comes from the analysis, we use the same set $S_e$ is in Theorem \ref{theorem-model-chain-limit-rule}.

\begin{lemma}
\label{lemma-sets}
Let $\vecspace{u}$ be the universe, $\vecspace{b}$ be the hash table with the size $m = 2 ^ b$, $p \in (0, 1)$ be the trimming rate and $\alpha' \in \{1, 1.5\}$. Let the chain length limit function be chosen according to Theorem \ref{theorem-model-chain-limit-rule} for $\alpha'$ and $p$. 

Let $S_1 \subseteq \dots \subseteq S_k$ be a sequence of represented sets and $S_e \subset \vecspace{u}$ be a set such that $|S_e| \leq \alpha'm$ and $S_k \subseteq S_e$. Let $h_0, h_1, \dots, h_l$ be a sequence of random uniformly chosen linear transformations selected to enforce the chain limit rule for the sequence of sets. Assume that $0 = i_0 < \dots < i_k = l$ is the sequence such that 
\begin{enumerate}
\item[(1)] the functions $h_{i_{j}}, h_{i_{j} + 1}, \dots, h_{i_{j + 1} - 1}$ create a long chain for the set $S_{j + 1}$ for every $j \in \{0, \dots, k - 1 \}$,
\item[(2)] the function $h_{i_{j}}$ does not create a long chain for the set $S_j$, $j \in \{1, \dots, k\}$.
\end{enumerate}
Then $\Expect{l} = \frac{1}{1 - p}$.
\end{lemma}
\begin{proof}
First, observe that if a function $h$ is suitable for the set $S_e$, then it is suitable for every set $S_1, \dots, S_k$.

By Lemma \ref{lemma-linear-transformations-trials} the expected number of trials needed to represent the set $S_e$ without a long chain is $\frac{1}{1 - p}$.

The sequence of functions $h_1, \dots, h_l$ is random and the selection of every function is uniform. The chain length limit function is chosen according to Theorem \ref{theorem-model-chain-limit-rule} -- it fits for the set $S_e$. Hence if we do not consider the sets $S_1, \dots, S_k$, we have that $\Expect{l} = \frac{1}{1 - p}$.

If a function $h_a$, $1 \leq a < l$ creates a long chain for a set $S_b$, $i_{b - 1} \leq a < i_b$, then it certainly creates a long chain for the set $S_e$. Thus if we fail with the function $h_a$ for the set $S_b$, then we fail with the same function for the set $S_e$, too. In this situation we continue by choosing functions $h_{a + 1}, h_{a + 2}, \dots$. The sequence of functions $h_1, \dots, h_l$ for the sequence of sets, remains the same for the single set $S_e$, provided that the random choice is the same. So the functions selected for the sequence of sets may be selected when rehashing the set $S_e$, too. Hence we do not need to consider the sequence of sets when choosing a right function for the set $S_e$. 

Thus $\Expect{l} = \frac{1}{1 - p}$ as observed before.
\end{proof}

\begin{theorem}
\label{theorem-no-delete-time}
Consider hashing with forbidden Delete operation. Then the operations Find and Insert have the expected amortised time complexity $O(1)$. Moreover, if the size of a hash table is $m$, then the find operation runs in $O(\log m \log \log m)$ time in the worst case.
\end{theorem}
\begin{proof}
For the expected amortised complexity analysis we use the potential method. Let the expression $\alpha m - m$ denote the potential of the scheme. This is the negative value of the number of the remaining successful insert operations which would make the table's load factor reach one. Whenever a successful insertion is performed we check if 
\begin{itemize} 
\item the prolonged chain does not violate the chain limit rule or
\item the load factor is not greater than $1$.
\end{itemize} If either of the two conditions is violated the whole table is rehashed. We have four cases to analyse.
\begin{itemize}
\item \emph{Find operation} or an \emph{unsuccessful Insertion operation} is performed. From Theorem \ref{theorem-expected-chain-length-universal} and Corollary \ref{corollary-trimming-linear} we know that it takes $O(1)$ expected time only. The potential does not change. Since the chains are bounded by $l(m)$, its worst case running time is $O(\log m \log \log m)$.

\item \emph{Insert operation} is performed and the \emph{Rehash operation} is \emph{not necessary}. Then from Theorem \ref{theorem-expected-chain-length-universal} the operation takes $O(1)$ time in the expected case. The potential is increased by one.

\item \emph{Rehash operation} after an \emph{Insert operation} is required because the \emph{load factor} exceeds one. The rehash itself takes the $O(m)$ time and the size of the table is doubled. Thus the resulting potential is $-m$. The potential before the operation equals $0$. So the operation took $O(1)$ expected amortised time.

\item Suppose that a Rehash operation after an \emph{Insert} is needed because of the \emph{violation of the chain limit rule}. We seek for a new function satisfying the rule without resizing the table. For convenience we define a sub-sequence of operations called a \emph{cycle}.
\begin{definition}
\label{cycle}
\emph{Cycles} create a partitioning of the original sequence of operations. Each \emph{cycle} is a sub-sequence consisting of the operations between two immediate inserts which cause the violation of the load factor rule. The first insert causing the violation is included in the cycle and the second one belongs to the next cycle.
\end{definition}
  
We refer to the current cycle as to the cycle which contains the analysed operation. Now we find the number of rehash operations that are caused by the chain limit rule violation and occur in a single cycle. Let $S_e$ be the set represented at the end of the current cycle. Then by Lemma \ref{lemma-sets} we need just $\frac{1}{1 - p}$ trials in the current cycle.

Now we compute the expected amortised complexity of the insert operations causing the violation of the chain limit rule in a single cycle. For every table that consists of $m$ slots at the beginning of the cycle, there are exactly $0.5 m$ insert operations raising the load factor from $0.5$ to $1$. The expected time spent by fixing the chain limit rule in a cycle is, by Lemma \ref{lemma-sets}, $\frac{1}{(1 - p)}O(m)$. We can divide this amount of time along the cycle of $0.5 m$ inserts. This distribution raises the expected amortised time for every insert operation only by a constant. The potential is incremented by one. The expected time of Insert operation without Rehash operation is $O(1)$.
\end{itemize}

We have one issue to care about. In the last case we distributed the time evenly along a complete cycle. If the number of inserts is not a power of two, we may possibly distribute a long time along a non-complete cycle. With this distribution we can not obtain a constant amortised time for the insert operation. However, we can distribute the time spent by fixing the chain limit rule from the last incomplete cycle along all the insert operations performed so far.

Thus the expected amortised time complexity of every analysed operation is $O(1)$.
\end{proof}

One can find a better potential function proving the previous result more formally. We use such an explicitly expressed potential function in the next theorem. We assume that Delete operation is allowed.
\begin{theorem}
\label{theorem-delete-time}
If the initial hash table is empty and Delete operation is allowed, then the expected amortised time complexity of every operation is constant. Moreover, if the size of a hash table is $m$, then the find operation runs in $O(\log m \log \log m)$ time in the worst case.
\end{theorem}
\begin{proof}
In this proof we have two types of the operation cycles. We need to distinguish between the work required to enforce the load factor rule and the time spent by keeping the chain limit rule. In the analysis with forbidden Delete operation the situation is simpler and these cycles are the same. And recall the difference, the chain length limit function is computed for $\alpha' = 1.5$. The load factor $\alpha$ is now in the interval $\left[0.25, 1\right]$.

We deal with the amortised time of Find and unsuccessful Insert or Delete operations in advance. Their expected running time is proportional to the expected chain length. From Theorem \ref{theorem-expected-chain-length-universal} it follows that this value is constant. Since chains are bounded by $l(m)$ we have that the worst case time of Find operation is $O(\log m \log \log m)$. These operations do not change the potential\footnote{The potential is defined later in the proof.}. Our analysis is thus simplified by omitting finds and unsuccessful delete and insert operations.

Let the sequence $o = \{o_i\}_{i=1}^{n}$ denote the performed operations, the $i$\textsuperscript{th} operation $o_i \in \lbrace Insert, Delete \rbrace$. The following two definitions make the analysis more exact and clear.

\begin{definition}[$\alpha$-cycle]
The \emph{$\alpha$-cycle} consists of the operations between the two immediate operations that cause the violation of the load factor rule. Every $\alpha$-cycle contains the second operation causing the violation and the first one is included in the previous $\alpha$-cycle. The operation $o_1$ is contained in the first $\alpha$-cycle.
\end{definition}
First, notice that for this definition it is not important if the upper or lower bound of the load factor is the cause of the violation. When a rehash operation is executed, the table size is chosen so that the load factor $\alpha$ was as near $0.5$ as possible. 

The next definition of the $\alpha$-cycle is intended for the analysis of the time spent by fixing the violations of the load factor rule.

\begin{definition}[l-cycle]
The \emph{l-cycles} are the partitioning of the sequence $o$ such that every l-cycle ends 
\begin{itemize}
\item after an operation causing the violation of the load factor rule or
\item when we performed $0.5 m$ insertions from the beginning of the l-cycle and the load factor did not exceed the value of $1$.
\end{itemize}
\end{definition}
The l-cycle allows us to analyse the work caused by the chain limit rule. Both l-cycles and $\alpha$-cycles divide the sequence $o$. Notice that if an $\alpha$-cycle ends at the position $i$, the corresponding l-cycle also ends at the same position. 

The analysis now takes the $i$\textsuperscript{th} operation for every $i = 1, \dots, n$. We show that the expected amortised time complexity of the opertion $o_i$ is $O(1)$ independently on its type. The potential now consists of the two parts, $p_1$ and $p_2$. Let $e = \frac{1}{(1 - p)}$ denote the expected number of rehash operations, the expected number of trials, when finding a suitable function. 

Above all we want every simple insertion and deletion to take $O(1)$ time only. Thus the potential consists of the part $p_1 = 4ei_{\alpha} + 8ed_{\alpha}$ where $i_{\alpha}$ is the number inserts and $d_{\alpha}$ is the number of delete operations performed so far in the current $\alpha$-cycle. 

Next, we need to distribute the work caused by the chain limit rule. The second part of the potential $p_2 = 2ei_{l} + (ce - r) m$. The value $i_l$ denotes the number of insertions performed so far in the current l-cycle. The variable $r$ denotes the number of performed Rehash operations, which are caused by the chain limit rule violation, from the initial state. The variable $c$ denotes the number of started l-cycles from the beginning so far. The overall potential $p = p_1 + p_2$.

The analysis of Delete operation is simpler and comes first. When a deletion is performed we have to discuss the two cases:
\begin{itemize}
\item Simple successful deletion is expected to take $O(1)$ time. We search in chains that have constant expected length. The potential is increased by $8e$ since the number of deletions in $p_1$ gets increased by one.

\item The load factor $\alpha$ violates the lower bound of the load factor rule. Realise that there are at least $0.25 m$ deletions performed in the current $\alpha$-cycle, let us explain why. At the beginning of the cycle the table has the size of $m$ slots and the load factor equals $0.5$. At the end of the cycle the load factor decreased to $0.25$. Such a descent may be caused by at least $0.25 m$ successful delete operations. Let us discuss the potential change. First, the potential difference of the part $p_1$ is less or equals $-2em$ since $i_{\alpha}$ and $d_{\alpha}$ get zeroed. The second part $p_2$ gets increased by at most $em$ since a new l-cycle is started and the value $i_l$ is zeroed. Rehashing of the table takes $O(em)$ expected time. Hence the expected amortised cost of the operation is $O(1)$. 
\end{itemize}

The analysis of the first two cases of Insert operation remains similar to the case with forbidden Delete:
\begin{itemize}
\item Suppose no Rehash operation after an Insert is performed. The searched chain has the constant expected length and the potential is increased by $4e + 2e$.

\item If the load factor exceeds the upper bound, then there are at least $0.5m$ insertions in the current $\alpha$-cycle. It follows that the potential $p_2$ is certainly greater or equals $2em$. After performing the operation a new l-cycle is started. The potential $p_2$ is raised by $em$ because the variable $c$ gets incremented by one. But it is also lowered by at least $2em$ because $i_l$ is set to zero. The rehash operation is expected to take $O(em)$ time. Regarding the fact that the potential $p_1$ only decreases we expect $O(1)$ amortised time for Insert operation violating the load factor rule.

\item Insert operation is the last one in the l-cycle and the chain limit rule is not violated. Then there are $0.5 m$ insertions in the current l-cycle, equivalently $i_l = 0.5m$ and $p_2 = em + (ce - r)m$. Since a new l-cycle is started, the $i_l$ term is set to zero and the value $c$ is incremented by one. Therefore there is no potential change in the part $p_2$. The part $p_1$ only decreases.

\item Analysis of Insert operation, which violates the chain limit rule, remains. Time spent by Rehash operation equals $O(m)$ times the number of fails when finding a suitable function. The potential $p_2$ is decreased by the same value, since the variable $r$ gets incremented by the number of fails. The potential increase in the part $p_1$ and $p_2$ equals $6e$. Without Rehash operation the expected time of Insert is $O(1)$. The expected amortised time complexity of the analysed operation is thus constant provided that $\Expect{(ce - r)m} = 0$.

First, we show why the expected number of fails in one l-cycle equals $e$. Let $S$ be the set stored after performing the analysed operation. Let $S_e$ be the set created from the set $S$ by the remaining inserts of the current l-cycle. Because the table's load factor is maintained lower than 1 and there are at most $0.5 m$ inserts in every l-cycle, we have that $|S_e| \leq 1.5m$ and $S \subseteq S_e$. From Lemma \ref{lemma-sets} we have that $e = \frac{1}{1 - p}$ is the expected number trials needed to find a suitable function for every l-cycle.

Second, we show that $\Expect{(ce - r)m} = 0$. Define the random variable $C_i$ equal to the number of rehash operations required to fix the chain limit rule in the $i$\textsuperscript{th} l-cycle for $i = 1, \dots, c$. From the previous observation it follows that $\Expect{C_i} = e$. Clearly $r = \sum\displaylimits_{i = 1}^{c} C_i$. From Lemma \ref{lemma-expected-value-properties} it follows that
\[
	\Expect{{(ce - r)m}} \leq {m}\left(ce - \Expect{\displaystyle\sum_{i = 1}^{c} C_i}\right) = m(ce - c\Expect{C_1}) = mc(e - e) = 0 \text{.}
\]
Since $\Expect{(ce - r)m} = 0$ and $p_0 = 0$, the expected potential is always non-negative. Hence $T_o = A_o + p_0 - p_n \leq A_o$ and our analysis is thus correct.
\end{itemize}

We showed that the expected amortised complexity is constant for every operation.
\end{proof}

Let us note that the fact $\left|\frac{(ce - r)m}{n}\right| \xrightarrow{n \rightarrow \infty} 0$ indicates that it might be possible to show that the amortised complexity is constant, without any expectation. Since universal hashing is a randomised algorithm, such result would be certainly remarkable. However, if it was really true, there would be still many troubles showing the result.

So, one may ask if using the Law of Large Numbers, Theorem \ref{theorem-weak-law-of-large-numbers}, can not help. Indeed, from Lemma \ref{lemma-linear-transformations-trials} we have that $\Variance{C_i}$ is finite and it may be applied:
\[
\begin{split}
\left|\frac{(ce - r)m}{n}\right|
	& \leq m \left|\frac{ce - \sum\displaylimits_{i = 1}^{c} C_i}{c}\right| \\
	& \leq m \left|\frac{c\Expect{C_1}}{c} - \frac{\sum\displaylimits_{i = 1}^{c} C_i}{c}\right| \\
	& = m \left|\Expect{C_1} - \frac{\sum\displaylimits_{i = 1}^{c} C_i}{c}\right| \text{.}
\end{split}
\]

Clearly, the convergence in probability stated by Theorem \ref{theorem-weak-law-of-large-numbers} holds:
\[
	\left|\Expect{X_1} - \frac{\sum\displaylimits_{i = 1}^{c} X_i}{c}\right| \xrightarrow{c \rightarrow \infty} 0 \text{.}
\]

But in the case of $m \left|\Expect{C_1} - \frac{\sum\displaylimits_{i = 1}^{c} C_i}{c}\right|$ we have to be careful. There is an infinite class of sequences $\{o_i\}_{i = 1}^{n}$ for which $m \in \Omega(c)$. Thus for these sequences, if $c \rightarrow \infty$, then $m \rightarrow \infty$ as well.

In order to obtain convergence in probability we have that
\[
\begin{split}
\Prob{m \left|\Expect{C_1} - \frac{\sum\displaylimits_{i = 1}^{c} C_i}{c}\right| \geq \epsilon} 
	& = \Prob{\left|\Expect{C_1} - \frac{\sum\displaylimits_{i = 1}^{c} C_i}{c}\right| \geq \frac{\epsilon}{m}}  \\
	& \leq \frac{\Variance{C_i} m ^ 2}{m \epsilon ^ 2} = \frac{m\Variance{C_i}}{\epsilon ^ 2} \text{.}
\end{split}
\]
This is a problem, since when $m \rightarrow \infty$, we can not expect that the probability converges to zero for a fixed $\epsilon$.

For the class of sequences satisfying $m \in \Omega(c)$ the inequalities we used for estimate of $\left|\frac{(ce - r)m}{n}\right|$ mean only a multiplicative factor. Thus the estimate is tight and we see that the bound obtained from the Chebyshev's inequality is not sufficient. Its asymptotic rate is not high enough and the factor $m$ appears. However, because we know the probability distribution of $C_i$, maybe we could exploit it and be more accurate. This problem remains open.
