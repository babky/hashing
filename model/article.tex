\section{The Power of Two Choices Applied to Universal Hashing}

Mitzenmacher and Upfal in \cite{1076315} showed that if we use $d$ independent hash functions and put the presently saved object into a bin with the shorter chain, then the the longest chain contains at most $\frac{\ln \ln n}{\ln d}$ elements.

Now we use their approach to show a similar result for the universal hashing with a random uniform choice of a hash function.

\begin{definition}[$c$-universal system]
\label{definition-c-universal-system}
Let $U = \lbrace 0, \dots, N - 1 \rbrace$, $B = \lbrace 0, \dots, m - 1 \rbrace$ and $H$ be a multiset of functions from $U$ to $B$. Then $H$ is a \emph{$c$-universal system of functions} if for every $x \in U$ and $y \in B$ 
\[
\left|\lbrace f \in H \setdelim f(x) = y \rbrace\right| \leq \frac{c |H|}{m} \textit{.}
\]
\end{definition}

\begin{definition}[Independent hash functions]
\label{definition-independent-hash-functions}
Let $H$ be a $c$-universal system of hash functions. Let $f, g \in H$ be two functions chosen randomly and uniformly from the system $H$. We say that the functions $f, g$ are \emph{independent} if for every $x_1, x_2 \in U$ and $y_1, y_2 \in B$ $$\Prob{f(x_1) = y_1, g(x_2} = y_2) = \Prob{f(x_1) = y_1} \Prob{f(x_2) = y_2} \textit{.}$$.
\end{definition}

\begin{theorem}
\label{theorem-universal-hashing-two-choices}
Assume the model of universal hashing using at least a $c$-universal system of hash functions. Also assume that the functions in the system are independent. Also assume that we store $n$ elements into a table of size $m$ using $d$ random uniformly chosen functions from the system and that $\alpha c \leq 1$. If every stored value is placed into a shorter chain given by the values of the $d$ chosen hash functions, then $$\Prob{\lpsl > \frac{\ln \ln n}{\ln d} + 4} \in o\left(\frac{1}{n}\right) \textit{.}$$
\end{theorem}

\subsection{Notation}
We need some notation, identical to the original one, in order to successfuly restate the result in terms of universal hashing.

$U$ denotes the universe, $B$ denotes the hash table and $S$ is the hashed set. The size of the hash table is denoted by $m = |B|$ and the number of hashed elements is $n = |S|$. The table's load is the variable $\alpha = \frac{n}{m}$. By the state of the system at time $t$ we understand the number of elements in the chains of the table after the insertion of the $t$\textsuperscript{th} element.
The position of the ball inserted at time t is $h(t)$. Let $\mu_i(t)$ be the number of already stored elements at time $t$ that are at least at the $i$\textsuperscript{th} position in their chains. The number of chains that contain $i$ elements at least at time $t$ is denoted by $\nu_i(t)$. Remark that $\nu_i(t) \leq \mu_i(t)$ for every $t \in \{0, \dots, n - 1 \}$. Let $\mu_i = \mu_i(n)$ and $\nu_i = \nu_i(n)$.

\subsection{The Actual Proof}
First, we create a sequence of values $\beta_i$ such that $\beta_i \geq \nu_i$ for every $i \in \{0, \dots, n - 1\}$ with a high probability. To estimate the above probabilities we use the Chernoff's bounds, Lemma \ref{lemma-chernoff-bound}.

\begin{lemma}[Chernoff bound]
\label{lemma-chernoff-bound}
Let $n \in \mathbb{N}$ and $p \in (0, 1)$. Then $$\Prob{Bi(n, p) > 2np} \leq e ^ {- \frac{np}{3}} \textit{.}$$
\end{lemma}

The sequence is used to show that $\Prob{\nu_{i^{*}} > 1} \in o(1/n)$ and $i^* = \frac{\ln \ln (n)}{\ln d} + O(1)$. So the probability of having a chain longer than $i^*$ is low and we may rehash the table whenever $\lpsl > i^*$.

\begin{lemma}
\label{lemma-height-of-inserted-ball}
If $\beta_i > v_i(t)$, then $\Prob{h(t + 1) > i} \leq \left(\frac{c(\beta_i)}{m}\right) ^ d$.
\end{lemma}
\begin{proof}
First we estimate the probability of placing an element into a set of chains. Let $x \in U$, $R \subseteq B$ and $|R| \leq \beta_i$, then $$\Prob{f(x) \in R} = \displaystyle\sum_{r \in R} \Prob{f(x) = r} \leq \frac{c\beta_i}{m} \textit{.}$$
The only way how to place the $t + 1$\textsuperscript{th} value after the $i$\textsuperscript{th} position in the chain is to place it there by every of the $d$ hash function. Since the functions are mutually indepedent of each other we have $\Prob{h(t + 1) > i} \leq \left(\frac{c \beta_i}{m}\right) ^ d$.
\end{proof}

Now we define the sequence of values $\beta_i$ and give an insight into the way how it is chosen. Let $p_i = \left(\frac{c\beta_i}{m}\right) ^ d$, then $p_i$ majorizes the probability that an element is in the $i + 1$\textsuperscript{th} position or later in its chain. This can be seen from Lemma \ref{lemma-height-of-inserted-ball}. From Lemma \ref{lemma-chernoff-bound} it follows that $\Prob{Bi(n, p_i) \geq 2np_i} \leq \exp\left(\frac{-np_i}{3}\right)$. Random variable $Bi(n, p_i)$ majorizes $\mu_{i + 1}$ and with a high probability we have that $2np_i \geq \mu_{i + 1} \geq \nu_{i + 1}$. So if we need a sequence of values $\beta_i$ such that $\beta_i \geq \nu_{i}$ with a high probability, then we may put $\beta_{i + 1} = 2np_i = 2n\left(\frac{c\beta_i}{m}\right) ^ d$.

We need to define the start of the sequence $\beta$. To do it, we first define the sequence of events $\epsilon_i$. The event $\epsilon_i$ occurs if and only if $\beta_i \geq \nu_i$. Now put $\beta_4 = \frac{n}{4}$ and remark that $\Prob{\epsilon_4} = 1$. If $\epsilon_4$ does not hold, then $n \leq 4 \nu_4 < 4 \frac{n}{4} = n$. And finally, let us define the random binary variable $Y_t^i \in \{0, 1\}$ as $$Y_t^i = 1 \Leftrightarrow h(t) \geq i + 1 \textit{ and } \nu_i(t - 1) \leq \beta_i \textit{.}$$

Assume that for every $i \in \{1, \dots, n\}$ the event $\epsilon_i$ occurs. Then $\displaystyle\sum_{t = 1}^{n} Y_t^i = \mu_{i + 1}$. This holds because $\beta_i \geq \nu_i \geq \nu_i(t)$ for every $t \in \{1, \dots, n\}$ and thus the second condition for $Y_t^i$ to hold is satisfied. 

From the previous it is clear that 
\[
\Prob{\mu_{i + 1} > k | \epsilon_i} \leq \Prob{\mu_{i + 1} > k | \epsilon_i} = \Prob{\displaystyle\sum_{t = 1}^{n} Y_t^{i} > k | \epsilon_i} \leq \frac{\Prob{\sum_{t = 1}^{n} Y_t^{i} > k}}{\Prob{\epsilon_i}}\textit{.}
\]
If we substitute $2np_i$ into $k$, then
\[
\Prob{\nu_{i + 1} > 2np_i | \epsilon_i} \leq \frac{1}{\exp(\frac{np_i}{3})\Prob{\epsilon_i}} \text{.}
\]

Moreover, if we assume that $np_i \geq 6 \ln n$, we have that 
\[
\Prob{\nu_{i + 1} > 2np_i | \epsilon_i} = \Prob{\neg \epsilon_{i + 1}} \leq \frac{1}{n ^ 2\Prob{\epsilon_i}} \textit{.}
\]

Let us make a computation how to sequentially estimate the probability of $\epsilon_i$.
\[
\begin{split}
\Prob{\neg \epsilon_{i + 1}} 
	& = \Prob{\neg \epsilon_{i + 1} | \epsilon_i}\Prob{\epsilon_i} + \Prob{\neg \epsilon_{i + 1} | \neg \epsilon_i}\Prob{\neg \epsilon_i} \\
	& = \Prob{\nu_{i + 1} > \beta_{i + 1} | \epsilon_i}\Prob{\epsilon_i} + \Prob{\neg \epsilon_i} \\
	& \leq \frac{1}{n ^ 2} + \Prob{\neg \epsilon_i} \leq \frac{i + 1}{n ^ 2} \textit{.}
\end{split} 
\]

It remains to show the analysis when $np_i < 6 \ln n$. Let $i^*$ be the first $i \in \mathbb{N}$ such that $np_i < 6\ln n$. 
At first note that from the previous it follows that 
\[
\Prob{\neg \epsilon_{i^*}} \leq \frac{i^*}{n ^ 2} \textit{.}
\]

The following computation is an estimate for $i^* + 1$:
\[
\begin{split}
\Prob{\nu_{i^* + 1} > 12 \ln n | \epsilon_{i^*}} 
	& \leq \Prob{\mu_{i^* + 1} > 12 \ln n | \epsilon_{i^*}} \\
	& \leq \frac{\Prob{\mu_{i^* + 1} > 12 \ln n}}{\Prob{\epsilon_{i^*}}} \\
	& \leq \frac{\Prob{Bi\left(n, \frac{6 \ln n}{n}\right) > 12 \ln n}}{\Prob{\epsilon_{i^*}}} \\
	& \leq \frac{1}{\exp\left(\frac{n\frac{6 \ln n}{n}}{3}\right)\Prob{\epsilon_{i^*}}} = \frac{1}{n ^ 2 \Prob{\epsilon_{i^*}}} \textit{.}
\end{split}
\]
Thus
\[
\Prob{\nu_{i^* + 1} > 12 \ln n} \leq \Prob{\neg \epsilon_{i^*}} + \frac{1}{n ^ 2} \leq \frac{i^* + 1}{n ^ 2} \textit{.}
\]

Now we compute the probability that there is a chain of length at least $i^* + 3$:
\[
\begin{split}
& \Prob{\nu_{i^* + 3} \geq 1} \\
	& \quad \leq \Prob{\mu_{i^* + 3} \geq 1} \leq \Prob{\mu_{i^* + 2} \geq 2} \\
	& \quad \leq \Prob{\mu_{i^* + 2} \geq 2 | \nu_{i^* + 1} \leq 12 \ln n}\Prob{\nu_{i^* + 1} \leq 12 \ln n} + \Prob{\nu_{i^* + 1} > 12 \ln n} \\
	& \quad \leq \frac{\Prob{Bi\left(n, \left(\frac{12 c \ln n)}{n}\right) ^ d\right) \geq 2}}{\Prob{\nu_{i^* + 1} \leq 12 \ln n}}\Prob{\nu_{i^* + 1} \leq 12 \ln n} + \Prob{\nu_{i^* + 1} > 12 \ln n} \\
	& \quad \leq \binom{n}{2} \left(\frac{12 c \ln n}{n}\right) ^ {2d} + \frac{i^* + 1}{n ^ 2} \in o\left(\frac{1}{n}\right) \textit{.}
\end{split}
\]

We need an estimate for $i^*$, it was defined as $i^* = \displaystyle\min \{i \in \mathbb{N} \setdelim np_i < 6 \ln n\}$. First, we need an explicit formula for $\beta_i$. By induction we prove that $$\beta_{i + 4} = \frac{n \left(\alpha c\right) ^ {\sum_{j = 1}^{i}d ^ j}}{2 ^ {2 d ^ i - \sum_{j = 0}^{i - 1}{d ^ j}}} \textit{.}$$
For $i = 0$ we have that $\beta_4 = \frac{n}{4} = \frac{n\left(\alpha c\right) ^ 0}{2 ^ {2}}$ and thus the first step is true. The induction step
\[
\begin{split}
\beta_{i + 5} 
	& = 2np_i = 2n \left(\frac{c\beta_{i + 4}}{m}\right) ^ d = 2n \left(\frac{cn\left(\alpha c\right) ^ {\sum_{j = 1}^{i}d ^ j}}{m 2 ^ {2 d ^ i - \sum_{j = 1}^{i}d^j}}\right) ^ d \\
	& = \frac{n\left(\alpha c\right) ^ {d + \sum_{j = 1}^{i} d ^ {j + 1}}}{2 ^ {2d ^ {i + 1} - \sum_{j = 0}^{i  -1} d ^ {j + 1} - 1}} = \frac{n \left(\alpha c\right) ^ {\sum_{j = 1}^{i + 1} d ^ j}}{2 ^ {2d ^ {i + 1} - \sum_{j = 0}^{i} d ^ j}} \textit{.}
\end{split}
\]
First notice that the bound $\frac{6 \ln n}{n}$ may be achieved only if $\alpha c \leq 1$, then $\beta_{i + 4} \leq \frac{n}{2 ^ {d ^ i}}$ and $i^* = \frac{\ln \ln n}{\ln d} + 4$.
\[
p_i = \left(\frac{c\beta_i}{m}\right) ^ d \leq \left(\frac{\alpha c}{2 ^ {d ^ {i - 4}}}\right) ^ d \leq \frac{1}{2 ^ {de^{\ln \ln n}}} = \frac{1}{2 ^ {d \ln n}} = \frac{1}{n ^ {d \ln 2}} < \frac{6 \ln n}{n}\\
\]

Next, for $1 < \alpha c$, we are not always able to satisfiy the inequality $np_i < 6 \ln n$. Thus the load factor must be chosen as $\alpha \leq \frac{1}{c}$.
